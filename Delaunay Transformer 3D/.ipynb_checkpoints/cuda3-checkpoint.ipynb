{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "opponent-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "device = \"cuda:1\"\n",
    "floattype = torch.float\n",
    "\n",
    "batchsize = 512\n",
    "nsamples = 8\n",
    "npoints = 5\n",
    "emsize = 512\n",
    "\n",
    "\n",
    "class Graph_Transformer(nn.Module):\n",
    "    def __init__(self, emsize = 64, nhead = 8, nhid = 1024, nlayers = 4, ndecoderlayers = 2, dropout = 0):\n",
    "        super().__init__()\n",
    "        self.emsize = emsize\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "        encoder_layers = TransformerEncoderLayer(emsize, nhead, nhid, dropout = dropout)\n",
    "        decoder_layers = TransformerDecoderLayer(emsize, nhead, nhid, dropout = dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layers, ndecoderlayers)\n",
    "        self.encoder = nn.Linear(3, emsize)\n",
    "        self.outputattention_query = nn.Linear(emsize, emsize, bias = False)\n",
    "        self.outputattention_key = nn.Linear(emsize, emsize, bias = False)\n",
    "        self.start_token = nn.Parameter(torch.randn([emsize], device = device))\n",
    "    \n",
    "    def generate_subsequent_mask(self, sz): #last dimension will be softmaxed over when adding to attention logits, if boolean the ones turn into -inf\n",
    "        #mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        #mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        \n",
    "        mask = torch.triu(torch.ones([sz, sz], dtype = torch.bool, device = device), diagonal = 1)\n",
    "        return mask\n",
    "    \n",
    "    def encode(self, src): #src must be [batchsize * nsamples, npoints, 3]\n",
    "        src = self.encoder(src).transpose(0, 1)\n",
    "        output = self.transformer_encoder(src)\n",
    "        return output #[npoints, batchsize * nsamples, emsize]\n",
    "    \n",
    "    def decode_next(self, memory, tgt, route_mask): #route mask is [batchsize * nsamples, npoints], both memory and tgt must have batchsize and nsamples in same dimension (the 1th one)\n",
    "        npoints = memory.size(0)\n",
    "        batchsize = tgt.size(1)\n",
    "        \"\"\"if I really wanted this to be efficient I'd only recompute the decoder for the last tgt, and just remebering what the others looked like from before (won't change due to mask)\"\"\"\n",
    "        \"\"\"have the option to freeze the autograd on all but the last part of tgt, although at the moment this is a very natural way to say: initial choices matter more\"\"\"\n",
    "        tgt_mask = self.generate_subsequent_mask(tgt.size(0))\n",
    "        output = self.transformer_decoder(tgt, memory, tgt_mask) #[tgt, batchsize * nsamples, emsize]\n",
    "        output_query = self.outputattention_query(memory).transpose(0, 1) #[batchsize * nsamples, npoints, emsize]\n",
    "        output_key = self.outputattention_key(output[-1]) #[batchsize * nsamples, emsize]\n",
    "        output_attention = torch.matmul(output_query * self.emsize ** -0.5, output_key.unsqueeze(-1)).squeeze(-1) #[batchsize * nsamples, npoints], technically don't need to scale attention as we divide by variance next anyway\n",
    "        output_attention_tanh = output_attention.tanh() #[batchsize * nsamples, npoints]\n",
    "        \n",
    "        #we clone the route_mask incase we want to backprop using it (else it was modified by inplace opporations)\n",
    "        output_attention = output_attention.masked_fill(route_mask.clone(), float('-inf')) #[batchsize * nsamples, npoints]\n",
    "        output_attention_tanh = output_attention_tanh.masked_fill(route_mask.clone(), float('-inf')) #[batchsize * nsamples, npoints]\n",
    "        \n",
    "        return output_attention_tanh, output_attention #[batchsize * nsamples, npoints]\n",
    "    \n",
    "    def calculate_logprob(self, memory, routes): #memory is [npoints, batchsize * nsamples, emsize], routes is [batchsize * nsamples, npoints - 4], rather than backproping the entire loop, this saves vram (and computation)\n",
    "        npoints = memory.size(0)\n",
    "        ninternalpoints = routes.size(1)\n",
    "        bigbatchsize = memory.size(1)\n",
    "        memory_ = memory.gather(0, routes.transpose(0, 1).unsqueeze(2).expand(-1, -1, self.emsize)) #[npoints - 4, batchsize * nsamples, emsize] reorder memory into order of routes\n",
    "        tgt = torch.cat([self.start_token.unsqueeze(0).unsqueeze(1).expand(1, bigbatchsize, -1), memory_[:-1]]) #[npoints - 4, batchsize * nroutes, emsize], want to go from memory to tgt\n",
    "        tgt_mask = self.generate_subsequent_mask(ninternalpoints)\n",
    "        output = self.transformer_decoder(tgt, memory, tgt_mask) #[npoints - 4, batchsize * nsamples, emsize]\n",
    "        \"\"\"want probability of going from key to query, but first need to normalise (softmax with mask)\"\"\"\n",
    "        output_query = self.outputattention_query(memory_).transpose(0, 1) #[batchsize * nsamples, npoints - 4, emsize]\n",
    "        output_key = self.outputattention_key(output).transpose(0, 1) #[batchsize * nsamples, npoints - 4, emsize]\n",
    "        attention_mask = torch.full([ninternalpoints, ninternalpoints], True, device = device).triu(1) #[npoints - 4, npoints - 4], True for i < j\n",
    "        output_attention = torch.matmul(output_query * self.emsize ** -0.5, output_key.transpose(-1, -2))\n",
    "        \"\"\"quick fix to stop divergence\"\"\"\n",
    "        output_attention_tanh = output_attention.tanh()\n",
    "        \n",
    "        output_attention_tanh = output_attention_tanh.masked_fill(attention_mask, float('-inf'))\n",
    "        output_attention_tanh = output_attention_tanh - output_attention_tanh.logsumexp(-2, keepdim = True) #[batchsize * nsamples, npoints - 4, npoints - 4]\n",
    "        \n",
    "        output_attention = output_attention.masked_fill(attention_mask, float('-inf'))\n",
    "        output_attention = output_attention - output_attention.logsumexp(-2, keepdim = True) #[batchsize * nsamples, npoints - 4, npoints - 4]\n",
    "        \n",
    "        \"\"\"infact I'm almost tempted to not mask choosing a previous point, so it's forced to learn it and somehow incorporate it into its computation, but without much impact on reinforcing good examples\"\"\"\n",
    "        logprob_tanh = output_attention_tanh.diagonal(dim1 = -1, dim2 = -2).sum(-1) #[batchsize * nsamples]\n",
    "        logprob = output_attention.diagonal(dim1 = -1, dim2 = -2).sum(-1) #[batchsize * nsamples]\n",
    "        return logprob_tanh, logprob #[batchsize * nsamples]\n",
    "\n",
    "NN = Graph_Transformer().to(device)\n",
    "optimizer = optim.Adam(NN.parameters())\n",
    "\n",
    "\n",
    "class environment:    \n",
    "    def reset(self, npoints, batchsize, nsamples=1, corner_points = None, initial_triangulation = None):\n",
    "        \"\"\"\n",
    "        corner_points, etc., shoudn't include a batch dimension\n",
    "        \"\"\"\n",
    "        if corner_points == None:\n",
    "            ncornerpoints = 4\n",
    "        else:\n",
    "            ncornerpoints = corner_points.size(0)\n",
    "        if npoints <= ncornerpoints:\n",
    "            print(\"Error: not enough points for valid problem instance\")\n",
    "            return\n",
    "        self.batchsize = (\n",
    "            batchsize * nsamples\n",
    "        )  # so that I don't have to rewrite all this code, we store these two dimensions together\n",
    "        self.nsamples = nsamples\n",
    "        self.npoints = npoints\n",
    "        self.points = (\n",
    "            torch.rand([batchsize, npoints - ncornerpoints, 3], dtype = floattype, device=device)\n",
    "            .unsqueeze(1)\n",
    "            .expand(-1, nsamples, -1, -1)\n",
    "            .reshape(self.batchsize, npoints - ncornerpoints, 3)\n",
    "        )\n",
    "        if corner_points == None:\n",
    "            self.corner_points = torch.tensor(\n",
    "                [[0, 0, 0], [3, 0, 0], [0, 3, 0], [0, 0, 3]], dtype = floattype, device=device\n",
    "            )\n",
    "        else:\n",
    "            self.corner_points = corner_points\n",
    "        self.points = torch.cat(\n",
    "            [\n",
    "                self.corner_points.unsqueeze(0).expand(self.batchsize, -1, -1),\n",
    "                self.points,\n",
    "            ],\n",
    "            dim=-2,\n",
    "        )  # [batchsize * nsamples, npoints, 3]\n",
    "        self.points_mask = torch.cat(\n",
    "            [\n",
    "                torch.ones([self.batchsize, ncornerpoints], dtype=torch.bool, device=device),\n",
    "                torch.zeros(\n",
    "                    [self.batchsize, npoints - ncornerpoints], dtype=torch.bool, device=device\n",
    "                ),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        self.points_sequence = torch.empty(\n",
    "            [self.batchsize, 0], dtype=torch.long, device=device\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        points are now triples\n",
    "        triangles are now quadruples\n",
    "        edges are now still just indices, but there are four of them per 'triangle', and they correspond to triples of points, not pairs\n",
    "        we use  0,2,1  0,3,2  0,1,3  1,2,3  as the order of the four 'edges'/faces\n",
    "        opposite face is always ordered such that the last two indices are swapped\n",
    "        faces are always read ANTICLOCKWISE\n",
    "        \n",
    "        first three points of tetrahedron MUST be read clockwise (from the outside) to get correct sign on incircle test\n",
    "        \n",
    "        new point will be inserted in zeroth position, so if corresponding face of REMOVED tetrahedron is [x,y,z] (being read anticlockwise from outside in) new tetrahedron is [p, x, y, z]\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        number of tetrahedra is not the same for each batch (in 3D), so store as a big list, and remember batch index that it comes from\n",
    "        \"\"\"\n",
    "        if corner_points == None:\n",
    "            initial_triangulation = torch.tensor([[0, 1, 2, 3]], dtype=torch.long, device=device)\n",
    "        \n",
    "        self.partial_delaunay_triangles = initial_triangulation.unsqueeze(0).expand(self.batchsize, -1, -1).reshape(-1, 4)\n",
    "        self.batch_index = torch.arange(self.batchsize, dtype = torch.long, device = device).unsqueeze(1).expand(-1, initial_triangulation.size(0)).reshape(-1)\n",
    "        \n",
    "        self.batch_triangles = self.partial_delaunay_triangles.size(0) #[0]\n",
    "        self.ntriangles = torch.full([self.batchsize], initial_triangulation.size(0), dtype = torch.long, device = device) #[self.batchsize]\n",
    "        \n",
    "        self.cost = torch.zeros([self.batchsize], dtype = floattype, device=device)\n",
    "\n",
    "        self.logprob = torch.zeros([self.batchsize], dtype = floattype, device=device, requires_grad=True)\n",
    "\n",
    "    def update(self, point_index):  # point_index is [batchsize]\n",
    "        \n",
    "        assert point_index.size(0) == self.batchsize\n",
    "        assert str(point_index.device) == device\n",
    "        assert self.points_mask.gather(1, point_index.unsqueeze(1)).sum() == 0\n",
    "        \n",
    "        triangles_coordinates = self.points[self.batch_index.unsqueeze(1), self.partial_delaunay_triangles] # [batch_triangles, 4, 3]\n",
    "        \n",
    "        newpoint = self.points[self.batch_index, point_index[self.batch_index]] # [batch_triangles, 3]\n",
    "\n",
    "        incircle_matrix = torch.cat(\n",
    "            [\n",
    "                newpoint.unsqueeze(1),\n",
    "                triangles_coordinates,\n",
    "            ],\n",
    "            dim=-2,\n",
    "        )  # [batch_triangles, 5, 3]\n",
    "        incircle_matrix = torch.cat(\n",
    "            [\n",
    "                (incircle_matrix * incircle_matrix).sum(-1, keepdim=True),\n",
    "                incircle_matrix,\n",
    "                torch.ones([self.batch_triangles, 5, 1], dtype = floattype, device=device),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )  # [batch_triangles, 5, 5]\n",
    "        assert incircle_matrix.dtype == floattype\n",
    "        assert str(incircle_matrix.device) == device\n",
    "        \n",
    "        incircle_test = (\n",
    "            incircle_matrix.det() > 0\n",
    "        )  # [batch_triangles], is True if inside incircle\n",
    "        \n",
    "        conflicts = incircle_test.sum()\n",
    "        \n",
    "        conflicting_triangles = self.partial_delaunay_triangles[incircle_test] # [conflicts, 4]\n",
    "        \n",
    "        conflicting_edges_index0 = torch.empty_like(conflicting_triangles)\n",
    "        indices = torch.LongTensor([0, 0, 0, 1])\n",
    "        conflicting_edges_index0 = conflicting_triangles[:, indices] # [conflicts, 4]\n",
    "        \n",
    "        conflicting_edges_index1 = torch.empty_like(conflicting_triangles)\n",
    "        indices = torch.LongTensor([2, 3, 1, 2])\n",
    "        conflicting_edges_index1 = conflicting_triangles[:, indices] # [conflicts, 4]\n",
    "        \n",
    "        conflicting_edges_index2 = torch.empty_like(conflicting_triangles)\n",
    "        indices = torch.LongTensor([1, 2, 3, 3])\n",
    "        conflicting_edges_index2 = conflicting_triangles[:, indices] # [conflicts, 4]\n",
    "        \n",
    "        conflicting_edges = torch.cat([conflicting_edges_index0.view(-1).unsqueeze(-1), conflicting_edges_index1.view(-1).unsqueeze(-1), conflicting_edges_index2.view(-1).unsqueeze(-1)], dim = -1).reshape(-1, 3) # [conflicts * 4, 3]\n",
    "        \n",
    "        edge_batch_index = self.batch_index[incircle_test].unsqueeze(1).expand(-1, 4).reshape(-1) # [conflicts * 4]\n",
    "        \n",
    "        indices = torch.LongTensor([0, 2, 1])\n",
    "        comparison_edges = conflicting_edges[:, indices] # [conflicts * 4, 3]        \n",
    "        \n",
    "        unravel_nomatch_mask = torch.ones([conflicts * 4], dtype = torch.bool, device = device) # [conflicts * 4]\n",
    "        i = 1\n",
    "        while True:\n",
    "            \n",
    "            todo_mask = unravel_nomatch_mask[:-i].logical_and(edge_batch_index[:-i] == edge_batch_index[i:])\n",
    "            if i % 4 == 0:\n",
    "                if todo_mask.sum() == 0:\n",
    "                    break\n",
    "            \n",
    "            match_mask = todo_mask.clone()\n",
    "            match_mask[todo_mask] = (conflicting_edges[:-i][todo_mask] != comparison_edges[i:][todo_mask]).sum(-1).logical_not()\n",
    "            \n",
    "            unravel_nomatch_mask[:-i][match_mask] = False\n",
    "            unravel_nomatch_mask[i:][match_mask] = False\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        batch_newtriangles = unravel_nomatch_mask.sum()\n",
    "        \n",
    "        nomatch_edges = conflicting_edges[unravel_nomatch_mask] # [batch_newtriangles, 3], already in correct order to insert into 1,2,3 (since already anticlockwise from outside in)\n",
    "        assert list(nomatch_edges.size()) == [batch_newtriangles, 3]\n",
    "        nomatch_batch_index = edge_batch_index[unravel_nomatch_mask] # [batch_newtriangles]\n",
    "        \n",
    "        nomatch_newpoint = point_index[nomatch_batch_index] # [batch_newtriangles]\n",
    "        \n",
    "        newtriangles = torch.cat([nomatch_newpoint.unsqueeze(1), nomatch_edges], dim = -1) # [batch_newtriangles, 4]\n",
    "        \n",
    "        \n",
    "        nremoved_triangles = torch.zeros([self.batchsize], dtype = torch.long, device = device)\n",
    "        nnew_triangles = torch.zeros([self.batchsize], dtype = torch.long, device = device)\n",
    "        \n",
    "        indices = self.batch_index[incircle_test]\n",
    "        nremoved_triangles.put_(indices, torch.ones_like(indices, dtype = torch.long), accumulate = True) # [batchsize]\n",
    "        \n",
    "        indices = edge_batch_index[unravel_nomatch_mask]\n",
    "        nnew_triangles.put_(indices, torch.ones_like(indices, dtype = torch.long), accumulate = True) # [batchsize]\n",
    "        \n",
    "        assert (nnew_triangles <= 2 * nremoved_triangles + 2).logical_not().sum().logical_not()\n",
    "        \n",
    "        \"\"\"\n",
    "        NOTE:\n",
    "        I THINK it's possible for nnew_triangles to be less than nremoved_triangles (or my code is just buggy...)\n",
    "        \"\"\"\n",
    "        \n",
    "        assert nnew_triangles.sum() == batch_newtriangles\n",
    "        assert nremoved_triangles.sum() == incircle_test.sum()\n",
    "        \n",
    "        nadditional_triangles = nnew_triangles - nremoved_triangles # [batchsize]\n",
    "        ntriangles = self.ntriangles + nadditional_triangles # [batchsize]\n",
    "        \n",
    "        partial_delaunay_triangles = torch.empty([ntriangles.sum(), 4], dtype = torch.long, device = device)\n",
    "        batch_index = torch.empty([ntriangles.sum()], dtype = torch.long, device = device)\n",
    "        \n",
    "        cumulative_triangles = torch.cat([torch.zeros([1], dtype = torch.long, device = device), nnew_triangles.cumsum(0)[:-1]]) # [batchsize], cumulative sum starts at zero\n",
    "        \n",
    "        \"\"\"\n",
    "        since may actually have LESS triangles than previous round, we insert all that survive into the first slots (in that batch)\n",
    "        \"\"\"\n",
    "        good_triangle_indices = torch.arange(incircle_test.logical_not().sum(), dtype = torch.long, device = device)\n",
    "        good_triangle_indices += cumulative_triangles[self.batch_index[incircle_test.logical_not()]]\n",
    "        bad_triangle_indices_mask = torch.ones([ntriangles.sum(0)], dtype = torch.bool, device = device)\n",
    "        bad_triangle_indices_mask.scatter_(0, good_triangle_indices, False)\n",
    "        \n",
    "        assert good_triangle_indices.size(0) == incircle_test.logical_not().sum()\n",
    "        assert bad_triangle_indices_mask.sum() == batch_newtriangles\n",
    "        \n",
    "        partial_delaunay_triangles[good_triangle_indices] = self.partial_delaunay_triangles[~incircle_test]\n",
    "        batch_index[good_triangle_indices] = self.batch_index[~incircle_test]\n",
    "        \n",
    "        partial_delaunay_triangles[bad_triangle_indices_mask] = newtriangles\n",
    "        batch_index[bad_triangle_indices_mask] = nomatch_batch_index\n",
    "        \n",
    "        self.partial_delaunay_triangles = partial_delaunay_triangles\n",
    "        self.batch_index = batch_index\n",
    "        \n",
    "        self.ntriangles = ntriangles\n",
    "        self.batch_triangles = self.partial_delaunay_triangles.size(0)\n",
    "        \n",
    "        self.points_mask.scatter_(\n",
    "            1, point_index.unsqueeze(1).expand(-1, self.npoints), True\n",
    "        )\n",
    "        self.points_sequence = torch.cat(\n",
    "            [self.points_sequence, point_index.unsqueeze(1)], dim=1\n",
    "        )\n",
    "        \n",
    "        self.cost += nremoved_triangles\n",
    "        return\n",
    "    \n",
    "    def sample_point(self, logits): #logits must be [batchsize * nsamples, npoints]\n",
    "        probs = torch.distributions.categorical.Categorical(logits = logits)\n",
    "        next_point = probs.sample() #size is [batchsize * nsamples]\n",
    "        self.update(next_point)\n",
    "        self.logprob = self.logprob + probs.log_prob(next_point)\n",
    "        return next_point #[batchsize * nsamples]\n",
    "    \n",
    "    def sampleandgreedy_point(self, logits): #logits must be [batchsize * nsamples, npoints], last sample will be the greedy choice (but we still need to keep track of its logits)\n",
    "        logits_sample = logits.view(-1, self.nsamples, self.npoints)[:, :-1, :]\n",
    "        probs = torch.distributions.categorical.Categorical(logits = logits_sample)\n",
    "        \n",
    "        sample_point = probs.sample() #[batchsize, (nsamples - 1)]\n",
    "        greedy_point = logits.view(-1, self.nsamples, self.npoints)[:, -1, :].max(-1, keepdim = True)[1] #[batchsize, 1]\n",
    "        next_point = torch.cat([sample_point, greedy_point], dim = 1).view(-1)\n",
    "        self.update(next_point)\n",
    "        self.logprob = self.logprob + torch.cat([probs.log_prob(sample_point), torch.zeros([sample_point.size(0), 1], device = device)], dim = 1).view(-1)\n",
    "        return next_point\n",
    "    \n",
    "\n",
    "env = environment()\n",
    "\n",
    "\n",
    "def train(epochs = 30000, npoints = 14, batchsize = 100, nsamples = 8):\n",
    "    NN.train()\n",
    "    for i in range(epochs):\n",
    "        env.reset(npoints, batchsize, nsamples)\n",
    "        \"\"\"include the boundary points, kinda makes sense that they should contribute (atm only in the encoder, difficult to see how in the decoder)\"\"\"\n",
    "        memory = NN.encode(env.points) #[npoints, batchsize * nsamples, emsize]\n",
    "        #### #### #### remember to include tgt.detach() when reinstate with torch.no_grad()\n",
    "        tgt = NN.start_token.unsqueeze(0).unsqueeze(1).expand(1, batchsize * nsamples, -1).detach() #[1, batchsize * nsamples, emsize]\n",
    "        #with torch.no_grad(): #to speed up computation, selecting routes is done without gradient\n",
    "        with torch.no_grad():\n",
    "            for j in range(4, npoints):\n",
    "                #### #### #### remember to include memory.detach() when reinstate with torch.no_grad()\n",
    "                _, logits = NN.decode_next(memory.detach(), tgt, env.points_mask)\n",
    "                next_point = env.sampleandgreedy_point(logits)\n",
    "                \"\"\"\n",
    "                for inputing the previous embedding into decoder\n",
    "                \"\"\"\n",
    "                tgt = torch.cat([tgt, memory.gather(0, next_point.unsqueeze(0).unsqueeze(2).expand(1, -1, memory.size(2)))]) #[nsofar, batchsize * nsamples, emsize]\n",
    "                \"\"\"\n",
    "                for inputing the previous decoder output into the decoder (allows for an evolving strategy, but doesn't allow for fast training\n",
    "                \"\"\"\n",
    "                ############\n",
    "\n",
    "        \n",
    "        NN.eval()\n",
    "        _, logprob = NN.calculate_logprob(memory, env.points_sequence) #[batchsize * nsamples]\n",
    "        NN.train()\n",
    "        \"\"\"\n",
    "        clip logprob so doesn't reinforce things it already knows\n",
    "        TBH WANT SOMETHING DIFFERENT ... want to massively increase training if find something unexpected and otherwise not\n",
    "        \"\"\"\n",
    "        greedy_prob = logprob.view(batchsize, nsamples)[:, -1].detach() #[batchsize]\n",
    "        greedy_baseline = env.cost.view(batchsize, nsamples)[:, -1] #[batchsize], greedy sample\n",
    "        fixed_baseline = 0.5 * torch.ones([1], device = device)\n",
    "        min_baseline = env.cost.view(batchsize, nsamples)[:, :-1].min(-1)[0] #[batchsize], minimum cost\n",
    "        baseline = greedy_baseline\n",
    "        positive_reinforcement = - F.relu( - (env.cost.view(batchsize, nsamples)[:, :-1] - baseline.unsqueeze(1))) #don't scale positive reinforcement\n",
    "        negative_reinforcement = F.relu(env.cost.view(batchsize, nsamples)[:, :-1] - baseline.unsqueeze(1))\n",
    "        positive_reinforcement_binary = env.cost.view(batchsize, nsamples)[:, :-1] - baseline.unsqueeze(1) <= -0.5\n",
    "        negative_reinforcement_binary = env.cost.view(batchsize, nsamples)[:, :-1] - baseline.unsqueeze(1) > 5\n",
    "        \"\"\"\n",
    "        binary positive reinforcement\n",
    "        \"\"\"\n",
    "        #loss = - ((logprob.view(batchsize, nsamples)[:, :-1] < -0.2) * logprob.view(batchsize, nsamples)[:, :-1] * positive_reinforcement_binary).mean() #+ (logprob.view(batchsize, nsamples)[:, :-1] > -1) * logprob.view(batchsize, nsamples)[:, :-1] * negative_reinforcement_binary\n",
    "        \"\"\"\n",
    "        clipped binary reinforcement\n",
    "        \"\"\"\n",
    "        loss = ( \n",
    "                - logprob.view(batchsize, nsamples)[:, :-1] \n",
    "                #* (logprob.view(batchsize, nsamples)[:, :-1] < 0) \n",
    "                * positive_reinforcement_binary \n",
    "                + logprob.view(batchsize, nsamples)[:, :-1] \n",
    "                #* (logprob.view(batchsize, nsamples)[:, :-1] > greedy_prob.unsqueeze(1) - 3) \n",
    "                * negative_reinforcement_binary \n",
    "        ).mean()\n",
    "        \"\"\"\n",
    "        clipped binary postive, clipped weighted negative\n",
    "        \"\"\"\n",
    "        #loss = ( - logprob.view(batchsize, nsamples)[:, :-1] * (logprob.view(batchsize, nsamples)[:, :-1] < -0.2) * positive_reinforcement_binary + logprob.view(batchsize, nsamples)[:, :-1] * (logprob.view(batchsize, nsamples)[:, :-1] > -2) * negative_reinforcement ).mean()\n",
    "        \"\"\"\n",
    "        clipped reinforcement without rescaling\n",
    "        \"\"\"\n",
    "        #loss = ((logprob.view(batchsize, nsamples)[:, :-1] < -0.7) * logprob.view(batchsize, nsamples)[:, :-1] * positive_reinforcement + (logprob.view(batchsize, nsamples)[:, :-1] > -5) * logprob.view(batchsize, nsamples)[:, :-1] * negative_reinforcement).mean()\n",
    "        \"\"\"\n",
    "        clipped reinforcement\n",
    "        \"\"\"\n",
    "        #loss = (logprob.view(batchsize, nsamples)[:, :-1] * positive_reinforcement / (positive_reinforcement.var() + 0.001).sqrt() + (logprob.view(batchsize, nsamples)[:, :-1] > -3) * logprob.view(batchsize, nsamples)[:, :-1] * negative_reinforcement / (negative_reinforcement.var() + 0.001).sqrt()).mean()\n",
    "        \"\"\"\n",
    "        balanced reinforcement\n",
    "        \"\"\"\n",
    "        #loss = (logprob.view(batchsize, nsamples)[:, :-1] * (positive_reinforcement / (positive_reinforcement.var() + 0.001).sqrt() + negative_reinforcement / (negative_reinforcement.var() + 0.001).sqrt())).mean()\n",
    "        \"\"\"\n",
    "        regular loss\n",
    "        \"\"\"\n",
    "        #loss = (logprob.view(batchsize, nsamples)[:, :-1] * (positive_reinforcement + negative_reinforcement)).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #print(NN.encoder.weight.grad)\n",
    "        optimizer.step()\n",
    "        #print(greedy_baseline.mean().item())\n",
    "        print(greedy_baseline.mean().item(), logprob.view(batchsize, nsamples)[:, -1].mean().item(), logprob.view(batchsize, nsamples)[:, :-1].mean().item(), logprob[batchsize - 1].item(), logprob[0].item(), env.logprob[0].item())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-intervention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1441.919921875 -357.1465148925781 -363.3877258300781 -362.6243591308594 -362.9994812011719 -362.99945068359375\n",
      "1529.3199462890625 -331.3104553222656 -356.8393859863281 -354.5503845214844 -358.065185546875 -358.06524658203125\n",
      "1316.429931640625 -359.1705017089844 -363.6056213378906 -363.31787109375 -364.5432434082031 -364.5431823730469\n",
      "1344.6500244140625 -359.54718017578125 -363.6337585449219 -364.15484619140625 -363.283203125 -363.28326416015625\n",
      "1361.699951171875 -360.5489807128906 -363.6783142089844 -363.4687194824219 -363.6650085449219 -363.66497802734375\n",
      "1403.3199462890625 -361.19952392578125 -363.6924133300781 -363.48199462890625 -364.26812744140625 -364.2681579589844\n",
      "1454.9599609375 -360.81689453125 -363.6833801269531 -364.11187744140625 -364.1959228515625 -364.1958923339844\n",
      "1476.7099609375 -360.279052734375 -363.6824951171875 -363.4613342285156 -363.3717346191406 -363.3717956542969\n",
      "1473.8499755859375 -360.533203125 -363.6838073730469 -363.8301696777344 -363.5639343261719 -363.56390380859375\n",
      "1476.0499267578125 -361.0174560546875 -363.6922302246094 -364.06011962890625 -363.58123779296875 -363.5812072753906\n",
      "1478.93994140625 -361.35577392578125 -363.71942138671875 -363.5290832519531 -363.5781555175781 -363.5780334472656\n",
      "1471.760009765625 -361.9256896972656 -363.71893310546875 -363.3318786621094 -363.5630798339844 -363.5630187988281\n",
      "1475.0899658203125 -362.3239440917969 -363.7237854003906 -363.6739501953125 -363.76007080078125 -363.76007080078125\n",
      "1501.659912109375 -362.5621032714844 -363.7279968261719 -363.9069519042969 -363.81494140625 -363.81494140625\n",
      "1525.929931640625 -362.7149963378906 -363.7329406738281 -363.83721923828125 -363.71356201171875 -363.7135314941406\n",
      "1565.239990234375 -362.83905029296875 -363.7274169921875 -363.7616882324219 -363.5848388671875 -363.5848388671875\n",
      "1603.9000244140625 -362.861328125 -363.7403869628906 -363.79461669921875 -363.56890869140625 -363.56890869140625\n",
      "1639.5 -362.9217834472656 -363.7298278808594 -363.75213623046875 -363.6289978027344 -363.62890625\n",
      "1664.97998046875 -362.91790771484375 -363.738525390625 -363.80108642578125 -363.7640686035156 -363.7641296386719\n",
      "1686.18994140625 -362.9573669433594 -363.73504638671875 -363.67474365234375 -363.85040283203125 -363.8504638671875\n",
      "1672.8599853515625 -362.99334716796875 -363.7367858886719 -363.79083251953125 -363.67327880859375 -363.6733703613281\n",
      "1644.5599365234375 -363.04241943359375 -363.73779296875 -363.638671875 -363.8583679199219 -363.8585205078125\n",
      "1611.0 -363.0852355957031 -363.7342224121094 -363.59149169921875 -363.7471923828125 -363.7471923828125\n",
      "1627.8800048828125 -363.0894470214844 -363.7368469238281 -363.70880126953125 -363.6980895996094 -363.697998046875\n",
      "1637.489990234375 -363.10687255859375 -363.736572265625 -363.6363220214844 -363.60821533203125 -363.6081848144531\n",
      "1619.8699951171875 -363.11810302734375 -363.7383117675781 -363.7103271484375 -363.65484619140625 -363.65484619140625\n",
      "1597.8499755859375 -363.1689453125 -363.73590087890625 -363.71868896484375 -363.7611999511719 -363.76123046875\n",
      "1582.93994140625 -363.17706298828125 -363.7381896972656 -363.69647216796875 -363.682373046875 -363.68231201171875\n",
      "1568.699951171875 -363.2000427246094 -363.73260498046875 -363.70550537109375 -363.6904296875 -363.6903381347656\n",
      "1555.659912109375 -363.1651611328125 -363.7353210449219 -363.7275390625 -363.669677734375 -363.6695861816406\n",
      "1550.1199951171875 -363.12451171875 -363.7401428222656 -363.76947021484375 -363.7666015625 -363.7664489746094\n",
      "1553.199951171875 -363.1142883300781 -363.7359619140625 -363.7076110839844 -363.796630859375 -363.7966613769531\n",
      "1556.919921875 -363.0899963378906 -363.73516845703125 -363.7159729003906 -363.6719970703125 -363.67193603515625\n",
      "1548.2799072265625 -363.0635986328125 -363.7393493652344 -363.8670654296875 -363.69970703125 -363.69970703125\n",
      "1551.5399169921875 -363.06005859375 -363.74249267578125 -363.78070068359375 -363.80859375 -363.80853271484375\n",
      "1540.3299560546875 -363.11468505859375 -363.73284912109375 -363.59918212890625 -363.77081298828125 -363.770751953125\n",
      "1538.6199951171875 -363.12786865234375 -363.7378234863281 -363.7653503417969 -363.788818359375 -363.7886962890625\n",
      "1529.949951171875 -363.130859375 -363.7367248535156 -363.5962219238281 -363.61517333984375 -363.6151428222656\n",
      "1538.3699951171875 -363.1525573730469 -363.7369079589844 -363.7127380371094 -363.74725341796875 -363.747314453125\n",
      "1535.3299560546875 -363.15545654296875 -363.7431640625 -363.80474853515625 -363.7281188964844 -363.7279968261719\n",
      "1531.260009765625 -363.20843505859375 -363.7367248535156 -363.72784423828125 -363.7129211425781 -363.7128601074219\n",
      "1526.9599609375 -363.24444580078125 -363.7412109375 -363.74578857421875 -363.7972412109375 -363.79730224609375\n",
      "1530.6099853515625 -363.305419921875 -363.73846435546875 -363.70556640625 -363.791748046875 -363.7919006347656\n",
      "1524.6300048828125 -363.3475036621094 -363.737548828125 -363.6957092285156 -363.5807800292969 -363.58074951171875\n",
      "1522.7899169921875 -363.38238525390625 -363.73486328125 -363.72271728515625 -363.7546081542969 -363.7545471191406\n",
      "1529.3800048828125 -363.3927917480469 -363.7384338378906 -363.7861328125 -363.71697998046875 -363.7169494628906\n",
      "1527.8299560546875 -363.3981628417969 -363.7384033203125 -363.73016357421875 -363.7534484863281 -363.7532958984375\n",
      "1514.3399658203125 -363.40234375 -363.73822021484375 -363.74163818359375 -363.74072265625 -363.7408447265625\n",
      "1526.0499267578125 -363.39544677734375 -363.74072265625 -363.67132568359375 -363.7118225097656 -363.7117919921875\n",
      "1526.3800048828125 -363.4093017578125 -363.7371826171875 -363.75390625 -363.74224853515625 -363.7421569824219\n",
      "1526.8299560546875 -363.4023742675781 -363.7414245605469 -363.74517822265625 -363.7490234375 -363.7489929199219\n",
      "1524.0299072265625 -363.4112548828125 -363.7414855957031 -363.6865234375 -363.7431335449219 -363.7430419921875\n",
      "1529.260009765625 -363.43597412109375 -363.7389831542969 -363.83648681640625 -363.77899169921875 -363.77899169921875\n",
      "1533.3199462890625 -363.4548645019531 -363.73565673828125 -363.63677978515625 -363.69061279296875 -363.6905517578125\n",
      "1534.719970703125 -363.45166015625 -363.73858642578125 -363.7362060546875 -363.7386779785156 -363.7386779785156\n",
      "1540.6500244140625 -363.44781494140625 -363.7398986816406 -363.73980712890625 -363.7724609375 -363.7724914550781\n",
      "1540.1400146484375 -363.4519958496094 -363.73846435546875 -363.7688293457031 -363.76458740234375 -363.7646179199219\n",
      "1545.27001953125 -363.4535217285156 -363.7409973144531 -363.6856994628906 -363.74383544921875 -363.7438659667969\n",
      "1550.52001953125 -363.4654235839844 -363.7394104003906 -363.7139892578125 -363.71282958984375 -363.712890625\n",
      "1542.7999267578125 -363.4862060546875 -363.73785400390625 -363.7669372558594 -363.7791442871094 -363.7790832519531\n",
      "1555.760009765625 -363.4915466308594 -363.739013671875 -363.7605895996094 -363.7158508300781 -363.7158508300781\n",
      "1550.419921875 -363.49505615234375 -363.7388916015625 -363.7706298828125 -363.72265625 -363.7226867675781\n",
      "1556.909912109375 -363.4996032714844 -363.7388610839844 -363.75762939453125 -363.7569885253906 -363.7569580078125\n",
      "1564.510009765625 -363.4996032714844 -363.7397766113281 -363.7225341796875 -363.798095703125 -363.7981262207031\n",
      "1554.679931640625 -363.5036926269531 -363.7403564453125 -363.7143249511719 -363.74761962890625 -363.7476806640625\n",
      "1550.719970703125 -363.515380859375 -363.7397155761719 -363.7192687988281 -363.71636962890625 -363.7164306640625\n",
      "1546.4000244140625 -363.5238952636719 -363.7376403808594 -363.7242431640625 -363.7275390625 -363.72760009765625\n",
      "1545.8699951171875 -363.5236511230469 -363.738037109375 -363.71746826171875 -363.7751770019531 -363.7751770019531\n",
      "1543.429931640625 -363.5197448730469 -363.7388000488281 -363.76190185546875 -363.71539306640625 -363.7154235839844\n",
      "1523.219970703125 -363.51202392578125 -363.739990234375 -363.760009765625 -363.7608337402344 -363.7608337402344\n",
      "1519.2799072265625 -363.515625 -363.7393798828125 -363.75006103515625 -363.720458984375 -363.72027587890625\n",
      "1521.75 -363.5143127441406 -363.7380676269531 -363.7393798828125 -363.7309875488281 -363.7309875488281\n",
      "1504.489990234375 -363.5096130371094 -363.7400817871094 -363.74713134765625 -363.74481201171875 -363.7447814941406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1462.8399658203125 -363.5110168457031 -363.74053955078125 -363.774658203125 -363.72705078125 -363.72705078125\n",
      "1427.18994140625 -363.5149841308594 -363.738525390625 -363.7469482421875 -363.7686462402344 -363.7687072753906\n",
      "1390.8499755859375 -363.5128479003906 -363.739990234375 -363.725830078125 -363.7421569824219 -363.7422180175781\n",
      "1370.239990234375 -363.51318359375 -363.74017333984375 -363.71075439453125 -363.7349853515625 -363.73504638671875\n",
      "1346.699951171875 -363.5071105957031 -363.7390441894531 -363.7453918457031 -363.70751953125 -363.7076416015625\n",
      "1335.4599609375 -363.49505615234375 -363.73980712890625 -363.7300109863281 -363.7015380859375 -363.7015075683594\n",
      "1318.1199951171875 -363.4812316894531 -363.73773193359375 -363.75933837890625 -363.6982421875 -363.6982727050781\n",
      "1312.949951171875 -363.4652404785156 -363.73870849609375 -363.73779296875 -363.75201416015625 -363.7519836425781\n",
      "1317.909912109375 -363.4122619628906 -363.73822021484375 -363.7687683105469 -363.7096252441406 -363.70953369140625\n",
      "1319.75 -363.36279296875 -363.7395935058594 -363.7977294921875 -363.70489501953125 -363.7049560546875\n",
      "1324.3399658203125 -363.3055114746094 -363.739013671875 -363.69036865234375 -363.7593994140625 -363.7593994140625\n",
      "1325.8800048828125 -363.2651062011719 -363.739501953125 -363.66290283203125 -363.72222900390625 -363.7221984863281\n",
      "1331.75 -363.22216796875 -363.738525390625 -363.7486572265625 -363.703369140625 -363.70343017578125\n",
      "1352.909912109375 -363.16937255859375 -363.73980712890625 -363.80377197265625 -363.7513732910156 -363.7514343261719\n",
      "1365.409912109375 -363.13494873046875 -363.7366027832031 -363.77801513671875 -363.740234375 -363.7402038574219\n",
      "1355.1300048828125 -363.09228515625 -363.7350158691406 -363.7197265625 -363.6725158691406 -363.6725158691406\n",
      "1347.969970703125 -363.0044860839844 -363.7373046875 -363.79644775390625 -363.74871826171875 -363.74871826171875\n",
      "1355.0899658203125 -362.8920593261719 -363.732421875 -363.69696044921875 -363.8599548339844 -363.8599548339844\n",
      "1344.280029296875 -362.6183776855469 -363.73382568359375 -363.91192626953125 -363.65447998046875 -363.6543884277344\n",
      "1335.5599365234375 -362.1167907714844 -363.72198486328125 -363.63983154296875 -363.6948547363281 -363.69482421875\n",
      "1321.22998046875 -361.3563232421875 -363.7118835449219 -363.6748962402344 -363.7828063964844 -363.7827453613281\n",
      "1316.77001953125 -360.6102294921875 -363.70458984375 -363.23736572265625 -363.01971435546875 -363.0196838378906\n",
      "1316.9599609375 -359.3506164550781 -363.62615966796875 -363.7253112792969 -363.719970703125 -363.7198791503906\n",
      "1332.260009765625 -356.0765686035156 -363.33447265625 -364.8677673339844 -363.9896240234375 -363.9895935058594\n",
      "1345.919921875 -346.2878723144531 -361.8526306152344 -362.7461853027344 -359.34283447265625 -359.34271240234375\n",
      "1337.5599365234375 -330.89410400390625 -356.7296142578125 -352.64910888671875 -356.16790771484375 -356.1678161621094\n",
      "1325.6099853515625 -285.2215270996094 -327.67498779296875 -330.67718505859375 -326.2127685546875 -326.21270751953125\n",
      "1350.2999267578125 -239.49575805664062 -284.9362487792969 -287.44281005859375 -282.30072021484375 -282.30072021484375\n",
      "1325.5999755859375 -280.7475280761719 -316.37646484375 -316.27972412109375 -323.623779296875 -323.6238098144531\n",
      "1301.97998046875 -288.0984191894531 -326.6899108886719 -335.8707275390625 -335.905029296875 -335.9051513671875\n",
      "1296.239990234375 -344.1510925292969 -360.8923034667969 -359.32403564453125 -360.66680908203125 -360.66680908203125\n",
      "1287.739990234375 -357.7412414550781 -363.507080078125 -363.1607666015625 -363.3719482421875 -363.3719787597656\n",
      "1285.3800048828125 -361.49273681640625 -363.71636962890625 -363.56988525390625 -364.02490234375 -364.0248718261719\n",
      "1285.949951171875 -362.8598327636719 -363.7369689941406 -363.70989990234375 -363.625732421875 -363.6257019042969\n",
      "1290.3499755859375 -363.4052734375 -363.7388916015625 -363.7059326171875 -363.7626647949219 -363.7625732421875\n",
      "1303.5699462890625 -363.6143798828125 -363.73822021484375 -363.7328796386719 -363.747802734375 -363.747802734375\n",
      "1382.22998046875 -363.69573974609375 -363.7393798828125 -363.7464294433594 -363.7349853515625 -363.7350769042969\n",
      "1454.909912109375 -363.72161865234375 -363.7395324707031 -363.74200439453125 -363.74139404296875 -363.7414855957031\n",
      "1564.7099609375 -363.7204895019531 -363.73956298828125 -363.739501953125 -363.7427673339844 -363.7427673339844\n",
      "1658.5499267578125 -363.7185363769531 -363.7393798828125 -363.73626708984375 -363.73748779296875 -363.7374572753906\n",
      "1729.18994140625 -363.71905517578125 -363.7392883300781 -363.73760986328125 -363.7411804199219 -363.7412414550781\n",
      "1745.8800048828125 -363.71966552734375 -363.7394714355469 -363.74017333984375 -363.7347412109375 -363.73480224609375\n",
      "1724.0699462890625 -363.7213134765625 -363.73931884765625 -363.7379150390625 -363.7396240234375 -363.73968505859375\n",
      "1707.18994140625 -363.7232666015625 -363.7394714355469 -363.73687744140625 -363.73883056640625 -363.73876953125\n",
      "1698.449951171875 -363.72479248046875 -363.7392272949219 -363.73760986328125 -363.74151611328125 -363.7415771484375\n",
      "1682.179931640625 -363.7259826660156 -363.7394714355469 -363.7381896972656 -363.740966796875 -363.740966796875\n",
      "1679.760009765625 -363.7270202636719 -363.7393798828125 -363.739990234375 -363.7380065917969 -363.7380065917969\n",
      "1668.6300048828125 -363.72784423828125 -363.73931884765625 -363.7386474609375 -363.741943359375 -363.7419128417969\n",
      "1669.1400146484375 -363.7285461425781 -363.7394104003906 -363.7375183105469 -363.738525390625 -363.73846435546875\n"
     ]
    }
   ],
   "source": [
    "train(epochs = 300000, npoints = 104, batchsize = 100, nsamples = 8)\n",
    "#big, binary loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "numerous-narrow",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 282.00 MiB (GPU 1; 15.90 GiB total capacity; 14.24 GiB already allocated; 237.38 MiB free; 14.77 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-721c1ac60edd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-0d990368ca7d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, npoints, batchsize, nsamples)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;34m\"\"\"include the boundary points, kinda makes sense that they should contribute (atm only in the encoder, difficult to see how in the decoder)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#[npoints, batchsize * nsamples, emsize]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m         \u001b[0;31m#### #### #### remember to include tgt.detach() when reinstate with torch.no_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_token\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnsamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#[1, batchsize * nsamples, emsize]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-0d990368ca7d>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#src must be [batchsize * nsamples, npoints, 3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;31m#[npoints, batchsize * nsamples, emsize]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/software/Conda/conda_environments/pytorch-1.8.0-cuda10.2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/software/Conda/conda_environments/pytorch-1.8.0-cuda10.2/lib/python3.8/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/software/Conda/conda_environments/pytorch-1.8.0-cuda10.2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/software/Conda/conda_environments/pytorch-1.8.0-cuda10.2/lib/python3.8/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0msrc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/software/Conda/conda_environments/pytorch-1.8.0-cuda10.2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/software/Conda/conda_environments/pytorch-1.8.0-cuda10.2/lib/python3.8/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/software/Conda/conda_environments/pytorch-1.8.0-cuda10.2/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 282.00 MiB (GPU 1; 15.90 GiB total capacity; 14.24 GiB already allocated; 237.38 MiB free; 14.77 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "train(epochs = 300000, npoints = 9, batchsize = 4000, nsamples = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "objective-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(NN.state_dict(),'3D_5points_11.45_fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-fault",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
