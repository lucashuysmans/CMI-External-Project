{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surgical-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "device = \"cuda:0\"\n",
    "floattype = torch.float\n",
    "\n",
    "batchsize = 512\n",
    "nsamples = 8\n",
    "npoints = 5\n",
    "emsize = 512\n",
    "\n",
    "\n",
    "class Graph_Transformer(nn.Module):\n",
    "    def __init__(self, emsize = 64, nhead = 8, nhid = 1024, nlayers = 3, ndecoderlayers = 0, dropout = 0.3):\n",
    "        super().__init__()\n",
    "        self.emsize = emsize\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "        encoder_layers = TransformerEncoderLayer(emsize, nhead, nhid, dropout = dropout)\n",
    "        decoder_layers = TransformerDecoderLayer(emsize, nhead, nhid, dropout = dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layers, ndecoderlayers)\n",
    "        self.encoder = nn.Linear(3, emsize)\n",
    "        self.outputattention_query = nn.Linear(emsize, emsize, bias = False)\n",
    "        self.outputattention_key = nn.Linear(emsize, emsize, bias = False)\n",
    "        self.start_token = nn.Parameter(torch.randn([emsize], device = device))\n",
    "    \n",
    "    def generate_subsequent_mask(self, sz): #last dimension will be softmaxed over when adding to attention logits, if boolean the ones turn into -inf\n",
    "        #mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        #mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        \n",
    "        mask = torch.triu(torch.ones([sz, sz], dtype = torch.bool, device = device), diagonal = 1)\n",
    "        return mask\n",
    "    \n",
    "    def encode(self, src): #src must be [batchsize * nsamples, npoints, 3]\n",
    "        src = self.encoder(src).transpose(0, 1)\n",
    "        output = self.transformer_encoder(src)\n",
    "        return output #[npoints, batchsize * nsamples, emsize]\n",
    "    \n",
    "    def decode_next(self, memory, tgt, route_mask): #route mask is [batchsize * nsamples, npoints], both memory and tgt must have batchsize and nsamples in same dimension (the 1th one)\n",
    "        npoints = memory.size(0)\n",
    "        batchsize = tgt.size(1)\n",
    "        \"\"\"if I really wanted this to be efficient I'd only recompute the decoder for the last tgt, and just remebering what the others looked like from before (won't change due to mask)\"\"\"\n",
    "        \"\"\"have the option to freeze the autograd on all but the last part of tgt, although at the moment this is a very natural way to say: initial choices matter more\"\"\"\n",
    "        tgt_mask = self.generate_subsequent_mask(tgt.size(0))\n",
    "        output = self.transformer_decoder(tgt, memory, tgt_mask) #[tgt, batchsize * nsamples, emsize]\n",
    "        output_query = self.outputattention_query(memory).transpose(0, 1) #[batchsize * nsamples, npoints, emsize]\n",
    "        output_key = self.outputattention_key(output[-1]) #[batchsize * nsamples, emsize]\n",
    "        output_attention = torch.matmul(output_query * self.emsize ** -0.5, output_key.unsqueeze(-1)).squeeze(-1) #[batchsize * nsamples, npoints], technically don't need to scale attention as we divide by variance next anyway\n",
    "        output_attention_tanh = output_attention.tanh() #[batchsize * nsamples, npoints]\n",
    "        \n",
    "        #we clone the route_mask incase we want to backprop using it (else it was modified by inplace opporations)\n",
    "        output_attention = output_attention.masked_fill(route_mask.clone(), float('-inf')) #[batchsize * nsamples, npoints]\n",
    "        output_attention_tanh = output_attention_tanh.masked_fill(route_mask.clone(), float('-inf')) #[batchsize * nsamples, npoints]\n",
    "        \n",
    "        return output_attention_tanh, output_attention #[batchsize * nsamples, npoints]\n",
    "    \n",
    "    def calculate_logprob(self, memory, routes): #memory is [npoints, batchsize * nsamples, emsize], routes is [batchsize * nsamples, npoints - 4], rather than backproping the entire loop, this saves vram (and computation)\n",
    "        npoints = memory.size(0)\n",
    "        ninternalpoints = routes.size(1)\n",
    "        bigbatchsize = memory.size(1)\n",
    "        memory_ = memory.gather(0, routes.transpose(0, 1).unsqueeze(2).expand(-1, -1, self.emsize)) #[npoints - 4, batchsize * nsamples, emsize] reorder memory into order of routes\n",
    "        tgt = torch.cat([self.start_token.unsqueeze(0).unsqueeze(1).expand(1, bigbatchsize, -1), memory_[:-1]]) #[npoints - 4, batchsize * nroutes, emsize], want to go from memory to tgt\n",
    "        tgt_mask = self.generate_subsequent_mask(ninternalpoints)\n",
    "        output = self.transformer_decoder(tgt, memory, tgt_mask) #[npoints - 4, batchsize * nsamples, emsize]\n",
    "        \"\"\"want probability of going from key to query, but first need to normalise (softmax with mask)\"\"\"\n",
    "        output_query = self.outputattention_query(memory_).transpose(0, 1) #[batchsize * nsamples, npoints - 4, emsize]\n",
    "        output_key = self.outputattention_key(output).transpose(0, 1) #[batchsize * nsamples, npoints - 4, emsize]\n",
    "        attention_mask = torch.full([ninternalpoints, ninternalpoints], True, device = device).triu(1) #[npoints - 4, npoints - 4], True for i < j\n",
    "        output_attention = torch.matmul(output_query * self.emsize ** -0.5, output_key.transpose(-1, -2))\n",
    "        \"\"\"quick fix to stop divergence\"\"\"\n",
    "        output_attention_tanh = output_attention.tanh()\n",
    "        \n",
    "        output_attention_tanh = output_attention_tanh.masked_fill(attention_mask, float('-inf'))\n",
    "        output_attention_tanh = output_attention_tanh - output_attention_tanh.logsumexp(-2, keepdim = True) #[batchsize * nsamples, npoints - 4, npoints - 4]\n",
    "        \n",
    "        output_attention = output_attention.masked_fill(attention_mask, float('-inf'))\n",
    "        output_attention = output_attention - output_attention.logsumexp(-2, keepdim = True) #[batchsize * nsamples, npoints - 4, npoints - 4]\n",
    "        \n",
    "        \"\"\"infact I'm almost tempted to not mask choosing a previous point, so it's forced to learn it and somehow incorporate it into its computation, but without much impact on reinforcing good examples\"\"\"\n",
    "        logprob_tanh = output_attention_tanh.diagonal(dim1 = -1, dim2 = -2).sum(-1) #[batchsize * nsamples]\n",
    "        logprob = output_attention.diagonal(dim1 = -1, dim2 = -2).sum(-1) #[batchsize * nsamples]\n",
    "        return logprob_tanh, logprob #[batchsize * nsamples]\n",
    "\n",
    "NN = Graph_Transformer().to(device)\n",
    "optimizer = optim.Adam(NN.parameters())\n",
    "\n",
    "\n",
    "class environment:    \n",
    "    def reset(self, npoints, batchsize, nsamples=1, corner_points = None, initial_triangulation = None):\n",
    "        \"\"\"\n",
    "        corner_points, etc., shoudn't include a batch dimension\n",
    "        \"\"\"\n",
    "        if corner_points == None:\n",
    "            ncornerpoints = 4\n",
    "        else:\n",
    "            ncornerpoints = corner_points.size(0)\n",
    "        if npoints <= ncornerpoints:\n",
    "            print(\"Error: not enough points for valid problem instance\")\n",
    "            return\n",
    "        self.batchsize = (\n",
    "            batchsize * nsamples\n",
    "        )  # so that I don't have to rewrite all this code, we store these two dimensions together\n",
    "        self.nsamples = nsamples\n",
    "        self.npoints = npoints\n",
    "        self.points = (\n",
    "            torch.rand([batchsize, npoints - ncornerpoints, 3], dtype = floattype, device=device)\n",
    "            .unsqueeze(1)\n",
    "            .expand(-1, nsamples, -1, -1)\n",
    "            .reshape(self.batchsize, npoints - ncornerpoints, 3)\n",
    "        )\n",
    "        if corner_points == None:\n",
    "            self.corner_points = torch.tensor(\n",
    "                [[0, 0, 0], [3, 0, 0], [0, 3, 0], [0, 0, 3]], dtype = floattype, device=device\n",
    "            )\n",
    "        else:\n",
    "            self.corner_points = corner_points\n",
    "        self.points = torch.cat(\n",
    "            [\n",
    "                self.corner_points.unsqueeze(0).expand(self.batchsize, -1, -1),\n",
    "                self.points,\n",
    "            ],\n",
    "            dim=-2,\n",
    "        )  # [batchsize * nsamples, npoints, 3]\n",
    "        self.points_mask = torch.cat(\n",
    "            [\n",
    "                torch.ones([self.batchsize, ncornerpoints], dtype=torch.bool, device=device),\n",
    "                torch.zeros(\n",
    "                    [self.batchsize, npoints - ncornerpoints], dtype=torch.bool, device=device\n",
    "                ),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        self.points_sequence = torch.empty(\n",
    "            [self.batchsize, 0], dtype=torch.long, device=device\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        points are now triples\n",
    "        triangles are now quadruples\n",
    "        edges are now still just indices, but there are four of them per 'triangle', and they correspond to triples of points, not pairs\n",
    "        we use  0,2,1  0,3,2  0,1,3  1,2,3  as the order of the four 'edges'/faces\n",
    "        opposite face is always ordered such that the last two indices are swapped\n",
    "        faces are always read ANTICLOCKWISE\n",
    "        \n",
    "        first three points of tetrahedron MUST be read clockwise (from the outside) to get correct sign on incircle test\n",
    "        \n",
    "        new point will be inserted in zeroth position, so if corresponding face of REMOVED tetrahedron is [x,y,z] (being read anticlockwise from outside in) new tetrahedron is [p, x, y, z]\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        number of tetrahedra is not the same for each batch (in 3D), so store as a big list, and remember batch index that it comes from\n",
    "        \"\"\"\n",
    "        if corner_points == None:\n",
    "            initial_triangulation = torch.tensor([[0, 1, 2, 3]], dtype=torch.long, device=device)\n",
    "        \n",
    "        self.partial_delaunay_triangles = initial_triangulation.unsqueeze(0).expand(self.batchsize, -1, -1).reshape(-1, 4)\n",
    "        self.batch_index = torch.arange(self.batchsize, dtype = torch.long, device = device).unsqueeze(1).expand(-1, initial_triangulation.size(0)).reshape(-1)\n",
    "        \n",
    "        self.batch_triangles = self.partial_delaunay_triangles.size(0) #[0]\n",
    "        self.ntriangles = torch.full([self.batchsize], initial_triangulation.size(0), dtype = torch.long, device = device) #[self.batchsize]\n",
    "        \n",
    "        self.cost = torch.zeros([self.batchsize], dtype = floattype, device=device)\n",
    "\n",
    "        self.logprob = torch.zeros([self.batchsize], dtype = floattype, device=device, requires_grad=True)\n",
    "\n",
    "    def update(self, point_index):  # point_index is [batchsize]\n",
    "        \n",
    "        assert point_index.size(0) == self.batchsize\n",
    "        assert str(point_index.device) == device\n",
    "        assert self.points_mask.gather(1, point_index.unsqueeze(1)).sum() == 0\n",
    "        \n",
    "        triangles_coordinates = self.points[self.batch_index.unsqueeze(1), self.partial_delaunay_triangles] # [batch_triangles, 4, 3]\n",
    "        \n",
    "        newpoint = self.points[self.batch_index, point_index[self.batch_index]] # [batch_triangles, 3]\n",
    "\n",
    "        incircle_matrix = torch.cat(\n",
    "            [\n",
    "                newpoint.unsqueeze(1),\n",
    "                triangles_coordinates,\n",
    "            ],\n",
    "            dim=-2,\n",
    "        )  # [batch_triangles, 5, 3]\n",
    "        incircle_matrix = torch.cat(\n",
    "            [\n",
    "                (incircle_matrix * incircle_matrix).sum(-1, keepdim=True),\n",
    "                incircle_matrix,\n",
    "                torch.ones([self.batch_triangles, 5, 1], dtype = floattype, device=device),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )  # [batch_triangles, 5, 5]\n",
    "        assert incircle_matrix.dtype == floattype\n",
    "        assert str(incircle_matrix.device) == device\n",
    "        \n",
    "        incircle_test = (\n",
    "            incircle_matrix.det() > 0\n",
    "        )  # [batch_triangles], is True if inside incircle\n",
    "        \n",
    "        conflicts = incircle_test.sum()\n",
    "        \n",
    "        conflicting_triangles = self.partial_delaunay_triangles[incircle_test] # [conflicts, 4]\n",
    "        \n",
    "        conflicting_edges_index0 = torch.empty_like(conflicting_triangles)\n",
    "        indices = torch.LongTensor([0, 0, 0, 1])\n",
    "        conflicting_edges_index0 = conflicting_triangles[:, indices] # [conflicts, 4]\n",
    "        \n",
    "        conflicting_edges_index1 = torch.empty_like(conflicting_triangles)\n",
    "        indices = torch.LongTensor([2, 3, 1, 2])\n",
    "        conflicting_edges_index1 = conflicting_triangles[:, indices] # [conflicts, 4]\n",
    "        \n",
    "        conflicting_edges_index2 = torch.empty_like(conflicting_triangles)\n",
    "        indices = torch.LongTensor([1, 2, 3, 3])\n",
    "        conflicting_edges_index2 = conflicting_triangles[:, indices] # [conflicts, 4]\n",
    "        \n",
    "        conflicting_edges = torch.cat([conflicting_edges_index0.view(-1).unsqueeze(-1), conflicting_edges_index1.view(-1).unsqueeze(-1), conflicting_edges_index2.view(-1).unsqueeze(-1)], dim = -1).reshape(-1, 3) # [conflicts * 4, 3]\n",
    "        \n",
    "        edge_batch_index = self.batch_index[incircle_test].unsqueeze(1).expand(-1, 4).reshape(-1) # [conflicts * 4]\n",
    "        \n",
    "        indices = torch.LongTensor([0, 2, 1])\n",
    "        comparison_edges = conflicting_edges[:, indices] # [conflicts * 4, 3]        \n",
    "        \n",
    "        unravel_nomatch_mask = torch.ones([conflicts * 4], dtype = torch.bool, device = device) # [conflicts * 4]\n",
    "        i = 1\n",
    "        while True:\n",
    "            \n",
    "            todo_mask = unravel_nomatch_mask[:-i].logical_and(edge_batch_index[:-i] == edge_batch_index[i:])\n",
    "            if i % 4 == 0:\n",
    "                if todo_mask.sum() == 0:\n",
    "                    break\n",
    "            \n",
    "            match_mask = todo_mask.clone()\n",
    "            match_mask[todo_mask] = (conflicting_edges[:-i][todo_mask] != comparison_edges[i:][todo_mask]).sum(-1).logical_not()\n",
    "            \n",
    "            unravel_nomatch_mask[:-i][match_mask] = False\n",
    "            unravel_nomatch_mask[i:][match_mask] = False\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        batch_newtriangles = unravel_nomatch_mask.sum()\n",
    "        \n",
    "        nomatch_edges = conflicting_edges[unravel_nomatch_mask] # [batch_newtriangles, 3], already in correct order to insert into 1,2,3 (since already anticlockwise from outside in)\n",
    "        assert list(nomatch_edges.size()) == [batch_newtriangles, 3]\n",
    "        nomatch_batch_index = edge_batch_index[unravel_nomatch_mask] # [batch_newtriangles]\n",
    "        \n",
    "        nomatch_newpoint = point_index[nomatch_batch_index] # [batch_newtriangles]\n",
    "        \n",
    "        newtriangles = torch.cat([nomatch_newpoint.unsqueeze(1), nomatch_edges], dim = -1) # [batch_newtriangles, 4]\n",
    "        \n",
    "        \n",
    "        nremoved_triangles = torch.zeros([self.batchsize], dtype = torch.long, device = device)\n",
    "        nnew_triangles = torch.zeros([self.batchsize], dtype = torch.long, device = device)\n",
    "        \n",
    "        indices = self.batch_index[incircle_test]\n",
    "        nremoved_triangles.put_(indices, torch.ones_like(indices, dtype = torch.long), accumulate = True) # [batchsize]\n",
    "        \n",
    "        indices = edge_batch_index[unravel_nomatch_mask]\n",
    "        nnew_triangles.put_(indices, torch.ones_like(indices, dtype = torch.long), accumulate = True) # [batchsize]\n",
    "        \n",
    "        assert (nnew_triangles <= 2 * nremoved_triangles + 2).logical_not().sum().logical_not()\n",
    "        \n",
    "        \"\"\"\n",
    "        NOTE:\n",
    "        I THINK it's possible for nnew_triangles to be less than nremoved_triangles (or my code is just buggy...)\n",
    "        \"\"\"\n",
    "        \n",
    "        assert nnew_triangles.sum() == batch_newtriangles\n",
    "        assert nremoved_triangles.sum() == incircle_test.sum()\n",
    "        \n",
    "        nadditional_triangles = nnew_triangles - nremoved_triangles # [batchsize]\n",
    "        ntriangles = self.ntriangles + nadditional_triangles # [batchsize]\n",
    "        \n",
    "        partial_delaunay_triangles = torch.empty([ntriangles.sum(), 4], dtype = torch.long, device = device)\n",
    "        batch_index = torch.empty([ntriangles.sum()], dtype = torch.long, device = device)\n",
    "        \n",
    "        cumulative_triangles = torch.cat([torch.zeros([1], dtype = torch.long, device = device), nnew_triangles.cumsum(0)[:-1]]) # [batchsize], cumulative sum starts at zero\n",
    "        \n",
    "        \"\"\"\n",
    "        since may actually have LESS triangles than previous round, we insert all that survive into the first slots (in that batch)\n",
    "        \"\"\"\n",
    "        good_triangle_indices = torch.arange(incircle_test.logical_not().sum(), dtype = torch.long, device = device)\n",
    "        good_triangle_indices += cumulative_triangles[self.batch_index[incircle_test.logical_not()]]\n",
    "        bad_triangle_indices_mask = torch.ones([ntriangles.sum(0)], dtype = torch.bool, device = device)\n",
    "        bad_triangle_indices_mask.scatter_(0, good_triangle_indices, False)\n",
    "        \n",
    "        assert good_triangle_indices.size(0) == incircle_test.logical_not().sum()\n",
    "        assert bad_triangle_indices_mask.sum() == batch_newtriangles\n",
    "        \n",
    "        partial_delaunay_triangles[good_triangle_indices] = self.partial_delaunay_triangles[~incircle_test]\n",
    "        batch_index[good_triangle_indices] = self.batch_index[~incircle_test]\n",
    "        \n",
    "        partial_delaunay_triangles[bad_triangle_indices_mask] = newtriangles\n",
    "        batch_index[bad_triangle_indices_mask] = nomatch_batch_index\n",
    "        \n",
    "        self.partial_delaunay_triangles = partial_delaunay_triangles\n",
    "        self.batch_index = batch_index\n",
    "        \n",
    "        self.ntriangles = ntriangles\n",
    "        self.batch_triangles = self.partial_delaunay_triangles.size(0)\n",
    "        \n",
    "        self.points_mask.scatter_(\n",
    "            1, point_index.unsqueeze(1).expand(-1, self.npoints), True\n",
    "        )\n",
    "        self.points_sequence = torch.cat(\n",
    "            [self.points_sequence, point_index.unsqueeze(1)], dim=1\n",
    "        )\n",
    "        \n",
    "        self.cost += nremoved_triangles\n",
    "        return\n",
    "    \n",
    "    def sample_point(self, logits): #logits must be [batchsize * nsamples, npoints]\n",
    "        probs = torch.distributions.categorical.Categorical(logits = logits)\n",
    "        next_point = probs.sample() #size is [batchsize * nsamples]\n",
    "        self.update(next_point)\n",
    "        self.logprob = self.logprob + probs.log_prob(next_point)\n",
    "        return next_point #[batchsize * nsamples]\n",
    "    \n",
    "    def sampleandgreedy_point(self, logits): #logits must be [batchsize * nsamples, npoints], last sample will be the greedy choice (but we still need to keep track of its logits)\n",
    "        logits_sample = logits.view(-1, self.nsamples, self.npoints)[:, :-1, :]\n",
    "        probs = torch.distributions.categorical.Categorical(logits = logits_sample)\n",
    "        \n",
    "        sample_point = probs.sample() #[batchsize, (nsamples - 1)]\n",
    "        greedy_point = logits.view(-1, self.nsamples, self.npoints)[:, -1, :].max(-1, keepdim = True)[1] #[batchsize, 1]\n",
    "        next_point = torch.cat([sample_point, greedy_point], dim = 1).view(-1)\n",
    "        self.update(next_point)\n",
    "        self.logprob = self.logprob + torch.cat([probs.log_prob(sample_point), torch.zeros([sample_point.size(0), 1], device = device)], dim = 1).view(-1)\n",
    "        return next_point\n",
    "    \n",
    "\n",
    "env = environment()\n",
    "\n",
    "\n",
    "def train(epochs = 30000, npoints = 14, batchsize = 100, nsamples = 8):\n",
    "    NN.train()\n",
    "    for i in range(epochs):\n",
    "        env.reset(npoints, batchsize, nsamples)\n",
    "        \"\"\"include the boundary points, kinda makes sense that they should contribute (atm only in the encoder, difficult to see how in the decoder)\"\"\"\n",
    "        memory = NN.encode(env.points) #[npoints, batchsize * nsamples, emsize]\n",
    "        #### #### #### remember to include tgt.detach() when reinstate with torch.no_grad()\n",
    "        tgt = NN.start_token.unsqueeze(0).unsqueeze(1).expand(1, batchsize * nsamples, -1).detach() #[1, batchsize * nsamples, emsize]\n",
    "        #with torch.no_grad(): #to speed up computation, selecting routes is done without gradient\n",
    "        with torch.no_grad():\n",
    "            for j in range(4, npoints):\n",
    "                #### #### #### remember to include memory.detach() when reinstate with torch.no_grad()\n",
    "                _, logits = NN.decode_next(memory.detach(), tgt, env.points_mask)\n",
    "                next_point = env.sampleandgreedy_point(logits)\n",
    "                \"\"\"\n",
    "                for inputing the previous embedding into decoder\n",
    "                \"\"\"\n",
    "                tgt = torch.cat([tgt, memory.gather(0, next_point.unsqueeze(0).unsqueeze(2).expand(1, -1, memory.size(2)))]) #[nsofar, batchsize * nsamples, emsize]\n",
    "                \"\"\"\n",
    "                for inputing the previous decoder output into the decoder (allows for an evolving strategy, but doesn't allow for fast training\n",
    "                \"\"\"\n",
    "                ############\n",
    "\n",
    "        \n",
    "        NN.eval()\n",
    "        _, logprob = NN.calculate_logprob(memory, env.points_sequence) #[batchsize * nsamples]\n",
    "        NN.train()\n",
    "        \"\"\"\n",
    "        clip logprob so doesn't reinforce things it already knows\n",
    "        TBH WANT SOMETHING DIFFERENT ... want to massively increase training if find something unexpected and otherwise not\n",
    "        \"\"\"\n",
    "        greedy_prob = logprob.view(batchsize, nsamples)[:, -1].detach() #[batchsize]\n",
    "        greedy_baseline = env.cost.view(batchsize, nsamples)[:, -1] #[batchsize], greedy sample\n",
    "        fixed_baseline = 0.5 * torch.ones([1], device = device)\n",
    "        min_baseline = env.cost.view(batchsize, nsamples)[:, :-1].min(-1)[0] #[batchsize], minimum cost\n",
    "        baseline = greedy_baseline\n",
    "        positive_reinforcement = - F.relu( - (env.cost.view(batchsize, nsamples)[:, :-1] - baseline.unsqueeze(1))) #don't scale positive reinforcement\n",
    "        negative_reinforcement = F.relu(env.cost.view(batchsize, nsamples)[:, :-1] - baseline.unsqueeze(1))\n",
    "        positive_reinforcement_binary = env.cost.view(batchsize, nsamples)[:, :-1] - baseline.unsqueeze(1) <= -0.05\n",
    "        negative_reinforcement_binary = env.cost.view(batchsize, nsamples)[:, :-1] - baseline.unsqueeze(1) > 1\n",
    "        \"\"\"\n",
    "        binary positive reinforcement\n",
    "        \"\"\"\n",
    "        #loss = - ((logprob.view(batchsize, nsamples)[:, :-1] < -0.2) * logprob.view(batchsize, nsamples)[:, :-1] * positive_reinforcement_binary).mean() #+ (logprob.view(batchsize, nsamples)[:, :-1] > -1) * logprob.view(batchsize, nsamples)[:, :-1] * negative_reinforcement_binary\n",
    "        \"\"\"\n",
    "        clipped binary reinforcement\n",
    "        \"\"\"\n",
    "        loss = ( \n",
    "                #- logprob.view(batchsize, nsamples)[:, :-1] \n",
    "                #* (logprob.view(batchsize, nsamples)[:, :-1] < 0) \n",
    "                #* positive_reinforcement_binary \n",
    "                logprob.view(batchsize, nsamples)[:, :-1] \n",
    "                * (logprob.view(batchsize, nsamples)[:, :-1] > greedy_prob.unsqueeze(1) - 2) \n",
    "                * negative_reinforcement_binary \n",
    "        ).mean()\n",
    "        \"\"\"\n",
    "        clipped binary postive, clipped weighted negative\n",
    "        \"\"\"\n",
    "        #loss = ( - logprob.view(batchsize, nsamples)[:, :-1] * (logprob.view(batchsize, nsamples)[:, :-1] < -0.2) * positive_reinforcement_binary + logprob.view(batchsize, nsamples)[:, :-1] * (logprob.view(batchsize, nsamples)[:, :-1] > -2) * negative_reinforcement ).mean()\n",
    "        \"\"\"\n",
    "        clipped reinforcement without rescaling\n",
    "        \"\"\"\n",
    "        #loss = ((logprob.view(batchsize, nsamples)[:, :-1] < -0.7) * logprob.view(batchsize, nsamples)[:, :-1] * positive_reinforcement + (logprob.view(batchsize, nsamples)[:, :-1] > -5) * logprob.view(batchsize, nsamples)[:, :-1] * negative_reinforcement).mean()\n",
    "        \"\"\"\n",
    "        clipped reinforcement\n",
    "        \"\"\"\n",
    "        #loss = (logprob.view(batchsize, nsamples)[:, :-1] * positive_reinforcement / (positive_reinforcement.var() + 0.001).sqrt() + (logprob.view(batchsize, nsamples)[:, :-1] > -3) * logprob.view(batchsize, nsamples)[:, :-1] * negative_reinforcement / (negative_reinforcement.var() + 0.001).sqrt()).mean()\n",
    "        \"\"\"\n",
    "        balanced reinforcement\n",
    "        \"\"\"\n",
    "        #loss = (logprob.view(batchsize, nsamples)[:, :-1] * (positive_reinforcement / (positive_reinforcement.var() + 0.001).sqrt() + negative_reinforcement / (negative_reinforcement.var() + 0.001).sqrt())).mean()\n",
    "        \"\"\"\n",
    "        regular loss\n",
    "        \"\"\"\n",
    "        #loss = (logprob.view(batchsize, nsamples)[:, :-1] * (positive_reinforcement + negative_reinforcement)).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #print(NN.encoder.weight.grad)\n",
    "        optimizer.step()\n",
    "        #print(greedy_baseline.mean().item())\n",
    "        print(greedy_baseline.mean().item(), logprob.view(batchsize, nsamples)[:, -1].mean().item(), logprob.view(batchsize, nsamples)[:, :-1].mean().item(), logprob[batchsize - 1].item(), logprob[0].item(), env.logprob[0].item())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-ready",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.02125072479248 -4.351661205291748 -4.760155200958252 -4.528672695159912 -4.659603118896484 -4.659603595733643\n",
      "14.401750564575195 -4.176693916320801 -4.723328113555908 -4.209637641906738 -4.334877014160156 -4.334877014160156\n",
      "13.952000617980957 -3.726300001144409 -4.5507588386535645 -3.563929796218872 -4.786027908325195 -4.786027908325195\n",
      "13.893250465393066 -3.661702871322632 -4.549194812774658 -3.4905683994293213 -4.865384578704834 -4.865384578704834\n",
      "14.06725025177002 -3.9382221698760986 -4.66710090637207 -4.078761577606201 -4.738765716552734 -4.738765716552734\n",
      "13.961250305175781 -3.747201442718506 -4.584826469421387 -3.6950888633728027 -5.060266017913818 -5.060266494750977\n",
      "13.640750885009766 -3.3605170249938965 -4.401162147521973 -3.224963903427124 -4.933384895324707 -4.933385372161865\n",
      "13.749751091003418 -3.4763927459716797 -4.449087142944336 -3.439135789871216 -4.213818550109863 -4.213818550109863\n",
      "13.844250679016113 -3.6970417499542236 -4.551572799682617 -3.776081085205078 -5.949582099914551 -5.949582099914551\n",
      "13.858500480651855 -3.855482339859009 -4.630786418914795 -3.806579351425171 -4.884736061096191 -4.884736061096191\n",
      "13.78600025177002 -3.826336622238159 -4.616903781890869 -4.263462543487549 -4.923172473907471 -4.9231719970703125\n",
      "13.661500930786133 -3.636491298675537 -4.5534892082214355 -3.6642961502075195 -4.668636798858643 -4.668637275695801\n",
      "13.520000457763672 -3.4853148460388184 -4.491998195648193 -3.2722275257110596 -5.187458515167236 -5.187458038330078\n",
      "13.583500862121582 -3.459562063217163 -4.482143878936768 -3.3641130924224854 -4.459402084350586 -4.459402084350586\n",
      "13.594500541687012 -3.548530340194702 -4.526431083679199 -3.0743601322174072 -4.555235385894775 -4.555235862731934\n",
      "13.569750785827637 -3.646806478500366 -4.571891784667969 -3.5054802894592285 -3.8273587226867676 -3.8273587226867676\n",
      "13.610250473022461 -3.6940479278564453 -4.594698429107666 -4.091196060180664 -5.145939826965332 -5.145939350128174\n",
      "13.621500968933105 -3.6275863647460938 -4.584870338439941 -2.9915554523468018 -3.3818421363830566 -3.3818421363830566\n",
      "13.50475025177002 -3.5071117877960205 -4.543473243713379 -3.6041133403778076 -4.523503303527832 -4.52350378036499\n",
      "13.474000930786133 -3.438927412033081 -4.530998229980469 -3.3605551719665527 -4.628305912017822 -4.6283063888549805\n",
      "13.460500717163086 -3.4552829265594482 -4.543869972229004 -3.018799066543579 -5.513426780700684 -5.513427734375\n",
      "13.500250816345215 -3.534475564956665 -4.575793743133545 -4.030911922454834 -5.322223663330078 -5.322223663330078\n",
      "13.482000350952148 -3.63280987739563 -4.607124328613281 -3.190311908721924 -4.1267924308776855 -4.1267924308776855\n",
      "13.427000999450684 -3.6628780364990234 -4.612290859222412 -3.6106534004211426 -5.4782514572143555 -5.478250980377197\n",
      "13.362751007080078 -3.6223714351654053 -4.603015899658203 -4.000391960144043 -6.004372596740723 -6.004373073577881\n",
      "13.419500350952148 -3.5184240341186523 -4.563058376312256 -3.2465338706970215 -4.400599479675293 -4.400599479675293\n",
      "13.347250938415527 -3.4267706871032715 -4.527528762817383 -3.7707996368408203 -3.3535444736480713 -3.3535447120666504\n",
      "13.307750701904297 -3.4030447006225586 -4.512667655944824 -2.9140405654907227 -3.4291462898254395 -3.4291462898254395\n",
      "13.339750289916992 -3.4791789054870605 -4.5592732429504395 -3.4824466705322266 -3.0815536975860596 -3.0815532207489014\n",
      "13.358000755310059 -3.522871255874634 -4.573506832122803 -3.5334632396698 -3.4288814067840576 -3.4288816452026367\n",
      "13.36775016784668 -3.5682601928710938 -4.59523344039917 -3.5440568923950195 -4.628242492675781 -4.628242492675781\n",
      "13.338250160217285 -3.5843186378479004 -4.590849876403809 -3.7476487159729004 -5.712804794311523 -5.712804794311523\n",
      "13.29800033569336 -3.5582785606384277 -4.588816165924072 -3.3434834480285645 -4.141204357147217 -4.141204357147217\n",
      "13.202750205993652 -3.4905776977539062 -4.5639119148254395 -3.923612117767334 -4.722973823547363 -4.722973346710205\n",
      "13.287750244140625 -3.46172833442688 -4.555013179779053 -3.6462960243225098 -4.252782344818115 -4.252781867980957\n",
      "13.297000885009766 -3.452906608581543 -4.560587406158447 -3.642937183380127 -4.547503471374512 -4.5475029945373535\n",
      "13.33275032043457 -3.4530704021453857 -4.563293933868408 -3.9584715366363525 -3.132380247116089 -3.132380247116089\n",
      "13.281250953674316 -3.451718330383301 -4.566268444061279 -3.57692289352417 -5.034606456756592 -5.034606456756592\n",
      "13.243000984191895 -3.430623769760132 -4.559267044067383 -3.4848713874816895 -4.038815498352051 -4.038815498352051\n",
      "13.26400089263916 -3.457387924194336 -4.5610151290893555 -3.4260785579681396 -3.345996141433716 -3.345996618270874\n",
      "13.273750305175781 -3.5280537605285645 -4.585022926330566 -3.3326034545898438 -5.268710136413574 -5.268710136413574\n",
      "13.19675064086914 -3.5474581718444824 -4.598276615142822 -3.186591863632202 -3.7627789974212646 -3.7627789974212646\n",
      "13.204500198364258 -3.5351359844207764 -4.591835975646973 -3.703418731689453 -5.958081245422363 -5.958081245422363\n",
      "13.189750671386719 -3.4495558738708496 -4.553122043609619 -3.1869001388549805 -4.8276262283325195 -4.8276262283325195\n",
      "13.162250518798828 -3.38706111907959 -4.523697853088379 -3.2723870277404785 -4.465049743652344 -4.465050220489502\n",
      "13.20775032043457 -3.4014365673065186 -4.52826452255249 -3.8315165042877197 -4.6784892082214355 -4.6784892082214355\n",
      "13.159000396728516 -3.4607720375061035 -4.562201976776123 -3.7455506324768066 -5.192410469055176 -5.192410469055176\n",
      "13.176250457763672 -3.493455171585083 -4.589423656463623 -3.8324828147888184 -3.688865900039673 -3.6888656616210938\n",
      "13.19425106048584 -3.448275566101074 -4.574370861053467 -3.3076584339141846 -5.00013542175293 -5.00013542175293\n",
      "13.102251052856445 -3.4033641815185547 -4.547886848449707 -4.019808769226074 -3.8815102577209473 -3.8815102577209473\n",
      "13.169500350952148 -3.425934314727783 -4.54716157913208 -3.5641164779663086 -5.188563346862793 -5.188563823699951\n",
      "13.191750526428223 -3.455521583557129 -4.56010103225708 -3.501068115234375 -4.1506242752075195 -4.1506242752075195\n",
      "13.201000213623047 -3.4339561462402344 -4.54125452041626 -3.2527623176574707 -5.340900421142578 -5.340900421142578\n",
      "13.18125057220459 -3.401948928833008 -4.535111427307129 -3.293720006942749 -4.678060531616211 -4.678060531616211\n",
      "13.119500160217285 -3.4138824939727783 -4.540553092956543 -3.0752782821655273 -4.341949462890625 -4.341948509216309\n",
      "13.175000190734863 -3.426450729370117 -4.560393810272217 -3.7319164276123047 -5.404173851013184 -5.404173851013184\n",
      "13.180750846862793 -3.3451344966888428 -4.520336627960205 -3.005971908569336 -4.93369197845459 -4.933691501617432\n",
      "13.126501083374023 -3.352454423904419 -4.533870220184326 -3.292670726776123 -5.337915420532227 -5.337915897369385\n",
      "13.16100025177002 -3.4190359115600586 -4.564083576202393 -3.7434444427490234 -4.546731472015381 -4.546731472015381\n",
      "13.132000923156738 -3.4027135372161865 -4.532440185546875 -4.109573841094971 -5.340333938598633 -5.340333938598633\n",
      "13.234750747680664 -3.3761394023895264 -4.518200397491455 -3.299476146697998 -5.928393363952637 -5.928393363952637\n",
      "13.16200065612793 -3.3494439125061035 -4.504328727722168 -3.099130392074585 -4.503854274749756 -4.503853797912598\n",
      "13.20050048828125 -3.3789165019989014 -4.532284736633301 -3.424065589904785 -3.730414390563965 -3.730414867401123\n",
      "13.21150016784668 -3.3819150924682617 -4.533586025238037 -3.11960506439209 -4.498590469360352 -4.498589992523193\n",
      "13.160000801086426 -3.3903214931488037 -4.544098377227783 -3.176076889038086 -3.430969715118408 -3.430969715118408\n",
      "13.144001007080078 -3.3888511657714844 -4.544100761413574 -3.549922466278076 -5.52935791015625 -5.52935791015625\n",
      "13.140250205993652 -3.353079319000244 -4.521488189697266 -3.1262993812561035 -5.24424934387207 -5.24424934387207\n",
      "13.097250938415527 -3.3814573287963867 -4.539064884185791 -3.876608371734619 -5.30438232421875 -5.30438232421875\n",
      "13.111001014709473 -3.4181737899780273 -4.548867225646973 -3.3852341175079346 -5.111748218536377 -5.111748695373535\n",
      "13.148750305175781 -3.3975486755371094 -4.534333229064941 -3.406766414642334 -4.376513481140137 -4.3765130043029785\n",
      "13.079000473022461 -3.3703932762145996 -4.516299724578857 -3.0405948162078857 -5.242232322692871 -5.242232322692871\n",
      "13.108000755310059 -3.3482842445373535 -4.515237808227539 -3.5871238708496094 -5.4899749755859375 -5.489975452423096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.075250625610352 -3.3672032356262207 -4.535396099090576 -3.153372287750244 -3.5493242740631104 -3.5493240356445312\n",
      "13.139250755310059 -3.3908255100250244 -4.548449516296387 -3.4884438514709473 -4.256360054016113 -4.2563605308532715\n",
      "13.071001052856445 -3.35089373588562 -4.53466272354126 -3.365556240081787 -4.0442423820495605 -4.0442423820495605\n",
      "13.035500526428223 -3.3254966735839844 -4.51710844039917 -3.444122552871704 -3.181459426879883 -3.1814589500427246\n",
      "13.034250259399414 -3.3466150760650635 -4.5276408195495605 -3.466601848602295 -3.9832630157470703 -3.9832632541656494\n",
      "13.01300048828125 -3.3739001750946045 -4.548716068267822 -3.3840746879577637 -5.0311970710754395 -5.0311970710754395\n",
      "13.122750282287598 -3.36470365524292 -4.533256530761719 -3.51994252204895 -3.776134967803955 -3.776134729385376\n",
      "13.037750244140625 -3.3428049087524414 -4.518719673156738 -3.469546318054199 -5.2646918296813965 -5.264691352844238\n",
      "13.070250511169434 -3.350510835647583 -4.517580986022949 -3.47652006149292 -3.6698381900787354 -3.6698384284973145\n",
      "13.004250526428223 -3.3855276107788086 -4.541031360626221 -3.0950417518615723 -5.336794853210449 -5.336794853210449\n",
      "13.01675033569336 -3.3842265605926514 -4.5386857986450195 -3.247668743133545 -3.3644275665283203 -3.3644275665283203\n",
      "13.00100040435791 -3.3334078788757324 -4.526797294616699 -3.6275179386138916 -5.107422828674316 -5.107422828674316\n",
      "12.989250183105469 -3.276167869567871 -4.503209114074707 -3.6658644676208496 -4.539950370788574 -4.539950847625732\n",
      "12.983250617980957 -3.3197031021118164 -4.526865482330322 -3.6086807250976562 -5.592368125915527 -5.592367649078369\n",
      "12.975750923156738 -3.3462600708007812 -4.532138347625732 -3.1537275314331055 -3.2268271446228027 -3.2268271446228027\n",
      "13.027250289916992 -3.368170976638794 -4.534471035003662 -3.5672059059143066 -4.134200096130371 -4.134200572967529\n",
      "13.034750938415527 -3.348383903503418 -4.532723903656006 -3.351619243621826 -4.178443431854248 -4.17844295501709\n",
      "13.020750999450684 -3.320294141769409 -4.511267185211182 -3.348853588104248 -5.304879188537598 -5.304879188537598\n",
      "13.031750679016113 -3.317969799041748 -4.51364803314209 -3.530287981033325 -3.244563341140747 -3.244563579559326\n",
      "12.99275016784668 -3.3418545722961426 -4.52860164642334 -3.2351198196411133 -5.073353290557861 -5.073352813720703\n",
      "13.084250450134277 -3.379786491394043 -4.5431928634643555 -3.6320691108703613 -3.8000659942626953 -3.800065755844116\n",
      "12.968250274658203 -3.336806535720825 -4.532575607299805 -2.8198904991149902 -3.3293514251708984 -3.3293514251708984\n",
      "12.980250358581543 -3.2960100173950195 -4.5115742683410645 -3.4109549522399902 -3.074050188064575 -3.074050188064575\n",
      "12.988500595092773 -3.288618803024292 -4.505234718322754 -3.242886543273926 -3.9387731552124023 -3.9387731552124023\n",
      "13.03225040435791 -3.342188596725464 -4.522375106811523 -3.093665599822998 -5.212041854858398 -5.212041854858398\n",
      "12.966500282287598 -3.360597848892212 -4.543360233306885 -3.6037333011627197 -5.246295928955078 -5.246295928955078\n",
      "12.932750701904297 -3.3413314819335938 -4.537044525146484 -3.474155902862549 -3.9452996253967285 -3.9452996253967285\n",
      "12.96150016784668 -3.3233463764190674 -4.526641845703125 -3.4964475631713867 -4.621245861053467 -4.621245861053467\n",
      "12.918000221252441 -3.325589179992676 -4.513659954071045 -3.156686305999756 -3.8248891830444336 -3.8248891830444336\n",
      "12.964250564575195 -3.329984426498413 -4.516435623168945 -3.213416576385498 -4.009243965148926 -4.009243965148926\n",
      "12.929750442504883 -3.3972692489624023 -4.547300338745117 -3.2054617404937744 -3.409271001815796 -3.409271001815796\n",
      "12.921250343322754 -3.389400005340576 -4.549139976501465 -3.467978000640869 -5.352534294128418 -5.35253381729126\n",
      "12.942000389099121 -3.2985568046569824 -4.511683940887451 -3.0661473274230957 -5.015591621398926 -5.015591621398926\n",
      "12.927750587463379 -3.253422975540161 -4.507701873779297 -3.2774858474731445 -5.529052257537842 -5.529052257537842\n",
      "12.88550090789795 -3.281853675842285 -4.519009590148926 -2.8279762268066406 -4.722557067871094 -4.7225565910339355\n",
      "12.946500778198242 -3.326237440109253 -4.539088726043701 -3.4318790435791016 -4.804647445678711 -4.804647922515869\n",
      "12.961000442504883 -3.352665662765503 -4.541244983673096 -3.171144723892212 -4.689524173736572 -4.689524173736572\n",
      "12.95525074005127 -3.3170969486236572 -4.520048141479492 -2.9431142807006836 -3.8627171516418457 -3.8627171516418457\n",
      "12.95775032043457 -3.2945773601531982 -4.502650737762451 -3.3648810386657715 -3.2308754920959473 -3.2308757305145264\n",
      "12.875750541687012 -3.3002431392669678 -4.508100986480713 -3.298266887664795 -4.310459613800049 -4.310459613800049\n",
      "12.869250297546387 -3.341381788253784 -4.523094177246094 -3.503978729248047 -3.8038063049316406 -3.8038063049316406\n",
      "12.876501083374023 -3.3550260066986084 -4.542888641357422 -3.376987934112549 -5.0930094718933105 -5.093008995056152\n",
      "12.893750190734863 -3.31669545173645 -4.518926620483398 -3.2632737159729004 -5.337033271789551 -5.337033271789551\n",
      "12.884000778198242 -3.2876782417297363 -4.5211920738220215 -3.497603416442871 -4.544377326965332 -4.544377326965332\n",
      "12.851750373840332 -3.3065669536590576 -4.52966833114624 -3.854370594024658 -3.023303985595703 -3.023303747177124\n",
      "12.845000267028809 -3.3372702598571777 -4.541355133056641 -3.584773540496826 -3.5715715885162354 -3.5715715885162354\n",
      "12.880001068115234 -3.3484935760498047 -4.5391082763671875 -3.3843979835510254 -4.38478422164917 -4.38478422164917\n",
      "12.800000190734863 -3.309080123901367 -4.516796588897705 -3.4041521549224854 -3.893611431121826 -3.893611431121826\n",
      "12.847001075744629 -3.289276599884033 -4.510597229003906 -2.9305429458618164 -5.158595085144043 -5.158595085144043\n",
      "12.802750587463379 -3.310753107070923 -4.5225510597229 -3.5280797481536865 -4.940783977508545 -4.940784454345703\n",
      "12.813750267028809 -3.3294191360473633 -4.544290065765381 -3.3680853843688965 -4.6575775146484375 -4.657577037811279\n",
      "12.839250564575195 -3.344006061553955 -4.5437774658203125 -3.491757869720459 -3.7366394996643066 -3.736639976501465\n",
      "12.783750534057617 -3.327568531036377 -4.538559913635254 -3.4115004539489746 -3.8354077339172363 -3.8354082107543945\n",
      "12.853500366210938 -3.3305346965789795 -4.527111053466797 -3.460628032684326 -5.239357948303223 -5.2393574714660645\n",
      "12.896500587463379 -3.3378570079803467 -4.519559860229492 -2.7065067291259766 -4.461459636688232 -4.461459636688232\n",
      "12.774500846862793 -3.3226680755615234 -4.524113655090332 -3.062069892883301 -5.156875133514404 -5.156875133514404\n",
      "12.802750587463379 -3.3216934204101562 -4.533308029174805 -2.8999404907226562 -3.3146626949310303 -3.314662456512451\n",
      "12.800251007080078 -3.3087081909179688 -4.532242298126221 -3.178311347961426 -4.311095237731934 -4.31109619140625\n",
      "12.800750732421875 -3.3045527935028076 -4.537332057952881 -3.8178038597106934 -5.278510093688965 -5.278509616851807\n",
      "12.802250862121582 -3.309396266937256 -4.528111934661865 -3.0998573303222656 -4.194699287414551 -4.194699287414551\n",
      "12.858250617980957 -3.3075978755950928 -4.5228776931762695 -3.5069050788879395 -4.687720775604248 -4.687720775604248\n",
      "12.821250915527344 -3.3026885986328125 -4.526172161102295 -3.6943304538726807 -4.747960090637207 -4.747960090637207\n",
      "12.778000831604004 -3.2917239665985107 -4.530305862426758 -3.6069507598876953 -5.0445027351379395 -5.0445027351379395\n",
      "12.762250900268555 -3.298764705657959 -4.531646251678467 -3.236910820007324 -4.884627819061279 -4.8846282958984375\n",
      "12.778000831604004 -3.336092948913574 -4.547522068023682 -3.3219752311706543 -4.099488258361816 -4.099487781524658\n",
      "12.761000633239746 -3.340754270553589 -4.544872760772705 -3.042079448699951 -4.547056198120117 -4.547056198120117\n",
      "12.70775032043457 -3.3331222534179688 -4.5359930992126465 -3.7952880859375 -3.753427505493164 -3.753427505493164\n",
      "12.768500328063965 -3.326260805130005 -4.5306806564331055 -3.126814842224121 -4.922259330749512 -4.922259330749512\n",
      "12.803250312805176 -3.3065130710601807 -4.513651371002197 -2.9376378059387207 -3.6110999584198 -3.6110999584198\n",
      "12.731000900268555 -3.287693977355957 -4.525269031524658 -3.489102602005005 -4.460630416870117 -4.460630893707275\n",
      "12.679500579833984 -3.2952327728271484 -4.52451753616333 -3.1998367309570312 -5.188019752502441 -5.188019275665283\n",
      "12.722750663757324 -3.299021005630493 -4.535449981689453 -3.425222873687744 -4.105683326721191 -4.10568380355835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.794500350952148 -3.3451895713806152 -4.546996593475342 -3.174355983734131 -3.783867835998535 -3.7838683128356934\n",
      "12.77500057220459 -3.3370513916015625 -4.553471088409424 -3.4654653072357178 -6.050811767578125 -6.050811767578125\n",
      "12.753750801086426 -3.297006130218506 -4.531771183013916 -2.869189739227295 -3.1335439682006836 -3.1335439682006836\n",
      "12.750250816345215 -3.294888496398926 -4.508756160736084 -3.0686168670654297 -5.448749542236328 -5.448749542236328\n",
      "12.715250968933105 -3.3343048095703125 -4.534941673278809 -3.059260845184326 -3.1400303840637207 -3.1400306224823\n",
      "12.687000274658203 -3.3420045375823975 -4.542012691497803 -3.3066205978393555 -4.524964332580566 -4.524965286254883\n",
      "12.765000343322754 -3.322968006134033 -4.5396647453308105 -3.4519731998443604 -4.616467475891113 -4.6164679527282715\n",
      "12.7160005569458 -3.3035948276519775 -4.54285192489624 -3.586879014968872 -5.059217929840088 -5.059218406677246\n",
      "12.729500770568848 -3.2789688110351562 -4.5225653648376465 -2.9075865745544434 -4.0162763595581055 -4.0162763595581055\n",
      "12.730751037597656 -3.2728195190429688 -4.523426532745361 -3.5781922340393066 -5.046298027038574 -5.046298503875732\n",
      "12.710500717163086 -3.2906360626220703 -4.525391101837158 -3.07600736618042 -3.915264129638672 -3.915264129638672\n",
      "12.688750267028809 -3.321319341659546 -4.534333229064941 -3.258395195007324 -3.2531323432922363 -3.2531325817108154\n",
      "12.754250526428223 -3.3192248344421387 -4.533440113067627 -3.5698206424713135 -4.752724647521973 -4.752724647521973\n",
      "12.725000381469727 -3.289665699005127 -4.521928310394287 -3.6125943660736084 -3.493124008178711 -3.493124008178711\n",
      "12.714750289916992 -3.304065704345703 -4.526511192321777 -2.985522985458374 -4.949480056762695 -4.949480056762695\n",
      "12.743250846862793 -3.3184754848480225 -4.537302017211914 -3.17936372756958 -5.169606685638428 -5.1696062088012695\n",
      "12.608250617980957 -3.309307813644409 -4.539186477661133 -3.496614933013916 -5.073282718658447 -5.073282241821289\n",
      "12.66550064086914 -3.324052095413208 -4.534237861633301 -3.3449835777282715 -4.0059661865234375 -4.0059661865234375\n",
      "12.684000968933105 -3.324483633041382 -4.54296875 -3.437340259552002 -4.970733642578125 -4.970733165740967\n",
      "12.707250595092773 -3.336491823196411 -4.533440589904785 -3.8195581436157227 -5.579380035400391 -5.579380512237549\n",
      "12.75925064086914 -3.307110548019409 -4.531445026397705 -3.3977794647216797 -3.7999963760375977 -3.7999963760375977\n",
      "12.678000450134277 -3.258579730987549 -4.5159502029418945 -3.2641236782073975 -3.183215618133545 -3.183215618133545\n",
      "12.66650104522705 -3.2854666709899902 -4.523553371429443 -3.615739345550537 -4.401616096496582 -4.401616096496582\n",
      "12.678250312805176 -3.3386924266815186 -4.554380893707275 -3.492541790008545 -3.546834945678711 -3.546834945678711\n",
      "12.621500968933105 -3.3448541164398193 -4.564577102661133 -3.361863851547241 -3.5863535404205322 -3.5863535404205322\n",
      "12.624250411987305 -3.3277831077575684 -4.554888725280762 -3.1030733585357666 -3.2871012687683105 -3.2871012687683105\n",
      "12.675251007080078 -3.288529396057129 -4.524184703826904 -3.371058940887451 -5.199789047241211 -5.199789047241211\n",
      "12.672750473022461 -3.2886126041412354 -4.530227184295654 -3.121649742126465 -4.613058567047119 -4.613058090209961\n",
      "12.676000595092773 -3.290109395980835 -4.527757167816162 -3.2126219272613525 -3.479682445526123 -3.479681968688965\n",
      "12.623000144958496 -3.325688600540161 -4.550317287445068 -3.304381847381592 -4.701821327209473 -4.7018208503723145\n",
      "12.624250411987305 -3.365755796432495 -4.565178871154785 -3.6952431201934814 -5.043476104736328 -5.043476581573486\n",
      "12.607250213623047 -3.3247580528259277 -4.556524753570557 -2.7218875885009766 -5.051008701324463 -5.051008224487305\n",
      "12.677000999450684 -3.2681829929351807 -4.531001567840576 -2.9590721130371094 -3.117189645767212 -3.117189407348633\n",
      "12.600000381469727 -3.2446954250335693 -4.5257649421691895 -2.9281578063964844 -3.774968147277832 -3.774967670440674\n",
      "12.605250358581543 -3.2821011543273926 -4.543081283569336 -3.1284258365631104 -4.554347991943359 -4.554347991943359\n",
      "12.59225082397461 -3.3329756259918213 -4.555215835571289 -3.6797075271606445 -4.328732490539551 -4.328732967376709\n",
      "12.635750770568848 -3.336113691329956 -4.558586120605469 -3.34104585647583 -5.186380863189697 -5.186380863189697\n",
      "12.609251022338867 -3.3094465732574463 -4.531581878662109 -4.086976051330566 -4.520162582397461 -4.520162582397461\n",
      "12.653250694274902 -3.270933151245117 -4.524537563323975 -3.5912489891052246 -5.096253871917725 -5.096254348754883\n",
      "12.640000343322754 -3.278681755065918 -4.5258002281188965 -3.5100574493408203 -4.222638130187988 -4.222638130187988\n",
      "12.672750473022461 -3.3052656650543213 -4.54006290435791 -2.532956838607788 -4.302917003631592 -4.302917003631592\n",
      "12.630500793457031 -3.316997766494751 -4.553865909576416 -3.0797104835510254 -4.251523494720459 -4.251523494720459\n",
      "12.599000930786133 -3.294116973876953 -4.5360612869262695 -3.3769772052764893 -4.517844200134277 -4.517844200134277\n",
      "12.58650016784668 -3.28479266166687 -4.529521942138672 -3.3017640113830566 -5.359251022338867 -5.359251499176025\n",
      "12.621750831604004 -3.2954232692718506 -4.527082443237305 -3.1977405548095703 -4.584650039672852 -4.584650039672852\n",
      "12.643250465393066 -3.2844367027282715 -4.529543399810791 -3.257636070251465 -4.6152849197387695 -4.6152849197387695\n",
      "12.659000396728516 -3.3395235538482666 -4.551788806915283 -3.6403064727783203 -5.105640411376953 -5.105639934539795\n",
      "12.57300090789795 -3.3479673862457275 -4.569787979125977 -3.497277021408081 -4.115108013153076 -4.115107536315918\n",
      "12.612500190734863 -3.316826343536377 -4.553183078765869 -3.502323627471924 -5.296985626220703 -5.296985626220703\n",
      "12.60425090789795 -3.2857418060302734 -4.534607887268066 -3.396563768386841 -5.214487075805664 -5.214486598968506\n",
      "12.613750457763672 -3.2577338218688965 -4.530263423919678 -2.713931083679199 -3.204061985015869 -3.204061985015869\n",
      "12.59950065612793 -3.285409450531006 -4.534590721130371 -3.480285882949829 -4.968060493469238 -4.968060493469238\n",
      "12.624000549316406 -3.323676586151123 -4.56289005279541 -3.685713291168213 -5.027789115905762 -5.0277886390686035\n",
      "12.577250480651855 -3.330545425415039 -4.559015274047852 -3.638284206390381 -4.241428375244141 -4.241428375244141\n",
      "12.594000816345215 -3.283289670944214 -4.525512218475342 -3.281632900238037 -4.858804702758789 -4.858803749084473\n",
      "12.616750717163086 -3.280346632003784 -4.530610084533691 -3.4464712142944336 -4.138260841369629 -4.138260841369629\n",
      "12.565500259399414 -3.288327217102051 -4.540551662445068 -3.4073357582092285 -3.8504865169525146 -3.8504865169525146\n",
      "12.55625057220459 -3.3053507804870605 -4.551249027252197 -3.207855463027954 -4.34927225112915 -4.349272727966309\n",
      "12.540000915527344 -3.3186564445495605 -4.554052829742432 -3.6432409286499023 -4.671761512756348 -4.671761512756348\n",
      "12.640501022338867 -3.329591989517212 -4.56579065322876 -3.1170542240142822 -4.921771049499512 -4.921770095825195\n",
      "12.622000694274902 -3.3259620666503906 -4.552049160003662 -3.1669585704803467 -3.8992276191711426 -3.8992276191711426\n",
      "12.555000305175781 -3.288951873779297 -4.531252861022949 -2.976621627807617 -4.041229248046875 -4.041229248046875\n",
      "12.579500198364258 -3.283792495727539 -4.530043125152588 -3.639739513397217 -5.051624298095703 -5.051624774932861\n",
      "12.62125015258789 -3.293278694152832 -4.5370893478393555 -3.6492152214050293 -4.384209632873535 -4.384209156036377\n",
      "12.625500679016113 -3.2908947467803955 -4.541150093078613 -3.3161585330963135 -4.077381134033203 -4.077381134033203\n",
      "12.54800033569336 -3.2815823554992676 -4.537512302398682 -3.044214963912964 -5.564216136932373 -5.564215660095215\n",
      "12.562500953674316 -3.2892837524414062 -4.541382789611816 -3.4761767387390137 -4.681915283203125 -4.681915283203125\n",
      "12.543500900268555 -3.3237311840057373 -4.553171157836914 -3.4140961170196533 -5.455913543701172 -5.455913066864014\n",
      "12.580000877380371 -3.30983567237854 -4.550812244415283 -3.437455654144287 -4.848081588745117 -4.848081111907959\n",
      "12.59575080871582 -3.296238899230957 -4.542947292327881 -3.075946569442749 -3.926356315612793 -3.926356315612793\n",
      "12.572000503540039 -3.255873441696167 -4.517610549926758 -3.6175875663757324 -5.076510429382324 -5.076510429382324\n",
      "12.589750289916992 -3.26533842086792 -4.534120559692383 -3.0581037998199463 -5.520416259765625 -5.520416259765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.511750221252441 -3.310901641845703 -4.556252956390381 -3.6054811477661133 -5.815359115600586 -5.815359115600586\n",
      "12.565750122070312 -3.3219387531280518 -4.55238676071167 -3.174598217010498 -5.04995059967041 -5.049951076507568\n",
      "12.497750282287598 -3.299499988555908 -4.553600788116455 -2.9791479110717773 -5.460391998291016 -5.460391521453857\n",
      "12.573250770568848 -3.298264265060425 -4.53557825088501 -3.8404643535614014 -4.983651161193848 -4.9836506843566895\n",
      "12.569000244140625 -3.2756032943725586 -4.528627395629883 -3.4875736236572266 -5.347578525543213 -5.347578048706055\n",
      "12.534000396728516 -3.29974102973938 -4.547499656677246 -3.3802976608276367 -5.1844329833984375 -5.1844329833984375\n",
      "12.531750679016113 -3.3147435188293457 -4.554494380950928 -3.405251979827881 -4.358010292053223 -4.358010292053223\n",
      "12.547500610351562 -3.2895827293395996 -4.554728984832764 -3.170408010482788 -4.951743125915527 -4.951743125915527\n",
      "12.48275089263916 -3.273259401321411 -4.538059711456299 -3.1608734130859375 -3.12247371673584 -3.1224734783172607\n",
      "12.502500534057617 -3.271878957748413 -4.538547992706299 -3.2077808380126953 -5.263941764831543 -5.263941764831543\n",
      "12.578250885009766 -3.289813995361328 -4.546187400817871 -3.117609977722168 -5.122482776641846 -5.1224822998046875\n",
      "12.518000602722168 -3.2702810764312744 -4.547419548034668 -3.7715792655944824 -4.419553756713867 -4.419554233551025\n",
      "12.51675033569336 -3.3118960857391357 -4.555720329284668 -3.2311627864837646 -4.505385875701904 -4.5053863525390625\n",
      "12.557750701904297 -3.3079421520233154 -4.5445990562438965 -3.2207298278808594 -5.823469638824463 -5.823469638824463\n",
      "12.546751022338867 -3.293442726135254 -4.541175842285156 -3.3968350887298584 -3.3008460998535156 -3.3008460998535156\n",
      "12.591750144958496 -3.294090986251831 -4.533031940460205 -3.4570116996765137 -3.8634767532348633 -3.8634767532348633\n",
      "12.54425048828125 -3.3141753673553467 -4.544696807861328 -3.550265073776245 -4.933266639709473 -4.933265686035156\n",
      "12.578500747680664 -3.3139357566833496 -4.556180000305176 -3.6200504302978516 -4.832488059997559 -4.832488059997559\n",
      "12.527000427246094 -3.3111276626586914 -4.555823802947998 -3.454336643218994 -4.578262805938721 -4.578262805938721\n",
      "12.542500495910645 -3.2904515266418457 -4.552051544189453 -3.3877108097076416 -4.556695461273193 -4.556695938110352\n",
      "12.542750358581543 -3.2720460891723633 -4.546849727630615 -3.0200464725494385 -4.397270679473877 -4.397270679473877\n",
      "12.540750503540039 -3.2760531902313232 -4.54647159576416 -3.6521925926208496 -4.5777587890625 -4.577759265899658\n",
      "12.544500350952148 -3.2908105850219727 -4.543656349182129 -3.4212985038757324 -5.262940406799316 -5.262940406799316\n",
      "12.492500305175781 -3.320709228515625 -4.5616326332092285 -3.302274227142334 -2.958998203277588 -2.958998203277588\n",
      "12.50925064086914 -3.3236303329467773 -4.56072473526001 -3.089743137359619 -3.772805690765381 -3.772805690765381\n",
      "12.467750549316406 -3.3067362308502197 -4.547055244445801 -3.127260446548462 -5.339970588684082 -5.33997106552124\n",
      "12.481250762939453 -3.2784268856048584 -4.543400287628174 -3.8910584449768066 -4.45939302444458 -4.459392547607422\n",
      "12.47925090789795 -3.270184278488159 -4.53981351852417 -2.9498722553253174 -3.6432461738586426 -3.6432461738586426\n",
      "12.50925064086914 -3.2550721168518066 -4.539668083190918 -4.070468425750732 -4.379083633422852 -4.379083633422852\n",
      "12.530750274658203 -3.2915821075439453 -4.552768230438232 -2.78120756149292 -5.2337565422058105 -5.2337565422058105\n",
      "12.462250709533691 -3.2967090606689453 -4.564506530761719 -3.221822738647461 -4.467825889587402 -4.467825889587402\n",
      "12.481500625610352 -3.2987794876098633 -4.556900501251221 -2.9718880653381348 -4.90368127822876 -4.90368127822876\n",
      "12.583000183105469 -3.28314471244812 -4.540756702423096 -3.1562299728393555 -4.113866329193115 -4.113865852355957\n",
      "12.520000457763672 -3.2801451683044434 -4.534533977508545 -3.384469509124756 -3.2986884117126465 -3.2986881732940674\n",
      "12.484750747680664 -3.286983013153076 -4.526970863342285 -3.31437349319458 -3.550259590148926 -3.550259590148926\n",
      "12.517251014709473 -3.306335687637329 -4.551328182220459 -3.03745698928833 -4.630080699920654 -4.630080223083496\n",
      "12.489250183105469 -3.3102529048919678 -4.551690101623535 -3.8141207695007324 -4.831345558166504 -4.831345558166504\n",
      "12.494500160217285 -3.3050620555877686 -4.544321537017822 -3.294987916946411 -4.270334720611572 -4.270334720611572\n",
      "12.528250694274902 -3.267246723175049 -4.546676158905029 -4.008572578430176 -3.2257046699523926 -3.2257044315338135\n",
      "12.501750946044922 -3.273407220840454 -4.54046106338501 -3.062221050262451 -4.696786403656006 -4.696785926818848\n",
      "12.484251022338867 -3.266618490219116 -4.550939083099365 -3.0827369689941406 -4.7239861488342285 -4.723986625671387\n",
      "12.44800090789795 -3.254883050918579 -4.540124893188477 -3.111711025238037 -3.2890467643737793 -3.2890467643737793\n",
      "12.54425048828125 -3.2802650928497314 -4.54726505279541 -3.621828079223633 -3.2665598392486572 -3.266559362411499\n",
      "12.485000610351562 -3.2909083366394043 -4.561194896697998 -3.726170778274536 -5.327248573303223 -5.327249050140381\n",
      "12.535000801086426 -3.2752623558044434 -4.547667026519775 -2.7959237098693848 -4.657561779022217 -4.657561779022217\n",
      "12.500500679016113 -3.280097723007202 -4.548992156982422 -3.1124374866485596 -4.40232515335083 -4.402325630187988\n",
      "12.533500671386719 -3.2836039066314697 -4.54329252243042 -3.344899892807007 -4.502066612243652 -4.502066612243652\n",
      "12.453500747680664 -3.314408779144287 -4.55808162689209 -3.5285048484802246 -4.669694900512695 -4.6696953773498535\n",
      "12.45525074005127 -3.3283302783966064 -4.564643383026123 -3.808291435241699 -4.445593357086182 -4.445593357086182\n",
      "12.4635009765625 -3.279156446456909 -4.535987377166748 -3.2095205783843994 -3.653272867202759 -3.653272867202759\n",
      "12.522250175476074 -3.277846336364746 -4.523181915283203 -4.03795051574707 -4.851349830627441 -4.851349830627441\n",
      "12.407000541687012 -3.2779922485351562 -4.540054798126221 -3.3089206218719482 -4.066830158233643 -4.066830158233643\n",
      "12.468000411987305 -3.3025267124176025 -4.557102203369141 -3.7873034477233887 -4.190668106079102 -4.190668106079102\n",
      "12.478500366210938 -3.2961266040802 -4.56600284576416 -3.2561757564544678 -4.8686676025390625 -4.8686676025390625\n",
      "12.512250900268555 -3.269257068634033 -4.5472941398620605 -3.4390687942504883 -5.064602851867676 -5.064602851867676\n",
      "12.515250205993652 -3.281827688217163 -4.548274993896484 -3.391371250152588 -4.938333511352539 -4.938333511352539\n",
      "12.451000213623047 -3.3025684356689453 -4.549628734588623 -3.0541040897369385 -3.4490208625793457 -3.4490206241607666\n",
      "12.412750244140625 -3.300410747528076 -4.548271656036377 -3.020803451538086 -5.162671089172363 -5.162671089172363\n",
      "12.486750602722168 -3.331515312194824 -4.562075614929199 -3.130898952484131 -5.061568260192871 -5.061568260192871\n",
      "12.42750072479248 -3.3189761638641357 -4.553886890411377 -3.598402500152588 -4.974935531616211 -4.974935531616211\n",
      "12.477750778198242 -3.273796319961548 -4.527801990509033 -3.1265268325805664 -4.593416690826416 -4.593417167663574\n",
      "12.485250473022461 -3.262080192565918 -4.5279622077941895 -3.712493896484375 -4.26057243347168 -4.26057243347168\n",
      "12.464500427246094 -3.2808408737182617 -4.542640209197998 -3.6722583770751953 -4.710011959075928 -4.710011959075928\n",
      "12.491250991821289 -3.3081319332122803 -4.561435222625732 -3.2901835441589355 -5.221641540527344 -5.221641540527344\n",
      "12.43375015258789 -3.3045494556427 -4.5543389320373535 -3.086036205291748 -3.6806697845458984 -3.6806697845458984\n",
      "12.426750183105469 -3.2881624698638916 -4.543353080749512 -3.4731976985931396 -3.611069440841675 -3.6110692024230957\n",
      "12.420001029968262 -3.2800300121307373 -4.542574405670166 -3.156707286834717 -4.940284729003906 -4.9402852058410645\n",
      "12.490750312805176 -3.3055474758148193 -4.541914463043213 -3.704545736312866 -5.195925712585449 -5.195925712585449\n",
      "12.441250801086426 -3.301156520843506 -4.549200057983398 -2.9785714149475098 -3.7938528060913086 -3.7938528060913086\n",
      "12.46500015258789 -3.2955729961395264 -4.558955669403076 -2.88926362991333 -3.854769706726074 -3.854769706726074\n",
      "12.487000465393066 -3.270061731338501 -4.540591239929199 -3.2040393352508545 -4.138242244720459 -4.138242244720459\n",
      "12.429750442504883 -3.2605583667755127 -4.53859806060791 -2.8937156200408936 -5.509273529052734 -5.509274005889893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.460000991821289 -3.2977683544158936 -4.550785541534424 -3.622847557067871 -4.835618019104004 -4.835618019104004\n",
      "12.524500846862793 -3.3345251083374023 -4.560277938842773 -3.352673292160034 -4.256934642791748 -4.256934642791748\n",
      "12.467500686645508 -3.3285646438598633 -4.561315059661865 -2.943089485168457 -5.298978805541992 -5.298978805541992\n",
      "12.429000854492188 -3.31752610206604 -4.5573601722717285 -2.919806480407715 -3.127995729446411 -3.127995491027832\n",
      "12.490750312805176 -3.2894980907440186 -4.547757625579834 -3.0006444454193115 -5.166658878326416 -5.166658878326416\n",
      "12.476250648498535 -3.2802236080169678 -4.5502166748046875 -3.5773003101348877 -3.7935681343078613 -3.793567657470703\n",
      "12.4350004196167 -3.2598955631256104 -4.537768840789795 -3.777878999710083 -3.400387763977051 -3.400387763977051\n",
      "12.495000839233398 -3.2837271690368652 -4.539565563201904 -3.455925226211548 -5.029068946838379 -5.029068946838379\n",
      "12.462250709533691 -3.309149980545044 -4.557426929473877 -3.0858068466186523 -5.263387203216553 -5.263387203216553\n",
      "12.45775032043457 -3.2987377643585205 -4.553743362426758 -2.754394054412842 -4.748946666717529 -4.748946666717529\n",
      "12.458250999450684 -3.2837345600128174 -4.551361560821533 -3.44173526763916 -4.046237945556641 -4.046237945556641\n",
      "12.424500465393066 -3.2818381786346436 -4.543773174285889 -3.2896485328674316 -4.733540058135986 -4.733540058135986\n",
      "12.544750213623047 -3.286458730697632 -4.543391227722168 -3.3156869411468506 -4.590881824493408 -4.590881824493408\n",
      "12.490750312805176 -3.2643916606903076 -4.535730838775635 -3.328183650970459 -3.0407769680023193 -3.0407767295837402\n",
      "12.47800064086914 -3.267239570617676 -4.540170192718506 -3.4930875301361084 -5.332653999328613 -5.332653999328613\n",
      "12.441250801086426 -3.2653191089630127 -4.5367512702941895 -3.040188789367676 -4.943268299102783 -4.943267345428467\n",
      "12.499750137329102 -3.289106607437134 -4.55560827255249 -3.3061270713806152 -5.2548370361328125 -5.2548370361328125\n",
      "12.469000816345215 -3.275324821472168 -4.542325496673584 -2.914752244949341 -4.244259357452393 -4.244259357452393\n",
      "12.456501007080078 -3.242105484008789 -4.548990249633789 -3.0659282207489014 -4.488299369812012 -4.488299369812012\n",
      "12.434000968933105 -3.249682664871216 -4.524759292602539 -3.0110251903533936 -3.148735523223877 -3.148735523223877\n",
      "12.462750434875488 -3.292742967605591 -4.544219493865967 -3.086134433746338 -4.3023457527160645 -4.302346229553223\n",
      "12.515000343322754 -3.3066701889038086 -4.55291748046875 -3.3193247318267822 -4.3874664306640625 -4.387465953826904\n",
      "12.4635009765625 -3.3073294162750244 -4.547789096832275 -3.7019295692443848 -4.05701208114624 -4.057012557983398\n",
      "12.46250057220459 -3.2733261585235596 -4.534745693206787 -3.445657730102539 -3.4534289836883545 -3.4534287452697754\n",
      "12.480751037597656 -3.278411865234375 -4.544640064239502 -3.855396270751953 -5.464953422546387 -5.4649529457092285\n",
      "12.45425033569336 -3.2950713634490967 -4.556401252746582 -3.2650578022003174 -4.8252105712890625 -4.8252105712890625\n",
      "12.458000183105469 -3.2865984439849854 -4.551637172698975 -3.4274988174438477 -4.912133693695068 -4.912133693695068\n",
      "12.439000129699707 -3.2723357677459717 -4.547128677368164 -3.269014835357666 -5.1990966796875 -5.199095726013184\n",
      "12.417250633239746 -3.293858766555786 -4.5552978515625 -3.4111499786376953 -4.656986236572266 -4.656986236572266\n",
      "12.463000297546387 -3.312770128250122 -4.548130512237549 -3.515226364135742 -3.933903455734253 -3.9339029788970947\n",
      "12.42650032043457 -3.31349515914917 -4.551220417022705 -2.894960641860962 -4.377241611480713 -4.377241611480713\n",
      "12.425000190734863 -3.268476724624634 -4.545698165893555 -2.6117103099823 -3.9747092723846436 -3.9747097492218018\n",
      "12.48650074005127 -3.2662394046783447 -4.537956714630127 -3.0933446884155273 -4.111499309539795 -4.111499309539795\n",
      "12.42750072479248 -3.2746715545654297 -4.549807071685791 -2.984807014465332 -4.6390061378479 -4.6390061378479\n",
      "12.421000480651855 -3.3009860515594482 -4.563689708709717 -3.7337961196899414 -4.420994758605957 -4.420994758605957\n",
      "12.435500144958496 -3.2978248596191406 -4.553704738616943 -3.034095287322998 -4.649574279785156 -4.6495747566223145\n",
      "12.465250968933105 -3.304252862930298 -4.549097537994385 -3.9797604084014893 -5.219474792480469 -5.219474792480469\n",
      "12.352750778198242 -3.283637285232544 -4.540402412414551 -3.5202550888061523 -5.034278392791748 -5.034278392791748\n",
      "12.351250648498535 -3.283054828643799 -4.5458526611328125 -2.6788108348846436 -4.4886016845703125 -4.4886016845703125\n",
      "12.387001037597656 -3.2921321392059326 -4.545629024505615 -2.794384479522705 -3.6794872283935547 -3.6794872283935547\n",
      "12.441250801086426 -3.290269136428833 -4.547862529754639 -3.4642698764801025 -5.55272912979126 -5.55272912979126\n",
      "12.416000366210938 -3.2881739139556885 -4.557579517364502 -3.134230613708496 -5.284204959869385 -5.284204959869385\n",
      "12.4347505569458 -3.2697787284851074 -4.546229362487793 -3.1851484775543213 -4.267110824584961 -4.267110824584961\n",
      "12.439250946044922 -3.2556066513061523 -4.535447120666504 -3.108402967453003 -3.8482916355133057 -3.8482918739318848\n",
      "12.456501007080078 -3.272045612335205 -4.546091079711914 -3.092696189880371 -4.858661651611328 -4.858661651611328\n",
      "12.411500930786133 -3.2784605026245117 -4.545902729034424 -3.889819622039795 -5.320657730102539 -5.320657730102539\n",
      "12.393750190734863 -3.305633783340454 -4.554383277893066 -3.3486061096191406 -4.9251484870910645 -4.925148010253906\n",
      "12.391250610351562 -3.308776617050171 -4.5725998878479 -3.7423784732818604 -3.519399642944336 -3.519399642944336\n",
      "12.360750198364258 -3.289780378341675 -4.5552778244018555 -3.0154452323913574 -4.858949661254883 -4.858949661254883\n",
      "12.395750999450684 -3.282050609588623 -4.538645267486572 -2.826946258544922 -5.028266906738281 -5.028266429901123\n",
      "12.379500389099121 -3.2864696979522705 -4.549339771270752 -3.171100616455078 -4.718864440917969 -4.718864440917969\n",
      "12.374250411987305 -3.3080790042877197 -4.55678129196167 -3.2508528232574463 -4.819755554199219 -4.819756031036377\n",
      "12.415750503540039 -3.3194448947906494 -4.5638861656188965 -3.598632335662842 -4.8064165115356445 -4.8064165115356445\n",
      "12.348250389099121 -3.2416274547576904 -4.537997245788574 -3.37420654296875 -4.984152793884277 -4.9841532707214355\n",
      "12.400250434875488 -3.252815008163452 -4.536428451538086 -3.1825742721557617 -3.6683993339538574 -3.6683995723724365\n",
      "12.421250343322754 -3.2628936767578125 -4.546818256378174 -3.6487860679626465 -5.705577373504639 -5.705577850341797\n",
      "12.346750259399414 -3.2993948459625244 -4.555958271026611 -3.318300724029541 -5.031229019165039 -5.031228542327881\n",
      "12.426250457763672 -3.3298592567443848 -4.567188739776611 -3.317718029022217 -2.727985143661499 -2.72798490524292\n",
      "12.41675090789795 -3.305443286895752 -4.549929141998291 -3.5105433464050293 -5.393661975860596 -5.393661975860596\n",
      "12.366750717163086 -3.2458431720733643 -4.532922267913818 -3.1950135231018066 -4.9461588859558105 -4.946158409118652\n",
      "12.456000328063965 -3.2554879188537598 -4.532887935638428 -3.6966404914855957 -5.14327335357666 -5.143272876739502\n",
      "12.443500518798828 -3.2654194831848145 -4.532411575317383 -2.5325427055358887 -4.284748077392578 -4.284748077392578\n",
      "12.38325023651123 -3.285973310470581 -4.5439629554748535 -2.8835768699645996 -4.362856864929199 -4.362856388092041\n",
      "12.377250671386719 -3.3247053623199463 -4.5631561279296875 -3.76175856590271 -3.4421586990356445 -3.4421591758728027\n",
      "12.394250869750977 -3.3113996982574463 -4.553674697875977 -3.3177926540374756 -5.330160617828369 -5.330160140991211\n",
      "12.422500610351562 -3.267688035964966 -4.533608913421631 -2.7552361488342285 -5.694486141204834 -5.694486141204834\n",
      "12.406000137329102 -3.2696163654327393 -4.528234481811523 -2.842439651489258 -4.307745933532715 -4.307745933532715\n",
      "12.41100025177002 -3.2683401107788086 -4.549013137817383 -3.246201515197754 -4.683431625366211 -4.683431625366211\n",
      "12.366500854492188 -3.2846338748931885 -4.549291133880615 -2.9343204498291016 -4.913153648376465 -4.913153648376465\n",
      "12.404250144958496 -3.3093535900115967 -4.549437046051025 -3.767852306365967 -5.493407249450684 -5.493407249450684\n",
      "12.387001037597656 -3.3149611949920654 -4.556135654449463 -3.7483208179473877 -4.537552833557129 -4.537553310394287\n",
      "12.443000793457031 -3.307556390762329 -4.552340030670166 -3.1720473766326904 -5.049638748168945 -5.049638748168945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.409500122070312 -3.272481679916382 -4.5515265464782715 -3.1501269340515137 -5.413640975952148 -5.413640975952148\n",
      "12.417750358581543 -3.2458815574645996 -4.533425331115723 -3.5457897186279297 -4.2107648849487305 -4.2107648849487305\n",
      "12.366750717163086 -3.244736909866333 -4.530466556549072 -3.078091859817505 -3.9027814865112305 -3.9027814865112305\n",
      "12.425000190734863 -3.257094144821167 -4.532994747161865 -3.161534309387207 -4.264013290405273 -4.264013767242432\n"
     ]
    }
   ],
   "source": [
    "train(npoints = 9, batchsize = 4000, nsamples = 8) #greedyprob - 3, without +ve reinforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-myrtle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
