{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "racial-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "device = \"cuda:1\"\n",
    "floattype = torch.float\n",
    "\n",
    "batchsize = 512\n",
    "nsamples = 8\n",
    "npoints = 5\n",
    "emsize = 512\n",
    "\n",
    "\n",
    "class Graph_Transformer(nn.Module):\n",
    "    def __init__(self, emsize = 64, nhead = 8, nhid = 1024, nlayers = 3, ndecoderlayers = 0, dropout = 0.3):\n",
    "        super().__init__()\n",
    "        self.emsize = emsize\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "        encoder_layers = TransformerEncoderLayer(emsize, nhead, nhid, dropout = dropout)\n",
    "        decoder_layers = TransformerDecoderLayer(emsize, nhead, nhid, dropout = dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layers, ndecoderlayers)\n",
    "        self.encoder = nn.Linear(3, emsize)\n",
    "        self.outputattention_query = nn.Linear(emsize, emsize, bias = False)\n",
    "        self.outputattention_key = nn.Linear(emsize, emsize, bias = False)\n",
    "        self.start_token = nn.Parameter(torch.randn([emsize], device = device))\n",
    "    \n",
    "    def generate_subsequent_mask(self, sz): #last dimension will be softmaxed over when adding to attention logits, if boolean the ones turn into -inf\n",
    "        #mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        #mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        \n",
    "        mask = torch.triu(torch.ones([sz, sz], dtype = torch.bool, device = device), diagonal = 1)\n",
    "        return mask\n",
    "    \n",
    "    def encode(self, src): #src must be [batchsize * nsamples, npoints, 3]\n",
    "        src = self.encoder(src).transpose(0, 1)\n",
    "        output = self.transformer_encoder(src)\n",
    "        return output #[npoints, batchsize * nsamples, emsize]\n",
    "    \n",
    "    def decode_next(self, memory, tgt, route_mask): #route mask is [batchsize * nsamples, npoints], both memory and tgt must have batchsize and nsamples in same dimension (the 1th one)\n",
    "        npoints = memory.size(0)\n",
    "        batchsize = tgt.size(1)\n",
    "        \"\"\"if I really wanted this to be efficient I'd only recompute the decoder for the last tgt, and just remebering what the others looked like from before (won't change due to mask)\"\"\"\n",
    "        \"\"\"have the option to freeze the autograd on all but the last part of tgt, although at the moment this is a very natural way to say: initial choices matter more\"\"\"\n",
    "        tgt_mask = self.generate_subsequent_mask(tgt.size(0))\n",
    "        output = self.transformer_decoder(tgt, memory, tgt_mask) #[tgt, batchsize * nsamples, emsize]\n",
    "        output_query = self.outputattention_query(memory).transpose(0, 1) #[batchsize * nsamples, npoints, emsize]\n",
    "        output_key = self.outputattention_key(output[-1]) #[batchsize * nsamples, emsize]\n",
    "        output_attention = torch.matmul(output_query * self.emsize ** -0.5, output_key.unsqueeze(-1)).squeeze(-1) #[batchsize * nsamples, npoints], technically don't need to scale attention as we divide by variance next anyway\n",
    "        output_attention_tanh = output_attention.tanh() #[batchsize * nsamples, npoints]\n",
    "        \n",
    "        #we clone the route_mask incase we want to backprop using it (else it was modified by inplace opporations)\n",
    "        output_attention = output_attention.masked_fill(route_mask.clone(), float('-inf')) #[batchsize * nsamples, npoints]\n",
    "        output_attention_tanh = output_attention_tanh.masked_fill(route_mask.clone(), float('-inf')) #[batchsize * nsamples, npoints]\n",
    "        \n",
    "        return output_attention_tanh, output_attention #[batchsize * nsamples, npoints]\n",
    "    \n",
    "    def calculate_logprob(self, memory, routes): #memory is [npoints, batchsize * nsamples, emsize], routes is [batchsize * nsamples, npoints - 4], rather than backproping the entire loop, this saves vram (and computation)\n",
    "        npoints = memory.size(0)\n",
    "        ninternalpoints = routes.size(1)\n",
    "        bigbatchsize = memory.size(1)\n",
    "        memory_ = memory.gather(0, routes.transpose(0, 1).unsqueeze(2).expand(-1, -1, self.emsize)) #[npoints - 4, batchsize * nsamples, emsize] reorder memory into order of routes\n",
    "        tgt = torch.cat([self.start_token.unsqueeze(0).unsqueeze(1).expand(1, bigbatchsize, -1), memory_[:-1]]) #[npoints - 4, batchsize * nroutes, emsize], want to go from memory to tgt\n",
    "        tgt_mask = self.generate_subsequent_mask(ninternalpoints)\n",
    "        output = self.transformer_decoder(tgt, memory, tgt_mask) #[npoints - 4, batchsize * nsamples, emsize]\n",
    "        \"\"\"want probability of going from key to query, but first need to normalise (softmax with mask)\"\"\"\n",
    "        output_query = self.outputattention_query(memory_).transpose(0, 1) #[batchsize * nsamples, npoints - 4, emsize]\n",
    "        output_key = self.outputattention_key(output).transpose(0, 1) #[batchsize * nsamples, npoints - 4, emsize]\n",
    "        attention_mask = torch.full([ninternalpoints, ninternalpoints], True, device = device).triu(1) #[npoints - 4, npoints - 4], True for i < j\n",
    "        output_attention = torch.matmul(output_query * self.emsize ** -0.5, output_key.transpose(-1, -2))\n",
    "        \"\"\"quick fix to stop divergence\"\"\"\n",
    "        output_attention_tanh = output_attention.tanh()\n",
    "        \n",
    "        output_attention_tanh = output_attention_tanh.masked_fill(attention_mask, float('-inf'))\n",
    "        output_attention_tanh = output_attention_tanh - output_attention_tanh.logsumexp(-2, keepdim = True) #[batchsize * nsamples, npoints - 4, npoints - 4]\n",
    "        \n",
    "        output_attention = output_attention.masked_fill(attention_mask, float('-inf'))\n",
    "        output_attention = output_attention - output_attention.logsumexp(-2, keepdim = True) #[batchsize * nsamples, npoints - 4, npoints - 4]\n",
    "        \n",
    "        \"\"\"infact I'm almost tempted to not mask choosing a previous point, so it's forced to learn it and somehow incorporate it into its computation, but without much impact on reinforcing good examples\"\"\"\n",
    "        logprob_tanh = output_attention_tanh.diagonal(dim1 = -1, dim2 = -2).sum(-1) #[batchsize * nsamples]\n",
    "        logprob = output_attention.diagonal(dim1 = -1, dim2 = -2).sum(-1) #[batchsize * nsamples]\n",
    "        return logprob_tanh, logprob #[batchsize * nsamples]\n",
    "\n",
    "NN = Graph_Transformer().to(device)\n",
    "optimizer = optim.Adam(NN.parameters())\n",
    "\n",
    "\n",
    "class environment:    \n",
    "    def reset(self, npoints, batchsize, nsamples=1, corner_points = None, initial_triangulation = None):\n",
    "        \"\"\"\n",
    "        corner_points, etc., shoudn't include a batch dimension\n",
    "        \"\"\"\n",
    "        if corner_points == None:\n",
    "            ncornerpoints = 4\n",
    "        else:\n",
    "            ncornerpoints = corner_points.size(0)\n",
    "        if npoints <= ncornerpoints:\n",
    "            print(\"Error: not enough points for valid problem instance\")\n",
    "            return\n",
    "        self.batchsize = (\n",
    "            batchsize * nsamples\n",
    "        )  # so that I don't have to rewrite all this code, we store these two dimensions together\n",
    "        self.nsamples = nsamples\n",
    "        self.npoints = npoints\n",
    "        self.points = (\n",
    "            torch.rand([batchsize, npoints - ncornerpoints, 3], dtype = floattype, device=device)\n",
    "            .unsqueeze(1)\n",
    "            .expand(-1, nsamples, -1, -1)\n",
    "            .reshape(self.batchsize, npoints - ncornerpoints, 3)\n",
    "        )\n",
    "        if corner_points == None:\n",
    "            self.corner_points = torch.tensor(\n",
    "                [[0, 0, 0], [3, 0, 0], [0, 3, 0], [0, 0, 3]], dtype = floattype, device=device\n",
    "            )\n",
    "        else:\n",
    "            self.corner_points = corner_points\n",
    "        self.points = torch.cat(\n",
    "            [\n",
    "                self.corner_points.unsqueeze(0).expand(self.batchsize, -1, -1),\n",
    "                self.points,\n",
    "            ],\n",
    "            dim=-2,\n",
    "        )  # [batchsize * nsamples, npoints, 3]\n",
    "        self.points_mask = torch.cat(\n",
    "            [\n",
    "                torch.ones([self.batchsize, ncornerpoints], dtype=torch.bool, device=device),\n",
    "                torch.zeros(\n",
    "                    [self.batchsize, npoints - ncornerpoints], dtype=torch.bool, device=device\n",
    "                ),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        self.points_sequence = torch.empty(\n",
    "            [self.batchsize, 0], dtype=torch.long, device=device\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        points are now triples\n",
    "        triangles are now quadruples\n",
    "        edges are now still just indices, but there are four of them per 'triangle', and they correspond to triples of points, not pairs\n",
    "        we use  0,2,1  0,3,2  0,1,3  1,2,3  as the order of the four 'edges'/faces\n",
    "        opposite face is always ordered such that the last two indices are swapped\n",
    "        faces are always read ANTICLOCKWISE\n",
    "        \n",
    "        first three points of tetrahedron MUST be read clockwise (from the outside) to get correct sign on incircle test\n",
    "        \n",
    "        new point will be inserted in zeroth position, so if corresponding face of REMOVED tetrahedron is [x,y,z] (being read anticlockwise from outside in) new tetrahedron is [p, x, y, z]\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        number of tetrahedra is not the same for each batch (in 3D), so store as a big list, and remember batch index that it comes from\n",
    "        \"\"\"\n",
    "        if corner_points == None:\n",
    "            initial_triangulation = torch.tensor([[0, 1, 2, 3]], dtype=torch.long, device=device)\n",
    "        \n",
    "        self.partial_delaunay_triangles = initial_triangulation.unsqueeze(0).expand(self.batchsize, -1, -1).reshape(-1, 4)\n",
    "        self.batch_index = torch.arange(self.batchsize, dtype = torch.long, device = device).unsqueeze(1).expand(-1, initial_triangulation.size(0)).reshape(-1)\n",
    "        \n",
    "        self.batch_triangles = self.partial_delaunay_triangles.size(0) #[0]\n",
    "        self.ntriangles = torch.full([self.batchsize], initial_triangulation.size(0), dtype = torch.long, device = device) #[self.batchsize]\n",
    "        \n",
    "        self.cost = torch.zeros([self.batchsize], dtype = floattype, device=device)\n",
    "\n",
    "        self.logprob = torch.zeros([self.batchsize], dtype = floattype, device=device, requires_grad=True)\n",
    "\n",
    "    def update(self, point_index):  # point_index is [batchsize]\n",
    "        \n",
    "        assert point_index.size(0) == self.batchsize\n",
    "        assert str(point_index.device) == device\n",
    "        assert self.points_mask.gather(1, point_index.unsqueeze(1)).sum() == 0\n",
    "        \n",
    "        triangles_coordinates = self.points[self.batch_index.unsqueeze(1), self.partial_delaunay_triangles] # [batch_triangles, 4, 3]\n",
    "        \n",
    "        newpoint = self.points[self.batch_index, point_index[self.batch_index]] # [batch_triangles, 3]\n",
    "\n",
    "        incircle_matrix = torch.cat(\n",
    "            [\n",
    "                newpoint.unsqueeze(1),\n",
    "                triangles_coordinates,\n",
    "            ],\n",
    "            dim=-2,\n",
    "        )  # [batch_triangles, 5, 3]\n",
    "        incircle_matrix = torch.cat(\n",
    "            [\n",
    "                (incircle_matrix * incircle_matrix).sum(-1, keepdim=True),\n",
    "                incircle_matrix,\n",
    "                torch.ones([self.batch_triangles, 5, 1], dtype = floattype, device=device),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )  # [batch_triangles, 5, 5]\n",
    "        assert incircle_matrix.dtype == floattype\n",
    "        assert str(incircle_matrix.device) == device\n",
    "        \n",
    "        incircle_test = (\n",
    "            incircle_matrix.det() > 0\n",
    "        )  # [batch_triangles], is True if inside incircle\n",
    "        \n",
    "        conflicts = incircle_test.sum()\n",
    "        \n",
    "        conflicting_triangles = self.partial_delaunay_triangles[incircle_test] # [conflicts, 4]\n",
    "        \n",
    "        conflicting_edges_index0 = torch.empty_like(conflicting_triangles)\n",
    "        indices = torch.LongTensor([0, 0, 0, 1])\n",
    "        conflicting_edges_index0 = conflicting_triangles[:, indices] # [conflicts, 4]\n",
    "        \n",
    "        conflicting_edges_index1 = torch.empty_like(conflicting_triangles)\n",
    "        indices = torch.LongTensor([2, 3, 1, 2])\n",
    "        conflicting_edges_index1 = conflicting_triangles[:, indices] # [conflicts, 4]\n",
    "        \n",
    "        conflicting_edges_index2 = torch.empty_like(conflicting_triangles)\n",
    "        indices = torch.LongTensor([1, 2, 3, 3])\n",
    "        conflicting_edges_index2 = conflicting_triangles[:, indices] # [conflicts, 4]\n",
    "        \n",
    "        conflicting_edges = torch.cat([conflicting_edges_index0.view(-1).unsqueeze(-1), conflicting_edges_index1.view(-1).unsqueeze(-1), conflicting_edges_index2.view(-1).unsqueeze(-1)], dim = -1).reshape(-1, 3) # [conflicts * 4, 3]\n",
    "        \n",
    "        edge_batch_index = self.batch_index[incircle_test].unsqueeze(1).expand(-1, 4).reshape(-1) # [conflicts * 4]\n",
    "        \n",
    "        indices = torch.LongTensor([0, 2, 1])\n",
    "        comparison_edges = conflicting_edges[:, indices] # [conflicts * 4, 3]        \n",
    "        \n",
    "        unravel_nomatch_mask = torch.ones([conflicts * 4], dtype = torch.bool, device = device) # [conflicts * 4]\n",
    "        i = 1\n",
    "        while True:\n",
    "            \n",
    "            todo_mask = unravel_nomatch_mask[:-i].logical_and(edge_batch_index[:-i] == edge_batch_index[i:])\n",
    "            if i % 4 == 0:\n",
    "                if todo_mask.sum() == 0:\n",
    "                    break\n",
    "            \n",
    "            match_mask = todo_mask.clone()\n",
    "            match_mask[todo_mask] = (conflicting_edges[:-i][todo_mask] != comparison_edges[i:][todo_mask]).sum(-1).logical_not()\n",
    "            \n",
    "            unravel_nomatch_mask[:-i][match_mask] = False\n",
    "            unravel_nomatch_mask[i:][match_mask] = False\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        batch_newtriangles = unravel_nomatch_mask.sum()\n",
    "        \n",
    "        nomatch_edges = conflicting_edges[unravel_nomatch_mask] # [batch_newtriangles, 3], already in correct order to insert into 1,2,3 (since already anticlockwise from outside in)\n",
    "        assert list(nomatch_edges.size()) == [batch_newtriangles, 3]\n",
    "        nomatch_batch_index = edge_batch_index[unravel_nomatch_mask] # [batch_newtriangles]\n",
    "        \n",
    "        nomatch_newpoint = point_index[nomatch_batch_index] # [batch_newtriangles]\n",
    "        \n",
    "        newtriangles = torch.cat([nomatch_newpoint.unsqueeze(1), nomatch_edges], dim = -1) # [batch_newtriangles, 4]\n",
    "        \n",
    "        \n",
    "        nremoved_triangles = torch.zeros([self.batchsize], dtype = torch.long, device = device)\n",
    "        nnew_triangles = torch.zeros([self.batchsize], dtype = torch.long, device = device)\n",
    "        \n",
    "        indices = self.batch_index[incircle_test]\n",
    "        nremoved_triangles.put_(indices, torch.ones_like(indices, dtype = torch.long), accumulate = True) # [batchsize]\n",
    "        \n",
    "        indices = edge_batch_index[unravel_nomatch_mask]\n",
    "        nnew_triangles.put_(indices, torch.ones_like(indices, dtype = torch.long), accumulate = True) # [batchsize]\n",
    "        \n",
    "        assert (nnew_triangles <= 2 * nremoved_triangles + 2).logical_not().sum().logical_not()\n",
    "        \n",
    "        \"\"\"\n",
    "        NOTE:\n",
    "        I THINK it's possible for nnew_triangles to be less than nremoved_triangles (or my code is just buggy...)\n",
    "        \"\"\"\n",
    "        \n",
    "        assert nnew_triangles.sum() == batch_newtriangles\n",
    "        assert nremoved_triangles.sum() == incircle_test.sum()\n",
    "        \n",
    "        nadditional_triangles = nnew_triangles - nremoved_triangles # [batchsize]\n",
    "        ntriangles = self.ntriangles + nadditional_triangles # [batchsize]\n",
    "        \n",
    "        partial_delaunay_triangles = torch.empty([ntriangles.sum(), 4], dtype = torch.long, device = device)\n",
    "        batch_index = torch.empty([ntriangles.sum()], dtype = torch.long, device = device)\n",
    "        \n",
    "        cumulative_triangles = torch.cat([torch.zeros([1], dtype = torch.long, device = device), nnew_triangles.cumsum(0)[:-1]]) # [batchsize], cumulative sum starts at zero\n",
    "        \n",
    "        \"\"\"\n",
    "        since may actually have LESS triangles than previous round, we insert all that survive into the first slots (in that batch)\n",
    "        \"\"\"\n",
    "        good_triangle_indices = torch.arange(incircle_test.logical_not().sum(), dtype = torch.long, device = device)\n",
    "        good_triangle_indices += cumulative_triangles[self.batch_index[incircle_test.logical_not()]]\n",
    "        bad_triangle_indices_mask = torch.ones([ntriangles.sum(0)], dtype = torch.bool, device = device)\n",
    "        bad_triangle_indices_mask.scatter_(0, good_triangle_indices, False)\n",
    "        \n",
    "        assert good_triangle_indices.size(0) == incircle_test.logical_not().sum()\n",
    "        assert bad_triangle_indices_mask.sum() == batch_newtriangles\n",
    "        \n",
    "        partial_delaunay_triangles[good_triangle_indices] = self.partial_delaunay_triangles[~incircle_test]\n",
    "        batch_index[good_triangle_indices] = self.batch_index[~incircle_test]\n",
    "        \n",
    "        partial_delaunay_triangles[bad_triangle_indices_mask] = newtriangles\n",
    "        batch_index[bad_triangle_indices_mask] = nomatch_batch_index\n",
    "        \n",
    "        self.partial_delaunay_triangles = partial_delaunay_triangles\n",
    "        self.batch_index = batch_index\n",
    "        \n",
    "        self.ntriangles = ntriangles\n",
    "        self.batch_triangles = self.partial_delaunay_triangles.size(0)\n",
    "        \n",
    "        self.points_mask.scatter_(\n",
    "            1, point_index.unsqueeze(1).expand(-1, self.npoints), True\n",
    "        )\n",
    "        self.points_sequence = torch.cat(\n",
    "            [self.points_sequence, point_index.unsqueeze(1)], dim=1\n",
    "        )\n",
    "        \n",
    "        self.cost += nremoved_triangles\n",
    "        return\n",
    "    \n",
    "    def sample_point(self, logits): #logits must be [batchsize * nsamples, npoints]\n",
    "        probs = torch.distributions.categorical.Categorical(logits = logits)\n",
    "        next_point = probs.sample() #size is [batchsize * nsamples]\n",
    "        self.update(next_point)\n",
    "        self.logprob = self.logprob + probs.log_prob(next_point)\n",
    "        return next_point #[batchsize * nsamples]\n",
    "    \n",
    "    def sampleandgreedy_point(self, logits): #logits must be [batchsize * nsamples, npoints], last sample will be the greedy choice (but we still need to keep track of its logits)\n",
    "        logits_sample = logits.view(-1, self.nsamples, self.npoints)[:, :-1, :]\n",
    "        probs = torch.distributions.categorical.Categorical(logits = logits_sample)\n",
    "        \n",
    "        sample_point = probs.sample() #[batchsize, (nsamples - 1)]\n",
    "        greedy_point = logits.view(-1, self.nsamples, self.npoints)[:, -1, :].max(-1, keepdim = True)[1] #[batchsize, 1]\n",
    "        next_point = torch.cat([sample_point, greedy_point], dim = 1).view(-1)\n",
    "        self.update(next_point)\n",
    "        self.logprob = self.logprob + torch.cat([probs.log_prob(sample_point), torch.zeros([sample_point.size(0), 1], device = device)], dim = 1).view(-1)\n",
    "        return next_point\n",
    "    \n",
    "\n",
    "env = environment()\n",
    "\n",
    "\n",
    "def train(epochs = 30000, npoints = 14, batchsize = 100, nsamples = 8):\n",
    "    NN.train()\n",
    "    for i in range(epochs):\n",
    "        env.reset(npoints, batchsize, nsamples)\n",
    "        \"\"\"include the boundary points, kinda makes sense that they should contribute (atm only in the encoder, difficult to see how in the decoder)\"\"\"\n",
    "        memory = NN.encode(env.points) #[npoints, batchsize * nsamples, emsize]\n",
    "        #### #### #### remember to include tgt.detach() when reinstate with torch.no_grad()\n",
    "        tgt = NN.start_token.unsqueeze(0).unsqueeze(1).expand(1, batchsize * nsamples, -1).detach() #[1, batchsize * nsamples, emsize]\n",
    "        #with torch.no_grad(): #to speed up computation, selecting routes is done without gradient\n",
    "        with torch.no_grad():\n",
    "            for j in range(4, npoints):\n",
    "                #### #### #### remember to include memory.detach() when reinstate with torch.no_grad()\n",
    "                _, logits = NN.decode_next(memory.detach(), tgt, env.points_mask)\n",
    "                next_point = env.sampleandgreedy_point(logits)\n",
    "                \"\"\"\n",
    "                for inputing the previous embedding into decoder\n",
    "                \"\"\"\n",
    "                tgt = torch.cat([tgt, memory.gather(0, next_point.unsqueeze(0).unsqueeze(2).expand(1, -1, memory.size(2)))]) #[nsofar, batchsize * nsamples, emsize]\n",
    "                \"\"\"\n",
    "                for inputing the previous decoder output into the decoder (allows for an evolving strategy, but doesn't allow for fast training\n",
    "                \"\"\"\n",
    "                ############\n",
    "\n",
    "        \n",
    "        NN.eval()\n",
    "        _, logprob = NN.calculate_logprob(memory, env.points_sequence) #[batchsize * nsamples]\n",
    "        NN.train()\n",
    "        \"\"\"\n",
    "        clip logprob so doesn't reinforce things it already knows\n",
    "        TBH WANT SOMETHING DIFFERENT ... want to massively increase training if find something unexpected and otherwise not\n",
    "        \"\"\"\n",
    "        greedy_prob = logprob.view(batchsize, nsamples)[:, -1].detach() #[batchsize]\n",
    "        greedy_baseline = env.cost.view(batchsize, nsamples)[:, -1] #[batchsize], greedy sample\n",
    "        fixed_baseline = 0.5 * torch.ones([1], device = device)\n",
    "        min_baseline = env.cost.view(batchsize, nsamples)[:, :-1].min(-1)[0] #[batchsize], minimum cost\n",
    "        baseline = greedy_baseline\n",
    "        positive_reinforcement = - F.relu( - (env.cost.view(batchsize, nsamples)[:, :-1] - baseline.unsqueeze(1))) #don't scale positive reinforcement\n",
    "        negative_reinforcement = F.relu(env.cost.view(batchsize, nsamples)[:, :-1] - baseline.unsqueeze(1))\n",
    "        positive_reinforcement_binary = env.cost.view(batchsize, nsamples)[:, :-1] - baseline.unsqueeze(1) <= -0.05\n",
    "        negative_reinforcement_binary = env.cost.view(batchsize, nsamples)[:, :-1] - baseline.unsqueeze(1) > 1\n",
    "        \"\"\"\n",
    "        binary positive reinforcement\n",
    "        \"\"\"\n",
    "        #loss = - ((logprob.view(batchsize, nsamples)[:, :-1] < -0.2) * logprob.view(batchsize, nsamples)[:, :-1] * positive_reinforcement_binary).mean() #+ (logprob.view(batchsize, nsamples)[:, :-1] > -1) * logprob.view(batchsize, nsamples)[:, :-1] * negative_reinforcement_binary\n",
    "        \"\"\"\n",
    "        clipped binary reinforcement\n",
    "        \"\"\"\n",
    "        loss = ( \n",
    "                #- logprob.view(batchsize, nsamples)[:, :-1] \n",
    "                #* (logprob.view(batchsize, nsamples)[:, :-1] < 0) \n",
    "                #* positive_reinforcement_binary \n",
    "                logprob.view(batchsize, nsamples)[:, :-1] \n",
    "                * (logprob.view(batchsize, nsamples)[:, :-1] > greedy_prob.unsqueeze(1) - 2) \n",
    "                * negative_reinforcement_binary \n",
    "        ).mean()\n",
    "        \"\"\"\n",
    "        clipped binary postive, clipped weighted negative\n",
    "        \"\"\"\n",
    "        #loss = ( - logprob.view(batchsize, nsamples)[:, :-1] * (logprob.view(batchsize, nsamples)[:, :-1] < -0.2) * positive_reinforcement_binary + logprob.view(batchsize, nsamples)[:, :-1] * (logprob.view(batchsize, nsamples)[:, :-1] > -2) * negative_reinforcement ).mean()\n",
    "        \"\"\"\n",
    "        clipped reinforcement without rescaling\n",
    "        \"\"\"\n",
    "        #loss = ((logprob.view(batchsize, nsamples)[:, :-1] < -0.7) * logprob.view(batchsize, nsamples)[:, :-1] * positive_reinforcement + (logprob.view(batchsize, nsamples)[:, :-1] > -5) * logprob.view(batchsize, nsamples)[:, :-1] * negative_reinforcement).mean()\n",
    "        \"\"\"\n",
    "        clipped reinforcement\n",
    "        \"\"\"\n",
    "        #loss = (logprob.view(batchsize, nsamples)[:, :-1] * positive_reinforcement / (positive_reinforcement.var() + 0.001).sqrt() + (logprob.view(batchsize, nsamples)[:, :-1] > -3) * logprob.view(batchsize, nsamples)[:, :-1] * negative_reinforcement / (negative_reinforcement.var() + 0.001).sqrt()).mean()\n",
    "        \"\"\"\n",
    "        balanced reinforcement\n",
    "        \"\"\"\n",
    "        #loss = (logprob.view(batchsize, nsamples)[:, :-1] * (positive_reinforcement / (positive_reinforcement.var() + 0.001).sqrt() + negative_reinforcement / (negative_reinforcement.var() + 0.001).sqrt())).mean()\n",
    "        \"\"\"\n",
    "        regular loss\n",
    "        \"\"\"\n",
    "        #loss = (logprob.view(batchsize, nsamples)[:, :-1] * (positive_reinforcement + negative_reinforcement)).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #print(NN.encoder.weight.grad)\n",
    "        optimizer.step()\n",
    "        #print(greedy_baseline.mean().item())\n",
    "        print(greedy_baseline.mean().item(), logprob.view(batchsize, nsamples)[:, -1].mean().item(), logprob.view(batchsize, nsamples)[:, :-1].mean().item(), logprob[batchsize - 1].item(), logprob[0].item(), env.logprob[0].item())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-charger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.81100082397461 -4.355165958404541 -4.764427185058594 -4.496849060058594 -5.217916965484619 -5.217916965484619\n",
      "14.209250450134277 -3.98140287399292 -4.685207366943359 -4.044405937194824 -4.052271366119385 -4.052271366119385\n",
      "13.947500228881836 -3.4520277976989746 -4.442547798156738 -3.7595489025115967 -5.641497611999512 -5.641497611999512\n",
      "13.751250267028809 -3.703979730606079 -4.579899311065674 -3.5851497650146484 -4.438642978668213 -4.438643455505371\n",
      "13.901500701904297 -3.7681872844696045 -4.611804008483887 -3.6520771980285645 -4.24777889251709 -4.247779846191406\n",
      "13.940500259399414 -3.6810290813446045 -4.567259311676025 -3.6922502517700195 -4.820395469665527 -4.820395469665527\n",
      "13.831750869750977 -3.5861833095550537 -4.520603179931641 -3.6169075965881348 -4.984911918640137 -4.9849114418029785\n",
      "13.737500190734863 -3.6414926052093506 -4.550784587860107 -3.21937894821167 -4.0261735916137695 -4.026174545288086\n",
      "13.799500465393066 -3.6854143142700195 -4.566110610961914 -3.6325719356536865 -4.873350143432617 -4.873350143432617\n",
      "13.679500579833984 -3.609118938446045 -4.547338485717773 -3.287051200866699 -3.8091907501220703 -3.809190511703491\n",
      "13.656500816345215 -3.497230052947998 -4.492953777313232 -3.303633689880371 -4.448080539703369 -4.448080539703369\n",
      "13.681750297546387 -3.5778696537017822 -4.5441203117370605 -2.898130416870117 -4.119593143463135 -4.119593143463135\n",
      "13.686250686645508 -3.6332790851593018 -4.585760116577148 -3.676590919494629 -4.142988204956055 -4.1429877281188965\n",
      "13.578500747680664 -3.5959794521331787 -4.577075958251953 -3.8945603370666504 -4.417386531829834 -4.417386054992676\n",
      "13.550750732421875 -3.5422544479370117 -4.558576583862305 -3.831935167312622 -4.523628234863281 -4.5236287117004395\n",
      "13.534250259399414 -3.5514824390411377 -4.571316719055176 -3.626577377319336 -4.50042200088501 -4.50042200088501\n",
      "13.497750282287598 -3.571526527404785 -4.584464073181152 -3.244734287261963 -4.32185173034668 -4.32185173034668\n",
      "13.50825023651123 -3.5487282276153564 -4.5683207511901855 -3.143904685974121 -5.575742244720459 -5.575742244720459\n",
      "13.426000595092773 -3.492936611175537 -4.554677486419678 -4.216015338897705 -3.716585874557495 -3.716585636138916\n",
      "13.36050033569336 -3.512521505355835 -4.558263301849365 -4.189822196960449 -4.921965599060059 -4.921965599060059\n",
      "13.375500679016113 -3.567772626876831 -4.571156024932861 -3.2676992416381836 -4.765637397766113 -4.765637397766113\n",
      "13.3097505569458 -3.576875686645508 -4.580785751342773 -3.555250644683838 -4.330661296844482 -4.330661773681641\n",
      "13.277501106262207 -3.519172430038452 -4.564093589782715 -3.334385395050049 -3.5033068656921387 -3.5033068656921387\n",
      "13.256250381469727 -3.45379900932312 -4.530061721801758 -3.8556060791015625 -3.648219108581543 -3.648219585418701\n",
      "13.257500648498535 -3.432145595550537 -4.541962146759033 -3.484018325805664 -5.23068380355835 -5.23068380355835\n",
      "13.249751091003418 -3.470970630645752 -4.5493621826171875 -4.112667083740234 -5.074294567108154 -5.0742950439453125\n",
      "13.262001037597656 -3.489274740219116 -4.563719749450684 -4.140090465545654 -4.0794219970703125 -4.079421520233154\n",
      "13.275250434875488 -3.528360605239868 -4.581313610076904 -3.3249917030334473 -4.793168544769287 -4.793168544769287\n",
      "13.197500228881836 -3.5246284008026123 -4.580331802368164 -3.572108745574951 -5.346500873565674 -5.346500396728516\n",
      "13.227750778198242 -3.4585914611816406 -4.555299282073975 -3.4904258251190186 -3.457979202270508 -3.4579789638519287\n",
      "13.228500366210938 -3.423553228378296 -4.544546127319336 -2.9637649059295654 -5.414982795715332 -5.41498327255249\n",
      "13.18950080871582 -3.442107677459717 -4.549139022827148 -3.034400463104248 -4.802153587341309 -4.802154064178467\n",
      "13.261000633239746 -3.4885942935943604 -4.5816192626953125 -3.0060651302337646 -4.420767784118652 -4.420767307281494\n",
      "13.240750312805176 -3.4939801692962646 -4.583830833435059 -3.9401602745056152 -3.2308804988861084 -3.2308804988861084\n",
      "13.148500442504883 -3.462167739868164 -4.567868232727051 -3.7421162128448486 -4.044015407562256 -4.044015407562256\n",
      "13.159000396728516 -3.4209699630737305 -4.558807849884033 -3.5911006927490234 -4.0720367431640625 -4.0720367431640625\n",
      "13.150750160217285 -3.3873536586761475 -4.548271179199219 -3.493589401245117 -5.282688140869141 -5.282688140869141\n",
      "13.133501052856445 -3.430187225341797 -4.561611652374268 -2.8416919708251953 -4.950876712799072 -4.950876712799072\n",
      "13.13425064086914 -3.4915475845336914 -4.586239814758301 -3.895845413208008 -4.700563907623291 -4.700564384460449\n",
      "13.077750205993652 -3.483013391494751 -4.584049701690674 -4.0368571281433105 -5.392188549041748 -5.392189025878906\n",
      "13.098250389099121 -3.3954741954803467 -4.5389814376831055 -3.66534161567688 -4.798126220703125 -4.798126220703125\n",
      "13.149250984191895 -3.3525984287261963 -4.523097515106201 -2.9453415870666504 -5.105347633361816 -5.105347633361816\n",
      "13.17025089263916 -3.4394729137420654 -4.562401294708252 -3.892641305923462 -5.661256790161133 -5.661256313323975\n",
      "13.067000389099121 -3.5126006603240967 -4.594322681427002 -3.841416835784912 -4.378533363342285 -4.378533840179443\n",
      "13.105000495910645 -3.404346227645874 -4.551224708557129 -3.1333847045898438 -4.891647815704346 -4.8916473388671875\n",
      "13.173501014709473 -3.3040153980255127 -4.499034881591797 -3.402122974395752 -3.4273288249969482 -3.4273288249969482\n",
      "13.099000930786133 -3.386082410812378 -4.5334649085998535 -3.131406784057617 -4.8862080574035645 -4.8862080574035645\n",
      "13.142000198364258 -3.499678611755371 -4.580524444580078 -3.980311393737793 -4.718483924865723 -4.718482971191406\n",
      "13.212000846862793 -3.444664716720581 -4.567874908447266 -3.265972852706909 -3.235506296157837 -3.235506534576416\n",
      "13.168500900268555 -3.305314064025879 -4.490571022033691 -2.8667550086975098 -3.8191165924072266 -3.8191165924072266\n",
      "13.071500778198242 -3.296720504760742 -4.489505290985107 -2.9855399131774902 -4.512282848358154 -4.5122833251953125\n",
      "13.094500541687012 -3.4130656719207764 -4.56544303894043 -3.402845859527588 -4.901091575622559 -4.901091575622559\n",
      "13.089750289916992 -3.478872776031494 -4.590516090393066 -2.9509005546569824 -5.4373393058776855 -5.437338829040527\n",
      "13.031500816345215 -3.4324111938476562 -4.566692352294922 -3.487048625946045 -5.318192005157471 -5.3181915283203125\n",
      "13.07075023651123 -3.3087618350982666 -4.507928848266602 -2.8284544944763184 -5.339873313903809 -5.339873313903809\n",
      "13.090250968933105 -3.3087921142578125 -4.500443935394287 -3.097468614578247 -4.093197822570801 -4.093197345733643\n",
      "13.088750839233398 -3.4072999954223633 -4.534650802612305 -3.5509376525878906 -4.832705020904541 -4.832705020904541\n",
      "13.002500534057617 -3.4938273429870605 -4.5927534103393555 -3.5595812797546387 -4.867591857910156 -4.867591857910156\n",
      "13.052000999450684 -3.432507276535034 -4.5609822273254395 -3.4536871910095215 -5.4967427253723145 -5.496743202209473\n",
      "13.017251014709473 -3.3082218170166016 -4.509901523590088 -3.2517056465148926 -4.328562259674072 -4.328562259674072\n",
      "12.967000961303711 -3.259629964828491 -4.490841388702393 -2.9709420204162598 -5.463078498840332 -5.463078022003174\n",
      "12.969500541687012 -3.305894136428833 -4.522397041320801 -3.3190956115722656 -4.801875591278076 -4.801876068115234\n",
      "12.97700023651123 -3.4045231342315674 -4.569937229156494 -3.809088706970215 -4.936302185058594 -4.936302185058594\n",
      "13.030000686645508 -3.449915647506714 -4.578842639923096 -3.7192869186401367 -4.626891136169434 -4.626890659332275\n",
      "13.033750534057617 -3.387054920196533 -4.552046298980713 -3.2479817867279053 -3.743549346923828 -3.743549346923828\n",
      "13.071001052856445 -3.3041300773620605 -4.496099472045898 -4.1974101066589355 -4.208632469177246 -4.208631992340088\n",
      "13.037750244140625 -3.297908306121826 -4.489200115203857 -3.4567251205444336 -3.4324867725372314 -3.4324870109558105\n",
      "12.936250686645508 -3.3584134578704834 -4.5307087898254395 -3.348729133605957 -3.769939661026001 -3.76993989944458\n",
      "13.001750946044922 -3.420332670211792 -4.567429065704346 -3.772251605987549 -4.241316795349121 -4.241316795349121\n",
      "12.974750518798828 -3.4100282192230225 -4.561263561248779 -3.2269654273986816 -4.207538604736328 -4.207538604736328\n",
      "13.008750915527344 -3.3456597328186035 -4.53253698348999 -3.1920313835144043 -4.252317428588867 -4.252317428588867\n",
      "12.967500686645508 -3.273880958557129 -4.503519535064697 -3.187394142150879 -5.863550186157227 -5.863549709320068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.938750267028809 -3.2952051162719727 -4.503719329833984 -3.5998244285583496 -3.656522750854492 -3.6565229892730713\n",
      "12.959500312805176 -3.358415126800537 -4.537306308746338 -2.8582990169525146 -4.868377208709717 -4.868376731872559\n",
      "12.946250915527344 -3.395904064178467 -4.563118934631348 -3.465453863143921 -3.5217812061309814 -3.5217812061309814\n",
      "12.95150089263916 -3.3769211769104004 -4.557080268859863 -3.0793795585632324 -5.246711254119873 -5.246711254119873\n",
      "12.9660005569458 -3.3150269985198975 -4.524441242218018 -3.4398999214172363 -5.117192268371582 -5.117191791534424\n",
      "12.987500190734863 -3.281057357788086 -4.509886741638184 -3.6234941482543945 -3.427922487258911 -3.4279227256774902\n",
      "12.899250984191895 -3.296741485595703 -4.517134189605713 -3.4447503089904785 -2.8740234375 -2.874023675918579\n",
      "12.91550064086914 -3.361137866973877 -4.540760040283203 -3.9347541332244873 -3.7181124687194824 -3.718111991882324\n",
      "12.90475082397461 -3.3773088455200195 -4.550600051879883 -3.5432474613189697 -3.0795955657958984 -3.0795960426330566\n",
      "12.991500854492188 -3.360240936279297 -4.538114547729492 -3.8531289100646973 -5.154742240905762 -5.15474271774292\n",
      "12.87125015258789 -3.3296356201171875 -4.520068168640137 -3.2130024433135986 -5.096502780914307 -5.096503257751465\n",
      "12.897000312805176 -3.327970504760742 -4.5244855880737305 -3.503692626953125 -4.04625129699707 -4.0462517738342285\n",
      "12.896750450134277 -3.32285475730896 -4.515621662139893 -3.5222432613372803 -4.154541969299316 -4.154541969299316\n",
      "12.867000579833984 -3.325467348098755 -4.52632474899292 -3.2408103942871094 -4.9913105964660645 -4.991310119628906\n",
      "12.874000549316406 -3.330073833465576 -4.527407169342041 -3.399834632873535 -3.395350933074951 -3.395350694656372\n",
      "12.938000679016113 -3.347287654876709 -4.549971580505371 -2.6908626556396484 -3.689089775085449 -3.689089775085449\n",
      "12.888500213623047 -3.3147902488708496 -4.518879413604736 -3.8665194511413574 -3.956819534301758 -3.956819534301758\n",
      "12.869250297546387 -3.293797731399536 -4.517082691192627 -3.347620964050293 -3.5749380588531494 -3.5749382972717285\n",
      "12.868000984191895 -3.3004908561706543 -4.528670310974121 -3.44204044342041 -5.04657506942749 -5.046575546264648\n",
      "12.779500961303711 -3.294308662414551 -4.531632423400879 -3.0972132682800293 -5.908568859100342 -5.908568859100342\n",
      "12.837750434875488 -3.3119776248931885 -4.532433032989502 -3.4434094429016113 -3.5823001861572266 -3.5823001861572266\n",
      "12.813000679016113 -3.3012335300445557 -4.522935390472412 -3.166276216506958 -5.272564888000488 -5.272564888000488\n",
      "12.88525104522705 -3.3143885135650635 -4.525329113006592 -2.9894814491271973 -5.699637413024902 -5.699637413024902\n",
      "12.847250938415527 -3.3293864727020264 -4.529565334320068 -3.7356109619140625 -5.298957824707031 -5.2989583015441895\n",
      "12.823750495910645 -3.3493683338165283 -4.541051387786865 -2.9533116817474365 -5.282472610473633 -5.282472610473633\n",
      "12.780750274658203 -3.326301097869873 -4.5332465171813965 -3.68678617477417 -3.842679500579834 -3.842679500579834\n",
      "12.884500503540039 -3.3381872177124023 -4.539831161499023 -3.361423969268799 -4.868724822998047 -4.868724822998047\n",
      "12.771750450134277 -3.313145637512207 -4.537993431091309 -3.6026453971862793 -5.621814250946045 -5.621814250946045\n",
      "12.856000900268555 -3.316380739212036 -4.527404308319092 -3.7081258296966553 -3.3826816082000732 -3.382681369781494\n",
      "12.816000938415527 -3.3032522201538086 -4.52867317199707 -3.176112413406372 -4.698184013366699 -4.698184490203857\n",
      "12.796751022338867 -3.2937471866607666 -4.527139186859131 -3.735245943069458 -3.137822151184082 -3.137822151184082\n",
      "12.73175048828125 -3.287323236465454 -4.525457859039307 -3.4863386154174805 -4.304670333862305 -4.304670333862305\n",
      "12.746000289916992 -3.31657338142395 -4.540879726409912 -3.156766414642334 -4.2364501953125 -4.2364501953125\n",
      "12.741750717163086 -3.327280044555664 -4.540677070617676 -3.0704808235168457 -4.225392818450928 -4.225392818450928\n",
      "12.759750366210938 -3.321790933609009 -4.5326247215271 -3.204305648803711 -4.910151958465576 -4.910151958465576\n",
      "12.771500587463379 -3.3071281909942627 -4.532013416290283 -2.9748952388763428 -4.290626525878906 -4.290626525878906\n",
      "12.784000396728516 -3.297365427017212 -4.524322986602783 -3.915912628173828 -5.278279781341553 -5.278279781341553\n",
      "12.749000549316406 -3.313174247741699 -4.534328460693359 -3.5602433681488037 -3.9401793479919434 -3.940178871154785\n",
      "12.797500610351562 -3.362474203109741 -4.559754371643066 -3.5413284301757812 -4.440556526184082 -4.44055700302124\n",
      "12.768250465393066 -3.356489896774292 -4.552741527557373 -3.046600818634033 -4.747857570648193 -4.747857570648193\n",
      "12.733500480651855 -3.3131375312805176 -4.520691871643066 -3.2012736797332764 -4.387893199920654 -4.387892723083496\n",
      "12.724250793457031 -3.32150936126709 -4.5302815437316895 -3.415493965148926 -5.284089088439941 -5.284089088439941\n",
      "12.717750549316406 -3.331990957260132 -4.535638332366943 -3.0839366912841797 -3.1019461154937744 -3.1019465923309326\n",
      "12.744000434875488 -3.321659564971924 -4.538401126861572 -3.653437614440918 -5.595580101013184 -5.595580101013184\n",
      "12.683500289916992 -3.3023643493652344 -4.546932220458984 -3.2087490558624268 -5.351789474487305 -5.351789474487305\n",
      "12.692501068115234 -3.2912585735321045 -4.5275702476501465 -3.479689598083496 -5.001837253570557 -5.001836776733398\n",
      "12.691750526428223 -3.3097944259643555 -4.541314601898193 -3.1041364669799805 -4.032959938049316 -4.032959938049316\n",
      "12.696500778198242 -3.3115134239196777 -4.5379862785339355 -3.284813404083252 -5.554172515869141 -5.554171562194824\n",
      "12.738250732421875 -3.3058972358703613 -4.545798301696777 -3.7815046310424805 -3.231567144393921 -3.231567144393921\n",
      "12.76300048828125 -3.3018102645874023 -4.530246734619141 -3.247103452682495 -3.8293771743774414 -3.8293776512145996\n",
      "12.670500755310059 -3.3000669479370117 -4.532641887664795 -3.4315335750579834 -5.238922119140625 -5.238922119140625\n",
      "12.706501007080078 -3.334845781326294 -4.543689727783203 -3.3835484981536865 -4.732234477996826 -4.732234477996826\n",
      "12.691000938415527 -3.315093517303467 -4.524488925933838 -2.869107961654663 -5.432008743286133 -5.432008743286133\n",
      "12.699501037597656 -3.3028080463409424 -4.524179458618164 -2.8394761085510254 -4.507364273071289 -4.507364273071289\n",
      "12.721500396728516 -3.309295415878296 -4.528347492218018 -3.1142992973327637 -3.340667724609375 -3.340667247772217\n",
      "12.726750373840332 -3.3225295543670654 -4.547231674194336 -2.7434136867523193 -3.7917537689208984 -3.7917537689208984\n",
      "12.647000312805176 -3.313612222671509 -4.540382385253906 -2.810638904571533 -5.7730512619018555 -5.7730512619018555\n",
      "12.69575023651123 -3.284762382507324 -4.530890464782715 -3.3106276988983154 -4.914987564086914 -4.914988040924072\n",
      "12.671751022338867 -3.269531011581421 -4.524954795837402 -2.96763277053833 -5.377937316894531 -5.377938270568848\n",
      "12.647000312805176 -3.2824244499206543 -4.5393571853637695 -3.2343108654022217 -4.044576644897461 -4.044576644897461\n",
      "12.691250801086426 -3.312727689743042 -4.54013729095459 -3.6331634521484375 -5.386917591094971 -5.3869171142578125\n",
      "12.655500411987305 -3.31424880027771 -4.547012805938721 -3.256225109100342 -5.206258296966553 -5.206258296966553\n",
      "12.62125015258789 -3.303842544555664 -4.538017272949219 -3.368480682373047 -5.1549763679504395 -5.1549763679504395\n",
      "12.652000427246094 -3.26652455329895 -4.516221046447754 -3.4444165229797363 -5.21214485168457 -5.212144374847412\n",
      "12.658750534057617 -3.2882487773895264 -4.537852764129639 -3.7104251384735107 -5.3209052085876465 -5.320905685424805\n",
      "12.678000450134277 -3.3531503677368164 -4.563564300537109 -3.5513381958007812 -3.6547183990478516 -3.6547183990478516\n",
      "12.709000587463379 -3.365429639816284 -4.568356037139893 -3.688049793243408 -3.259772300720215 -3.259772777557373\n",
      "12.59850025177002 -3.2999470233917236 -4.547173976898193 -3.3507730960845947 -4.506133556365967 -4.506134033203125\n",
      "12.693000793457031 -3.273167610168457 -4.523231029510498 -3.086435317993164 -5.317320823669434 -5.317320823669434\n",
      "12.605751037597656 -3.280625343322754 -4.5326080322265625 -3.8818156719207764 -4.568981647491455 -4.568982124328613\n",
      "12.5885009765625 -3.337634325027466 -4.559350967407227 -3.6203970909118652 -4.24691104888916 -4.246911525726318\n",
      "12.647000312805176 -3.3458805084228516 -4.550426959991455 -3.1061384677886963 -5.097433090209961 -5.097433090209961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.597750663757324 -3.3290855884552 -4.556053638458252 -3.5829076766967773 -5.236530303955078 -5.236530780792236\n",
      "12.573250770568848 -3.273120403289795 -4.533703804016113 -3.61751127243042 -3.537630081176758 -3.537630081176758\n",
      "12.630250930786133 -3.277452230453491 -4.540175914764404 -3.026517868041992 -3.903585433959961 -3.903585910797119\n",
      "12.512750625610352 -3.290072441101074 -4.546700954437256 -3.1669580936431885 -3.4593443870544434 -3.4593443870544434\n",
      "12.586000442504883 -3.3109278678894043 -4.556080341339111 -2.998535394668579 -5.022104263305664 -5.022104263305664\n",
      "12.577250480651855 -3.318429470062256 -4.55328893661499 -3.520061731338501 -4.214509010314941 -4.214509963989258\n",
      "12.601500511169434 -3.313441276550293 -4.545118808746338 -3.0740795135498047 -2.9537174701690674 -2.9537174701690674\n",
      "12.624750137329102 -3.2792410850524902 -4.530512809753418 -3.029961585998535 -4.566949367523193 -4.566949367523193\n",
      "12.59850025177002 -3.319573402404785 -4.539305686950684 -3.635241985321045 -3.890763282775879 -3.890763282775879\n",
      "12.583250999450684 -3.3576197624206543 -4.572887897491455 -3.059035062789917 -5.992167949676514 -5.992167949676514\n",
      "12.574000358581543 -3.3451919555664062 -4.558701515197754 -3.0472311973571777 -3.353104591369629 -3.353104829788208\n",
      "12.580000877380371 -3.2906501293182373 -4.5417656898498535 -2.8176145553588867 -5.131178379058838 -5.131178855895996\n",
      "12.539751052856445 -3.272876024246216 -4.532176494598389 -2.798516273498535 -5.696962356567383 -5.696961879730225\n",
      "12.594500541687012 -3.275521993637085 -4.54641580581665 -3.1832470893859863 -5.160299777984619 -5.160299777984619\n",
      "12.594000816345215 -3.3230464458465576 -4.552362442016602 -2.8526358604431152 -4.639630317687988 -4.639630317687988\n",
      "12.536251068115234 -3.346160650253296 -4.563652515411377 -2.936446189880371 -3.493868827819824 -3.493868827819824\n",
      "12.586250305175781 -3.3294239044189453 -4.558657646179199 -3.099982976913452 -3.7071685791015625 -3.7071685791015625\n",
      "12.59575080871582 -3.2719995975494385 -4.528885364532471 -3.4190571308135986 -4.758362770080566 -4.758362293243408\n",
      "12.601500511169434 -3.2682394981384277 -4.520022392272949 -3.3288283348083496 -4.329883098602295 -4.329883098602295\n",
      "12.55250072479248 -3.3260743618011475 -4.554812908172607 -3.0053622722625732 -6.253388404846191 -6.253387451171875\n",
      "12.59225082397461 -3.350409746170044 -4.567717552185059 -3.3537726402282715 -3.33990216255188 -3.339902400970459\n",
      "12.577250480651855 -3.315800428390503 -4.554116725921631 -3.6306169033050537 -4.663695335388184 -4.663694858551025\n",
      "12.597500801086426 -3.274291753768921 -4.531022548675537 -3.3276634216308594 -4.023508071899414 -4.023508071899414\n",
      "12.534250259399414 -3.275744915008545 -4.533947944641113 -3.4779984951019287 -4.5892133712768555 -4.5892133712768555\n",
      "12.561250686645508 -3.298665761947632 -4.542789459228516 -3.4957311153411865 -3.834221363067627 -3.834221363067627\n",
      "12.549250602722168 -3.309243679046631 -4.550314903259277 -3.3837525844573975 -5.003382205963135 -5.003381729125977\n",
      "12.542250633239746 -3.311269760131836 -4.537508964538574 -3.6079225540161133 -3.810213804244995 -3.810213804244995\n",
      "12.579751014709473 -3.279266595840454 -4.532762050628662 -3.2651548385620117 -3.7348480224609375 -3.7348477840423584\n",
      "12.551750183105469 -3.2240443229675293 -4.513017654418945 -3.1365880966186523 -5.32037353515625 -5.32037353515625\n",
      "12.589000701904297 -3.285139322280884 -4.538936138153076 -3.611392021179199 -5.627562522888184 -5.627562522888184\n",
      "12.570500373840332 -3.3488593101501465 -4.574594497680664 -3.202061653137207 -4.991490840911865 -4.991490840911865\n",
      "12.6222505569458 -3.3422224521636963 -4.57839298248291 -3.5233845710754395 -5.19014310836792 -5.190142631530762\n",
      "12.573750495910645 -3.286141872406006 -4.542504787445068 -3.4162402153015137 -3.928633213043213 -3.928633213043213\n",
      "12.52400016784668 -3.256356716156006 -4.531742572784424 -3.352607011795044 -4.2875213623046875 -4.287520885467529\n",
      "12.570250511169434 -3.2646567821502686 -4.517608642578125 -3.201962471008301 -5.341776371002197 -5.3417768478393555\n",
      "12.473250389099121 -3.319082260131836 -4.549087047576904 -3.6304378509521484 -4.6398606300354 -4.639861106872559\n",
      "12.509000778198242 -3.364366292953491 -4.579545021057129 -2.7723662853240967 -3.9264960289001465 -3.9264960289001465\n",
      "12.503750801086426 -3.3425631523132324 -4.563628196716309 -3.1752450466156006 -3.932398796081543 -3.932398796081543\n",
      "12.481500625610352 -3.2743489742279053 -4.543271064758301 -3.636298179626465 -4.865265846252441 -4.8652663230896\n",
      "12.50475025177002 -3.2602903842926025 -4.530008316040039 -3.353299856185913 -4.737815856933594 -4.737815856933594\n",
      "12.500000953674316 -3.3046875 -4.55174446105957 -3.57966947555542 -4.688010215759277 -4.688010215759277\n",
      "12.518000602722168 -3.3217685222625732 -4.564428329467773 -3.0569441318511963 -4.921350955963135 -4.921350955963135\n",
      "12.520000457763672 -3.3433496952056885 -4.573821544647217 -3.0168776512145996 -4.403099060058594 -4.4030985832214355\n",
      "12.535000801086426 -3.28183913230896 -4.542882442474365 -3.195878028869629 -5.281782627105713 -5.281783103942871\n",
      "12.580750465393066 -3.2369983196258545 -4.506012916564941 -2.8038110733032227 -4.616715431213379 -4.616714954376221\n",
      "12.448250770568848 -3.2450900077819824 -4.526035785675049 -3.0872128009796143 -4.393101692199707 -4.393101692199707\n",
      "12.447000503540039 -3.327265739440918 -4.567280292510986 -3.494058132171631 -4.900477409362793 -4.900477886199951\n",
      "12.49625015258789 -3.3563010692596436 -4.579293727874756 -3.5517685413360596 -4.298567771911621 -4.298567771911621\n",
      "12.530750274658203 -3.329169988632202 -4.5685343742370605 -2.882230758666992 -4.657824993133545 -4.657825469970703\n",
      "12.534250259399414 -3.252336263656616 -4.5461812019348145 -3.5209598541259766 -5.264285087585449 -5.264284610748291\n",
      "12.607000350952148 -3.2365126609802246 -4.520178318023682 -2.708418846130371 -4.806587219238281 -4.8065876960754395\n",
      "12.530500411987305 -3.2566568851470947 -4.522279739379883 -3.6349263191223145 -4.005027770996094 -4.005027770996094\n",
      "12.548750877380371 -3.3097612857818604 -4.557617664337158 -3.576298236846924 -5.182584285736084 -5.182584285736084\n",
      "12.46150016784668 -3.358268976211548 -4.573932647705078 -3.3672432899475098 -3.0377697944641113 -3.037769317626953\n",
      "12.513250350952148 -3.32468318939209 -4.563633441925049 -3.294261932373047 -5.279259204864502 -5.279258728027344\n",
      "12.489750862121582 -3.2802510261535645 -4.540760517120361 -3.2833290100097656 -4.637866020202637 -4.637866020202637\n",
      "12.502250671386719 -3.2474563121795654 -4.522063732147217 -3.309182643890381 -4.024021148681641 -4.024021148681641\n",
      "12.530750274658203 -3.267624616622925 -4.540390968322754 -3.1779332160949707 -5.381782054901123 -5.381782531738281\n",
      "12.503000259399414 -3.3423311710357666 -4.566718578338623 -3.2863047122955322 -5.0263285636901855 -5.026328086853027\n",
      "12.48900032043457 -3.3709404468536377 -4.573247909545898 -2.883725643157959 -5.557600021362305 -5.557600021362305\n",
      "12.520500183105469 -3.3242011070251465 -4.56122350692749 -3.2899818420410156 -4.79421854019165 -4.794218063354492\n",
      "12.47700023651123 -3.2391583919525146 -4.522343635559082 -3.979217052459717 -4.8700385093688965 -4.870038986206055\n",
      "12.446500778198242 -3.2455639839172363 -4.525259494781494 -2.9068379402160645 -5.113587379455566 -5.113587379455566\n",
      "12.461250305175781 -3.2880020141601562 -4.552084922790527 -3.4935989379882812 -3.9589805603027344 -3.9589805603027344\n",
      "12.530000686645508 -3.320565700531006 -4.555601596832275 -3.6786398887634277 -3.5454859733581543 -3.5454859733581543\n",
      "12.50825023651123 -3.312112808227539 -4.556854724884033 -4.011167049407959 -3.3090996742248535 -3.3090991973876953\n",
      "12.460250854492188 -3.261526584625244 -4.5392231941223145 -2.864183187484741 -5.315570831298828 -5.315570831298828\n",
      "12.506000518798828 -3.2240469455718994 -4.520540714263916 -3.1676297187805176 -3.683124303817749 -3.68312406539917\n",
      "12.428250312805176 -3.2533791065216064 -4.525929927825928 -3.6399433612823486 -4.750020503997803 -4.750020980834961\n",
      "12.447500228881836 -3.3066792488098145 -4.566189289093018 -3.3904483318328857 -4.934191703796387 -4.9341912269592285\n",
      "12.48550033569336 -3.347916603088379 -4.576754570007324 -2.674649715423584 -3.6664698123931885 -3.6664695739746094\n",
      "12.489250183105469 -3.344693422317505 -4.5719523429870605 -3.803333044052124 -4.794320106506348 -4.794320106506348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.50200080871582 -3.2939207553863525 -4.548238277435303 -3.309027910232544 -3.700039863586426 -3.700040102005005\n",
      "12.571001052856445 -3.2562382221221924 -4.521960258483887 -2.693903684616089 -4.79849910736084 -4.79849910736084\n",
      "12.551750183105469 -3.2495272159576416 -4.526599407196045 -2.737175703048706 -4.6145172119140625 -4.6145172119140625\n",
      "12.496000289916992 -3.2704784870147705 -4.541479587554932 -3.388160228729248 -4.101633071899414 -4.101633071899414\n",
      "12.505500793457031 -3.3246524333953857 -4.572462558746338 -3.3691208362579346 -4.578279495239258 -4.5782790184021\n",
      "12.452750205993652 -3.313932180404663 -4.574703216552734 -2.943967580795288 -4.310301780700684 -4.310301780700684\n",
      "12.466750144958496 -3.297163724899292 -4.561773300170898 -3.238243341445923 -4.622844696044922 -4.622844696044922\n",
      "12.471250534057617 -3.2729268074035645 -4.536248683929443 -3.293766498565674 -2.897134780883789 -2.897134780883789\n",
      "12.449250221252441 -3.280770778656006 -4.536455154418945 -3.559849739074707 -5.114657402038574 -5.114657402038574\n",
      "12.526500701904297 -3.266874074935913 -4.534195899963379 -2.3456809520721436 -4.777569770812988 -4.777569770812988\n",
      "12.485250473022461 -3.289031982421875 -4.552145481109619 -3.2003986835479736 -3.5485119819641113 -3.5485119819641113\n",
      "12.429750442504883 -3.284777879714966 -4.54926872253418 -3.0409789085388184 -3.743417978286743 -3.743417978286743\n",
      "12.425750732421875 -3.3092401027679443 -4.562251091003418 -3.892518997192383 -3.758368730545044 -3.758368492126465\n",
      "12.434250831604004 -3.307904005050659 -4.564810276031494 -3.212041139602661 -4.7624735832214355 -4.7624735832214355\n",
      "12.46150016784668 -3.286877155303955 -4.5462260246276855 -3.565664529800415 -4.049305438995361 -4.049305438995361\n",
      "12.462750434875488 -3.3069875240325928 -4.550720691680908 -2.989107370376587 -4.3368635177612305 -4.3368635177612305\n",
      "12.465250968933105 -3.329378843307495 -4.554349422454834 -3.233275890350342 -4.195248603820801 -4.195248603820801\n",
      "12.495750427246094 -3.3288683891296387 -4.554569244384766 -3.4593663215637207 -5.731865406036377 -5.731865882873535\n",
      "12.478750228881836 -3.3118386268615723 -4.553079605102539 -3.441469669342041 -5.500364780426025 -5.500364780426025\n",
      "12.494250297546387 -3.2787911891937256 -4.545731544494629 -2.7979469299316406 -3.1413025856018066 -3.1413025856018066\n",
      "12.43125057220459 -3.268829584121704 -4.538937091827393 -2.9211924076080322 -3.9554924964904785 -3.9554924964904785\n",
      "12.46725082397461 -3.296046018600464 -4.5545220375061035 -3.429863452911377 -5.5856475830078125 -5.585648059844971\n",
      "12.45525074005127 -3.307023525238037 -4.558767318725586 -3.5396018028259277 -4.41484260559082 -4.414842128753662\n",
      "12.508000373840332 -3.316533327102661 -4.5559210777282715 -3.823230743408203 -4.943519592285156 -4.943519592285156\n",
      "12.477750778198242 -3.296677827835083 -4.541570663452148 -3.248465061187744 -4.703292369842529 -4.703292369842529\n",
      "12.500500679016113 -3.28700852394104 -4.534934043884277 -2.7415008544921875 -4.812528610229492 -4.81252908706665\n",
      "12.491000175476074 -3.3303401470184326 -4.541891574859619 -3.7513489723205566 -5.353801727294922 -5.353801727294922\n",
      "12.450750350952148 -3.350351333618164 -4.562817096710205 -3.2590770721435547 -5.121114253997803 -5.121114253997803\n",
      "12.420750617980957 -3.3111915588378906 -4.55431604385376 -3.373137950897217 -4.747466087341309 -4.74746561050415\n",
      "12.440500259399414 -3.2576282024383545 -4.54056453704834 -3.096442937850952 -4.70175838470459 -4.70175838470459\n",
      "12.459500312805176 -3.25447154045105 -4.53601598739624 -2.817444086074829 -3.7791168689727783 -3.779116630554199\n",
      "12.464250564575195 -3.2755777835845947 -4.553049087524414 -3.1880404949188232 -5.721824645996094 -5.721824645996094\n",
      "12.49275016784668 -3.3285536766052246 -4.5657243728637695 -3.443040370941162 -4.778439521789551 -4.778439998626709\n",
      "12.436500549316406 -3.312509059906006 -4.558042526245117 -2.8853955268859863 -4.268387317657471 -4.268387317657471\n",
      "12.470250129699707 -3.287151336669922 -4.545921325683594 -2.887763261795044 -3.741525173187256 -3.741525173187256\n",
      "12.444000244140625 -3.2720823287963867 -4.530407428741455 -3.287261962890625 -3.6729636192321777 -3.6729633808135986\n",
      "12.47075080871582 -3.2970621585845947 -4.543689727783203 -3.37646746635437 -4.897880554199219 -4.897880554199219\n",
      "12.483250617980957 -3.316380500793457 -4.545531272888184 -3.511176347732544 -3.1387100219726562 -3.1387102603912354\n",
      "12.420001029968262 -3.3264682292938232 -4.556845188140869 -3.38381290435791 -4.290557861328125 -4.290557861328125\n",
      "12.416000366210938 -3.3068580627441406 -4.554976463317871 -3.398165464401245 -5.45380163192749 -5.453802108764648\n",
      "12.485750198364258 -3.305741786956787 -4.55007266998291 -2.7033462524414062 -5.026928901672363 -5.0269293785095215\n",
      "12.405250549316406 -3.2626006603240967 -4.542576313018799 -3.526071786880493 -4.514399528503418 -4.514399528503418\n",
      "12.417000770568848 -3.2232816219329834 -4.53080415725708 -2.9595749378204346 -4.8397111892700195 -4.839710712432861\n",
      "12.479750633239746 -3.2779059410095215 -4.540115833282471 -3.526838541030884 -3.801608085632324 -3.801607847213745\n",
      "12.426000595092773 -3.318843126296997 -4.560555458068848 -2.9368395805358887 -5.171335220336914 -5.171335697174072\n",
      "12.415750503540039 -3.3273024559020996 -4.552807807922363 -3.008967161178589 -4.889348983764648 -4.889348983764648\n",
      "12.472250938415527 -3.30580472946167 -4.552065372467041 -4.228657245635986 -4.699934005737305 -4.699934482574463\n",
      "12.433500289916992 -3.2509024143218994 -4.5288496017456055 -3.9462685585021973 -4.7715301513671875 -4.7715301513671875\n",
      "12.457250595092773 -3.2540359497070312 -4.52570104598999 -3.2862048149108887 -4.395230293273926 -4.395230293273926\n",
      "12.430000305175781 -3.2897517681121826 -4.540462017059326 -3.452014923095703 -5.250123023986816 -5.250123977661133\n",
      "12.465750694274902 -3.3286774158477783 -4.560173034667969 -3.1626312732696533 -5.580502510070801 -5.580502033233643\n",
      "12.433000564575195 -3.341439723968506 -4.5720319747924805 -2.929381847381592 -4.479611396789551 -4.479611873626709\n",
      "12.355000495910645 -3.2932612895965576 -4.549732208251953 -3.5523524284362793 -5.358485221862793 -5.358485221862793\n",
      "12.383000373840332 -3.260617733001709 -4.530592918395996 -3.1599576473236084 -5.058315277099609 -5.058315753936768\n",
      "12.465750694274902 -3.2683584690093994 -4.533565044403076 -2.9969120025634766 -5.3751115798950195 -5.3751115798950195\n",
      "12.382000923156738 -3.284364700317383 -4.5397629737854 -2.977670907974243 -5.046093940734863 -5.046093940734863\n",
      "12.459250450134277 -3.309999704360962 -4.558459758758545 -3.4379429817199707 -5.415197849273682 -5.41519832611084\n",
      "12.430500984191895 -3.306488513946533 -4.550256729125977 -2.8333845138549805 -4.848737716674805 -4.848737716674805\n",
      "12.42300033569336 -3.295764446258545 -4.555933952331543 -3.0550220012664795 -5.036561489105225 -5.036561965942383\n",
      "12.447250366210938 -3.278393268585205 -4.55111837387085 -3.2144486904144287 -4.592776298522949 -4.592776298522949\n",
      "12.431750297546387 -3.2408547401428223 -4.528203010559082 -3.4452290534973145 -5.284792900085449 -5.284793376922607\n",
      "12.39525032043457 -3.2435083389282227 -4.524515628814697 -3.528336524963379 -5.440535545349121 -5.440535545349121\n",
      "12.425000190734863 -3.2821171283721924 -4.547516345977783 -3.6431360244750977 -4.564230442047119 -4.564230442047119\n",
      "12.456250190734863 -3.327573776245117 -4.56519079208374 -3.3473358154296875 -4.15969705581665 -4.15969705581665\n",
      "12.409500122070312 -3.3112151622772217 -4.559632778167725 -3.2187018394470215 -3.9449968338012695 -3.9449968338012695\n",
      "12.420500755310059 -3.2830636501312256 -4.552371025085449 -3.1929774284362793 -4.56286096572876 -4.56286096572876\n",
      "12.407500267028809 -3.245384454727173 -4.524953842163086 -3.2071118354797363 -4.005225658416748 -4.005225658416748\n",
      "12.449501037597656 -3.2654292583465576 -4.537067890167236 -3.160503387451172 -4.474303245544434 -4.474303722381592\n",
      "12.50100040435791 -3.297989845275879 -4.545767307281494 -3.482900619506836 -5.332352638244629 -5.332353115081787\n",
      "12.411250114440918 -3.295898675918579 -4.5475754737854 -2.6275997161865234 -5.112883567810059 -5.112883567810059\n",
      "12.397750854492188 -3.288390874862671 -4.550826549530029 -3.729165554046631 -6.013270854949951 -6.013270854949951\n",
      "12.44425106048584 -3.2566843032836914 -4.539783954620361 -3.4149322509765625 -4.516695022583008 -4.51669454574585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.407750129699707 -3.2416598796844482 -4.535449028015137 -3.171513557434082 -5.755425453186035 -5.755424976348877\n",
      "12.378750801086426 -3.2579991817474365 -4.542059898376465 -3.394674301147461 -5.49908447265625 -5.499084949493408\n",
      "12.42300033569336 -3.30070424079895 -4.56512451171875 -3.594093084335327 -4.415641784667969 -4.415641784667969\n",
      "12.398000717163086 -3.309678554534912 -4.5556488037109375 -3.1741795539855957 -4.677671432495117 -4.677670955657959\n",
      "12.438750267028809 -3.2896528244018555 -4.545741558074951 -2.957296133041382 -4.41743278503418 -4.41743278503418\n",
      "12.397000312805176 -3.300262451171875 -4.549722194671631 -3.1181161403656006 -4.854728698730469 -4.854729175567627\n",
      "12.395000457763672 -3.3147292137145996 -4.5586466789245605 -3.1354904174804688 -4.606433868408203 -4.606433868408203\n",
      "12.432000160217285 -3.319631814956665 -4.555824279785156 -2.7378385066986084 -5.396183967590332 -5.39618444442749\n",
      "12.409250259399414 -3.3084664344787598 -4.541747093200684 -2.9102463722229004 -4.856021881103516 -4.856021404266357\n",
      "12.40825080871582 -3.29101824760437 -4.546898365020752 -2.86659574508667 -3.6374874114990234 -3.6374874114990234\n",
      "12.43850040435791 -3.281961679458618 -4.544969081878662 -3.989882469177246 -4.961915016174316 -4.961915016174316\n",
      "12.419750213623047 -3.2506086826324463 -4.531466960906982 -2.9679129123687744 -2.5908288955688477 -2.5908291339874268\n",
      "12.42300033569336 -3.2446303367614746 -4.534561634063721 -2.8544797897338867 -5.799555778503418 -5.799556255340576\n",
      "12.422750473022461 -3.2761104106903076 -4.543955326080322 -2.829616069793701 -4.018472194671631 -4.018472194671631\n",
      "12.391500473022461 -3.3172733783721924 -4.5642571449279785 -3.5253102779388428 -3.9006614685058594 -3.9006614685058594\n",
      "12.41450023651123 -3.312802791595459 -4.563588619232178 -3.686244487762451 -5.052394866943359 -5.052394866943359\n",
      "12.426000595092773 -3.2997162342071533 -4.55321741104126 -3.3055920600891113 -3.1128292083740234 -3.1128292083740234\n",
      "12.358000755310059 -3.267362594604492 -4.543283462524414 -3.246525287628174 -4.935296535491943 -4.935296535491943\n",
      "12.411500930786133 -3.2723183631896973 -4.53317928314209 -3.175334930419922 -4.970888137817383 -4.970887660980225\n",
      "12.391000747680664 -3.296900749206543 -4.550171852111816 -3.2450714111328125 -4.746735572814941 -4.746735572814941\n",
      "12.386750221252441 -3.3080503940582275 -4.554332733154297 -3.892770767211914 -4.921164512634277 -4.9211649894714355\n",
      "12.400250434875488 -3.3262670040130615 -4.568066120147705 -3.944732189178467 -5.482643127441406 -5.482643127441406\n",
      "12.409000396728516 -3.2871344089508057 -4.556024551391602 -3.4875845909118652 -2.870971202850342 -2.8709709644317627\n",
      "12.409250259399414 -3.2622251510620117 -4.5294647216796875 -3.4071052074432373 -3.7941930294036865 -3.7941930294036865\n",
      "12.387750625610352 -3.2680416107177734 -4.532218933105469 -2.9123053550720215 -3.489253520965576 -3.489253282546997\n",
      "12.408000946044922 -3.329136371612549 -4.5542755126953125 -2.8637893199920654 -5.439828395843506 -5.439828395843506\n",
      "12.425251007080078 -3.351027011871338 -4.565438747406006 -3.0968823432922363 -5.313163757324219 -5.313163757324219\n",
      "12.409750938415527 -3.2854249477386475 -4.547626972198486 -3.0413289070129395 -5.900060653686523 -5.900060176849365\n",
      "12.414250373840332 -3.2117857933044434 -4.51821231842041 -3.2338240146636963 -5.1654534339904785 -5.1654534339904785\n",
      "12.36775016784668 -3.199514389038086 -4.521543979644775 -3.230968952178955 -4.725401878356934 -4.725401878356934\n",
      "12.3697509765625 -3.2570366859436035 -4.543318748474121 -3.1090312004089355 -3.716744899749756 -3.7167444229125977\n",
      "12.405500411987305 -3.319810390472412 -4.558051586151123 -3.466212272644043 -3.318234443664551 -3.318234443664551\n",
      "12.374000549316406 -3.352919816970825 -4.57455587387085 -2.8579201698303223 -3.7927768230438232 -3.7927770614624023\n",
      "12.310250282287598 -3.300611734390259 -4.553925037384033 -3.5486555099487305 -4.7164130210876465 -4.7164130210876465\n",
      "12.388500213623047 -3.2712788581848145 -4.538943767547607 -3.4018912315368652 -5.721142768859863 -5.721142768859863\n",
      "12.396750450134277 -3.2459845542907715 -4.5140461921691895 -3.525485038757324 -5.443416595458984 -5.443416118621826\n",
      "12.399750709533691 -3.279796838760376 -4.531875133514404 -3.1710727214813232 -3.8177154064178467 -3.8177154064178467\n",
      "12.3722505569458 -3.338348627090454 -4.573383808135986 -3.7476449012756348 -3.6645936965942383 -3.664593458175659\n",
      "12.40250015258789 -3.3399147987365723 -4.573701858520508 -3.965155601501465 -5.23603630065918 -5.23603630065918\n",
      "12.391250610351562 -3.2733237743377686 -4.547967433929443 -3.3484911918640137 -3.5554981231689453 -3.5554981231689453\n",
      "12.40475082397461 -3.2270309925079346 -4.5138325691223145 -3.663459300994873 -5.334355354309082 -5.334355354309082\n",
      "12.363750457763672 -3.226083278656006 -4.515782833099365 -3.3843135833740234 -5.032317638397217 -5.032317638397217\n",
      "12.35425090789795 -3.2891042232513428 -4.5393452644348145 -3.513488292694092 -5.087968349456787 -5.087968826293945\n",
      "12.409250259399414 -3.350142002105713 -4.576568603515625 -3.100205421447754 -5.430765151977539 -5.430764675140381\n",
      "12.380000114440918 -3.3374295234680176 -4.567790985107422 -3.2053823471069336 -4.518209457397461 -4.518209457397461\n",
      "12.413750648498535 -3.2602527141571045 -4.541693687438965 -3.3325939178466797 -4.454706192016602 -4.454706192016602\n",
      "12.403250694274902 -3.2034378051757812 -4.510080814361572 -3.2201755046844482 -3.6015498638153076 -3.6015498638153076\n",
      "12.393750190734863 -3.2113468647003174 -4.51629114151001 -3.035686492919922 -3.1525540351867676 -3.1525542736053467\n",
      "12.368250846862793 -3.2683253288269043 -4.546695232391357 -3.374551296234131 -4.460153579711914 -4.460153579711914\n",
      "12.351500511169434 -3.3503551483154297 -4.575854301452637 -3.770474672317505 -3.4081549644470215 -3.4081547260284424\n",
      "12.354750633239746 -3.3536736965179443 -4.571340084075928 -2.9853053092956543 -4.512068271636963 -4.512068271636963\n",
      "12.37125015258789 -3.263836622238159 -4.533807754516602 -2.6638119220733643 -5.634809494018555 -5.634809494018555\n",
      "12.373000144958496 -3.2476766109466553 -4.529389381408691 -2.808239698410034 -3.7566308975219727 -3.7566308975219727\n",
      "12.370750427246094 -3.2565860748291016 -4.533808708190918 -3.6814920902252197 -3.4388504028320312 -3.4388504028320312\n",
      "12.371000289916992 -3.278240203857422 -4.5349321365356445 -3.6032967567443848 -4.872412204742432 -4.872412204742432\n",
      "12.410500526428223 -3.3223180770874023 -4.556544303894043 -2.4834628105163574 -3.9747304916381836 -3.9747302532196045\n",
      "12.42300033569336 -3.3488352298736572 -4.569935321807861 -3.4863052368164062 -6.212944984436035 -6.212944030761719\n",
      "12.398500442504883 -3.3100461959838867 -4.569174766540527 -2.993168830871582 -2.9598567485809326 -2.9598569869995117\n",
      "12.40250015258789 -3.2514917850494385 -4.535426616668701 -3.5093698501586914 -5.127286911010742 -5.127286911010742\n",
      "12.33025074005127 -3.2308855056762695 -4.530970096588135 -3.273062229156494 -4.667758941650391 -4.667759418487549\n",
      "12.362250328063965 -3.252837657928467 -4.5401129722595215 -3.126870632171631 -5.522312164306641 -5.522312164306641\n",
      "12.326000213623047 -3.292816638946533 -4.558589935302734 -3.0370755195617676 -3.2424538135528564 -3.2424535751342773\n",
      "12.342750549316406 -3.3292434215545654 -4.576528072357178 -2.544009208679199 -3.442204236984253 -3.442204236984253\n",
      "12.36400032043457 -3.3363966941833496 -4.57450008392334 -3.150682210922241 -4.573359966278076 -4.573359966278076\n",
      "12.371500968933105 -3.3110053539276123 -4.5661540031433105 -3.206995964050293 -4.38491153717041 -4.384911060333252\n"
     ]
    }
   ],
   "source": [
    "train(npoints = 9, batchsize = 4000, nsamples = 8) #greedyprob - 2, without +ve reinforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-variation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
