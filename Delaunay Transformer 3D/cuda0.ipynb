{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "device = \"cpu\"\n",
    "floattype = torch.float\n",
    "\n",
    "batchsize = 512\n",
    "nsamples = 8\n",
    "npoints = 5\n",
    "emsize = 512\n",
    "\n",
    "\n",
    "class Graph_Transformer(nn.Module):\n",
    "    def __init__(self, emsize = 32, nhead = 8, nhid = 1024, nlayers = 3, ndecoderlayers = 0, dropout = 0):\n",
    "        super().__init__()\n",
    "        self.emsize = emsize\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "        encoder_layers = TransformerEncoderLayer(emsize, nhead, nhid, dropout = dropout)\n",
    "        decoder_layers = TransformerDecoderLayer(emsize, nhead, nhid, dropout = dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layers, ndecoderlayers)\n",
    "        self.encoder = nn.Linear(3, emsize)\n",
    "        self.outputattention_query = nn.Linear(emsize, emsize, bias = False)\n",
    "        self.outputattention_key = nn.Linear(emsize, emsize, bias = False)\n",
    "        self.start_token = nn.Parameter(torch.randn([emsize], device = device))\n",
    "    \n",
    "    def generate_subsequent_mask(self, sz): #last dimension will be softmaxed over when adding to attention logits, if boolean the ones turn into -inf\n",
    "        #mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        #mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        \n",
    "        mask = torch.triu(torch.ones([sz, sz], dtype = torch.bool, device = device), diagonal = 1)\n",
    "        return mask\n",
    "    \n",
    "    def encode(self, src): #src must be [batchsize * nsamples, npoints, 3]\n",
    "        src = self.encoder(src).transpose(0, 1)\n",
    "        output = self.transformer_encoder(src)\n",
    "        return output #[npoints, batchsize * nsamples, emsize]\n",
    "    \n",
    "    def decode_next(self, memory, tgt, route_mask): #route mask is [batchsize * nsamples, npoints], both memory and tgt must have batchsize and nsamples in same dimension (the 1th one)\n",
    "        npoints = memory.size(0)\n",
    "        batchsize = tgt.size(1)\n",
    "        \"\"\"if I really wanted this to be efficient I'd only recompute the decoder for the last tgt, and just remebering what the others looked like from before (won't change due to mask)\"\"\"\n",
    "        \"\"\"have the option to freeze the autograd on all but the last part of tgt, although at the moment this is a very natural way to say: initial choices matter more\"\"\"\n",
    "        tgt_mask = self.generate_subsequent_mask(tgt.size(0))\n",
    "        output = self.transformer_decoder(tgt, memory, tgt_mask) #[tgt, batchsize * nsamples, emsize]\n",
    "        output_query = self.outputattention_query(memory).transpose(0, 1) #[batchsize * nsamples, npoints, emsize]\n",
    "        output_key = self.outputattention_key(output[-1]) #[batchsize * nsamples, emsize]\n",
    "        output_attention = torch.matmul(output_query * self.emsize ** -0.5, output_key.unsqueeze(-1)).squeeze(-1) #[batchsize * nsamples, npoints], technically don't need to scale attention as we divide by variance next anyway\n",
    "        output_attention_tanh = output_attention.tanh() #[batchsize * nsamples, npoints]\n",
    "        \n",
    "        #we clone the route_mask incase we want to backprop using it (else it was modified by inplace opporations)\n",
    "        output_attention = output_attention.masked_fill(route_mask.clone(), float('-inf')) #[batchsize * nsamples, npoints]\n",
    "        output_attention_tanh = output_attention_tanh.masked_fill(route_mask.clone(), float('-inf')) #[batchsize * nsamples, npoints]\n",
    "        \n",
    "        return output_attention_tanh, output_attention #[batchsize * nsamples, npoints]\n",
    "    \n",
    "    def calculate_logprob(self, memory, routes): #memory is [npoints, batchsize * nsamples, emsize], routes is [batchsize * nsamples, npoints - 4], rather than backproping the entire loop, this saves vram (and computation)\n",
    "        npoints = memory.size(0)\n",
    "        ninternalpoints = routes.size(1)\n",
    "        bigbatchsize = memory.size(1)\n",
    "        memory_ = memory.gather(0, routes.transpose(0, 1).unsqueeze(2).expand(-1, -1, self.emsize)) #[npoints - 4, batchsize * nsamples, emsize] reorder memory into order of routes\n",
    "        tgt = torch.cat([self.start_token.unsqueeze(0).unsqueeze(1).expand(1, bigbatchsize, -1), memory_[:-1]]) #[npoints - 4, batchsize * nroutes, emsize], want to go from memory to tgt\n",
    "        tgt_mask = self.generate_subsequent_mask(ninternalpoints)\n",
    "        output = self.transformer_decoder(tgt, memory, tgt_mask) #[npoints - 4, batchsize * nsamples, emsize]\n",
    "        \"\"\"want probability of going from key to query, but first need to normalise (softmax with mask)\"\"\"\n",
    "        output_query = self.outputattention_query(memory_).transpose(0, 1) #[batchsize * nsamples, npoints - 4, emsize]\n",
    "        output_key = self.outputattention_key(output).transpose(0, 1) #[batchsize * nsamples, npoints - 4, emsize]\n",
    "        attention_mask = torch.full([ninternalpoints, ninternalpoints], True, device = device).triu(1) #[npoints - 4, npoints - 4], True for i < j\n",
    "        output_attention = torch.matmul(output_query * self.emsize ** -0.5, output_key.transpose(-1, -2))\n",
    "        \"\"\"quick fix to stop divergence\"\"\"\n",
    "        output_attention_tanh = output_attention.tanh()\n",
    "        \n",
    "        output_attention_tanh = output_attention_tanh.masked_fill(attention_mask, float('-inf'))\n",
    "        output_attention_tanh = output_attention_tanh - output_attention_tanh.logsumexp(-2, keepdim = True) #[batchsize * nsamples, npoints - 4, npoints - 4]\n",
    "        \n",
    "        output_attention = output_attention.masked_fill(attention_mask, float('-inf'))\n",
    "        output_attention = output_attention - output_attention.logsumexp(-2, keepdim = True) #[batchsize * nsamples, npoints - 4, npoints - 4]\n",
    "        \n",
    "        \"\"\"infact I'm almost tempted to not mask choosing a previous point, so it's forced to learn it and somehow incorporate it into its computation, but without much impact on reinforcing good examples\"\"\"\n",
    "        logprob_tanh = output_attention_tanh.diagonal(dim1 = -1, dim2 = -2).sum(-1) #[batchsize * nsamples]\n",
    "        logprob = output_attention.diagonal(dim1 = -1, dim2 = -2).sum(-1) #[batchsize * nsamples]\n",
    "        return logprob_tanh, logprob #[batchsize * nsamples]\n",
    "\n",
    "NN = Graph_Transformer().to(device)\n",
    "optimizer = optim.Adam(NN.parameters())\n",
    "\n",
    "\n",
    "class environment:    \n",
    "    def reset(self, npoints, batchsize, nsamples=1, corner_points = None, initial_triangulation = None):\n",
    "        \"\"\"\n",
    "        corner_points, etc., shoudn't include a batch dimension\n",
    "        \"\"\"\n",
    "        if corner_points == None:\n",
    "            ncornerpoints = 4\n",
    "        else:\n",
    "            ncornerpoints = corner_points.size(0)\n",
    "        if npoints <= ncornerpoints:\n",
    "            print(\"Error: not enough points for valid problem instance\")\n",
    "            return\n",
    "        self.batchsize = (\n",
    "            batchsize * nsamples\n",
    "        )  # so that I don't have to rewrite all this code, we store these two dimensions together\n",
    "        self.nsamples = nsamples\n",
    "        self.npoints = npoints\n",
    "        self.points = (\n",
    "            torch.rand([batchsize, npoints - ncornerpoints, 3], dtype = floattype, device=device)\n",
    "            .unsqueeze(1)\n",
    "            .expand(-1, nsamples, -1, -1)\n",
    "            .reshape(self.batchsize, npoints - ncornerpoints, 3)\n",
    "        )\n",
    "        if corner_points == None:\n",
    "            self.corner_points = torch.tensor(\n",
    "                [[0, 0, 0], [3, 0, 0], [0, 3, 0], [0, 0, 3]], dtype = floattype, device=device\n",
    "            )\n",
    "        else:\n",
    "            self.corner_points = corner_points\n",
    "        self.points = torch.cat(\n",
    "            [\n",
    "                self.corner_points.unsqueeze(0).expand(self.batchsize, -1, -1),\n",
    "                self.points,\n",
    "            ],\n",
    "            dim=-2,\n",
    "        )  # [batchsize * nsamples, npoints, 3]\n",
    "        self.points_mask = torch.cat(\n",
    "            [\n",
    "                torch.ones([self.batchsize, ncornerpoints], dtype=torch.bool, device=device),\n",
    "                torch.zeros(\n",
    "                    [self.batchsize, npoints - ncornerpoints], dtype=torch.bool, device=device\n",
    "                ),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        self.points_sequence = torch.empty(\n",
    "            [self.batchsize, 0], dtype=torch.long, device=device\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        points are now triples\n",
    "        triangles are now quadruples\n",
    "        edges are now still just indices, but there are four of them per 'triangle', and they correspond to triples of points, not pairs\n",
    "        we use  0,2,1  0,3,2  0,1,3  1,2,3  as the order of the four 'edges'/faces\n",
    "        opposite face is always ordered such that the last two indices are swapped\n",
    "        faces are always read ANTICLOCKWISE\n",
    "        \n",
    "        first three points of tetrahedron MUST be read clockwise (from the outside) to get correct sign on incircle test\n",
    "        \n",
    "        new point will be inserted in zeroth position, so if corresponding face of REMOVED tetrahedron is [x,y,z] (being read anticlockwise from outside in) new tetrahedron is [p, x, y, z]\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        number of tetrahedra is not the same for each batch (in 3D), so store as a big list, and remember batch index that it comes from\n",
    "        \"\"\"\n",
    "        if corner_points == None:\n",
    "            initial_triangulation = torch.tensor([[0, 1, 2, 3]], dtype=torch.long, device=device)\n",
    "        \n",
    "        self.partial_delaunay_triangles = initial_triangulation.unsqueeze(0).expand(self.batchsize, -1, -1).reshape(-1, 4)\n",
    "        self.batch_index = torch.arange(self.batchsize, dtype = torch.long, device = device).unsqueeze(1).expand(-1, initial_triangulation.size(0)).reshape(-1)\n",
    "        \n",
    "        self.batch_triangles = self.partial_delaunay_triangles.size(0) #[0]\n",
    "        self.ntriangles = torch.full([self.batchsize], initial_triangulation.size(0), dtype = torch.long, device = device) #[self.batchsize]\n",
    "        \n",
    "        self.cost = torch.zeros([self.batchsize], dtype = floattype, device=device)\n",
    "\n",
    "        self.logprob = torch.zeros([self.batchsize], dtype = floattype, device=device, requires_grad=True)\n",
    "\n",
    "    def update(self, point_index):  # point_index is [batchsize]\n",
    "        \n",
    "        assert point_index.size(0) == self.batchsize\n",
    "        assert str(point_index.device) == device\n",
    "        assert self.points_mask.gather(1, point_index.unsqueeze(1)).sum() == 0\n",
    "        \n",
    "        triangles_coordinates = self.points[self.batch_index.unsqueeze(1), self.partial_delaunay_triangles] # [batch_triangles, 4, 3]\n",
    "        \n",
    "        newpoint = self.points[self.batch_index, point_index[self.batch_index]] # [batch_triangles, 3]\n",
    "\n",
    "        incircle_matrix = torch.cat(\n",
    "            [\n",
    "                newpoint.unsqueeze(1),\n",
    "                triangles_coordinates,\n",
    "            ],\n",
    "            dim=-2,\n",
    "        )  # [batch_triangles, 5, 3]\n",
    "        incircle_matrix = torch.cat(\n",
    "            [\n",
    "                (incircle_matrix * incircle_matrix).sum(-1, keepdim=True),\n",
    "                incircle_matrix,\n",
    "                torch.ones([self.batch_triangles, 5, 1], dtype = floattype, device=device),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )  # [batch_triangles, 5, 5]\n",
    "        assert incircle_matrix.dtype == floattype\n",
    "        assert str(incircle_matrix.device) == device\n",
    "        \n",
    "        incircle_test = (\n",
    "            incircle_matrix.det() > 0\n",
    "        )  # [batch_triangles], is True if inside incircle\n",
    "        \n",
    "        conflicts = incircle_test.sum()\n",
    "        \n",
    "        conflicting_triangles = self.partial_delaunay_triangles[incircle_test] # [conflicts, 4]\n",
    "        \n",
    "        conflicting_edges_index0 = torch.empty_like(conflicting_triangles)\n",
    "        indices = torch.LongTensor([0, 0, 0, 1])\n",
    "        conflicting_edges_index0 = conflicting_triangles[:, indices] # [conflicts, 4]\n",
    "        \n",
    "        conflicting_edges_index1 = torch.empty_like(conflicting_triangles)\n",
    "        indices = torch.LongTensor([2, 3, 1, 2])\n",
    "        conflicting_edges_index1 = conflicting_triangles[:, indices] # [conflicts, 4]\n",
    "        \n",
    "        conflicting_edges_index2 = torch.empty_like(conflicting_triangles)\n",
    "        indices = torch.LongTensor([1, 2, 3, 3])\n",
    "        conflicting_edges_index2 = conflicting_triangles[:, indices] # [conflicts, 4]\n",
    "        \n",
    "        conflicting_edges = torch.cat([conflicting_edges_index0.view(-1).unsqueeze(-1), conflicting_edges_index1.view(-1).unsqueeze(-1), conflicting_edges_index2.view(-1).unsqueeze(-1)], dim = -1).reshape(-1, 3) # [conflicts * 4, 3]\n",
    "        \n",
    "        edge_batch_index = self.batch_index[incircle_test].unsqueeze(1).expand(-1, 4).reshape(-1) # [conflicts * 4]\n",
    "        \n",
    "        indices = torch.LongTensor([0, 2, 1])\n",
    "        comparison_edges = conflicting_edges[:, indices] # [conflicts * 4, 3]        \n",
    "        \n",
    "        unravel_nomatch_mask = torch.ones([conflicts * 4], dtype = torch.bool, device = device) # [conflicts * 4]\n",
    "        i = 1\n",
    "        while True:\n",
    "            \n",
    "            todo_mask = unravel_nomatch_mask[:-i].logical_and(edge_batch_index[:-i] == edge_batch_index[i:])\n",
    "            if i % 4 == 0:\n",
    "                if todo_mask.sum() == 0:\n",
    "                    break\n",
    "            \n",
    "            match_mask = todo_mask.clone()\n",
    "            match_mask[todo_mask] = (conflicting_edges[:-i][todo_mask] != comparison_edges[i:][todo_mask]).sum(-1).logical_not()\n",
    "            \n",
    "            unravel_nomatch_mask[:-i][match_mask] = False\n",
    "            unravel_nomatch_mask[i:][match_mask] = False\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        batch_newtriangles = unravel_nomatch_mask.sum()\n",
    "        \n",
    "        nomatch_edges = conflicting_edges[unravel_nomatch_mask] # [batch_newtriangles, 3], already in correct order to insert into 1,2,3 (since already anticlockwise from outside in)\n",
    "        assert list(nomatch_edges.size()) == [batch_newtriangles, 3]\n",
    "        nomatch_batch_index = edge_batch_index[unravel_nomatch_mask] # [batch_newtriangles]\n",
    "        \n",
    "        nomatch_newpoint = point_index[nomatch_batch_index] # [batch_newtriangles]\n",
    "        \n",
    "        newtriangles = torch.cat([nomatch_newpoint.unsqueeze(1), nomatch_edges], dim = -1) # [batch_newtriangles, 4]\n",
    "        \n",
    "        \n",
    "        nremoved_triangles = torch.zeros([self.batchsize], dtype = torch.long, device = device)\n",
    "        nnew_triangles = torch.zeros([self.batchsize], dtype = torch.long, device = device)\n",
    "        \n",
    "        indices = self.batch_index[incircle_test]\n",
    "        nremoved_triangles.put_(indices, torch.ones_like(indices, dtype = torch.long), accumulate = True) # [batchsize]\n",
    "        \n",
    "        indices = edge_batch_index[unravel_nomatch_mask]\n",
    "        nnew_triangles.put_(indices, torch.ones_like(indices, dtype = torch.long), accumulate = True) # [batchsize]\n",
    "        \n",
    "        assert (nnew_triangles <= 2 * nremoved_triangles + 2).logical_not().sum().logical_not()\n",
    "        \n",
    "        \"\"\"\n",
    "        NOTE:\n",
    "        I THINK it's possible for nnew_triangles to be less than nremoved_triangles (or my code is just buggy...)\n",
    "        \"\"\"\n",
    "        \n",
    "        assert nnew_triangles.sum() == batch_newtriangles\n",
    "        assert nremoved_triangles.sum() == incircle_test.sum()\n",
    "        \n",
    "        nadditional_triangles = nnew_triangles - nremoved_triangles # [batchsize]\n",
    "        ntriangles = self.ntriangles + nadditional_triangles # [batchsize]\n",
    "        \n",
    "        partial_delaunay_triangles = torch.empty([ntriangles.sum(), 4], dtype = torch.long, device = device)\n",
    "        batch_index = torch.empty([ntriangles.sum()], dtype = torch.long, device = device)\n",
    "        \n",
    "        cumulative_triangles = torch.cat([torch.zeros([1], dtype = torch.long, device = device), nnew_triangles.cumsum(0)[:-1]]) # [batchsize], cumulative sum starts at zero\n",
    "        \n",
    "        \"\"\"\n",
    "        since may actually have LESS triangles than previous round, we insert all that survive into the first slots (in that batch)\n",
    "        \"\"\"\n",
    "        good_triangle_indices = torch.arange(incircle_test.logical_not().sum(), dtype = torch.long, device = device)\n",
    "        good_triangle_indices += cumulative_triangles[self.batch_index[incircle_test.logical_not()]]\n",
    "        bad_triangle_indices_mask = torch.ones([ntriangles.sum(0)], dtype = torch.bool, device = device)\n",
    "        bad_triangle_indices_mask.scatter_(0, good_triangle_indices, False)\n",
    "        \n",
    "        assert good_triangle_indices.size(0) == incircle_test.logical_not().sum()\n",
    "        assert bad_triangle_indices_mask.sum() == batch_newtriangles\n",
    "        \n",
    "        partial_delaunay_triangles[good_triangle_indices] = self.partial_delaunay_triangles[~incircle_test]\n",
    "        batch_index[good_triangle_indices] = self.batch_index[~incircle_test]\n",
    "        \n",
    "        partial_delaunay_triangles[bad_triangle_indices_mask] = newtriangles\n",
    "        batch_index[bad_triangle_indices_mask] = nomatch_batch_index\n",
    "        \n",
    "        self.partial_delaunay_triangles = partial_delaunay_triangles\n",
    "        self.batch_index = batch_index\n",
    "        \n",
    "        self.ntriangles = ntriangles\n",
    "        self.batch_triangles = self.partial_delaunay_triangles.size(0)\n",
    "        \n",
    "        self.points_mask.scatter_(\n",
    "            1, point_index.unsqueeze(1).expand(-1, self.npoints), True\n",
    "        )\n",
    "        self.points_sequence = torch.cat(\n",
    "            [self.points_sequence, point_index.unsqueeze(1)], dim=1\n",
    "        )\n",
    "        \n",
    "        self.cost += nremoved_triangles\n",
    "        return\n",
    "    \n",
    "    def sample_point(self, logits): #logits must be [batchsize * nsamples, npoints]\n",
    "        probs = torch.distributions.categorical.Categorical(logits = logits)\n",
    "        next_point = probs.sample() #size is [batchsize * nsamples]\n",
    "        self.update(next_point)\n",
    "        self.logprob = self.logprob + probs.log_prob(next_point)\n",
    "        return next_point #[batchsize * nsamples]\n",
    "    \n",
    "    def sampleandgreedy_point(self, logits): #logits must be [batchsize * nsamples, npoints], last sample will be the greedy choice (but we still need to keep track of its logits)\n",
    "        logits_sample = logits.view(-1, self.nsamples, self.npoints)[:, :-1, :]\n",
    "        probs = torch.distributions.categorical.Categorical(logits = logits_sample)\n",
    "        \n",
    "        sample_point = probs.sample() #[batchsize, (nsamples - 1)]\n",
    "        greedy_point = logits.view(-1, self.nsamples, self.npoints)[:, -1, :].max(-1, keepdim = True)[1] #[batchsize, 1]\n",
    "        next_point = torch.cat([sample_point, greedy_point], dim = 1).view(-1)\n",
    "        self.update(next_point)\n",
    "        self.logprob = self.logprob + torch.cat([probs.log_prob(sample_point), torch.zeros([sample_point.size(0), 1], device = device)], dim = 1).view(-1)\n",
    "        return next_point\n",
    "    \n",
    "\n",
    "env = environment()\n",
    "\n",
    "\n",
    "def train(epochs = 30000, npoints = 14, batchsize = 100, nsamples = 8):\n",
    "    NN.train()\n",
    "    for i in range(epochs):\n",
    "        env.reset(npoints, batchsize, nsamples)\n",
    "        \"\"\"include the boundary points, kinda makes sense that they should contribute (atm only in the encoder, difficult to see how in the decoder)\"\"\"\n",
    "        memory = NN.encode(env.points) #[npoints, batchsize * nsamples, emsize]\n",
    "        #### #### #### remember to include tgt.detach() when reinstate with torch.no_grad()\n",
    "        tgt = NN.start_token.unsqueeze(0).unsqueeze(1).expand(1, batchsize * nsamples, -1).detach() #[1, batchsize * nsamples, emsize]\n",
    "        #with torch.no_grad(): #to speed up computation, selecting routes is done without gradient\n",
    "        with torch.no_grad():\n",
    "            for j in range(4, npoints):\n",
    "                #### #### #### remember to include memory.detach() when reinstate with torch.no_grad()\n",
    "                _, logits = NN.decode_next(memory.detach(), tgt, env.points_mask)\n",
    "                next_point = env.sampleandgreedy_point(logits)\n",
    "                \"\"\"\n",
    "                for inputing the previous embedding into decoder\n",
    "                \"\"\"\n",
    "                tgt = torch.cat([tgt, memory.gather(0, next_point.unsqueeze(0).unsqueeze(2).expand(1, -1, memory.size(2)))]) #[nsofar, batchsize * nsamples, emsize]\n",
    "                \"\"\"\n",
    "                for inputing the previous decoder output into the decoder (allows for an evolving strategy, but doesn't allow for fast training\n",
    "                \"\"\"\n",
    "                ############\n",
    "\n",
    "        \n",
    "        NN.eval()\n",
    "        _, logprob = NN.calculate_logprob(memory, env.points_sequence) #[batchsize * nsamples]\n",
    "        NN.train()\n",
    "        \"\"\"\n",
    "        clip logprob so doesn't reinforce things it already knows\n",
    "        TBH WANT SOMETHING DIFFERENT ... want to massively increase training if find something unexpected and otherwise not\n",
    "        \"\"\"\n",
    "        greedy_prob = logprob.view(batchsize, nsamples)[:, -1].detach() #[batchsize]\n",
    "        greedy_baseline = env.cost.view(batchsize, nsamples)[:, -1] #[batchsize], greedy sample\n",
    "        fixed_baseline = 0.5 * torch.ones([1], device = device)\n",
    "        min_baseline = env.cost.view(batchsize, nsamples)[:, :-1].min(-1)[0] #[batchsize], minimum cost\n",
    "        baseline = greedy_baseline\n",
    "        positive_reinforcement = - F.relu( - (env.cost.view(batchsize, nsamples)[:, :-1] - baseline.unsqueeze(1))) #don't scale positive reinforcement\n",
    "        negative_reinforcement = F.relu(env.cost.view(batchsize, nsamples)[:, :-1] - baseline.unsqueeze(1))\n",
    "        positive_reinforcement_binary = env.cost.view(batchsize, nsamples)[:, :-1] - baseline.unsqueeze(1) <= -0.05\n",
    "        negative_reinforcement_binary = env.cost.view(batchsize, nsamples)[:, :-1] - baseline.unsqueeze(1) > 1\n",
    "        \"\"\"\n",
    "        binary positive reinforcement\n",
    "        \"\"\"\n",
    "        #loss = - ((logprob.view(batchsize, nsamples)[:, :-1] < -0.2) * logprob.view(batchsize, nsamples)[:, :-1] * positive_reinforcement_binary).mean() #+ (logprob.view(batchsize, nsamples)[:, :-1] > -1) * logprob.view(batchsize, nsamples)[:, :-1] * negative_reinforcement_binary\n",
    "        \"\"\"\n",
    "        clipped binary reinforcement\n",
    "        \"\"\"\n",
    "        #loss = ( \n",
    "        #        - logprob.view(batchsize, nsamples)[:, :-1] \n",
    "        #        #* (logprob.view(batchsize, nsamples)[:, :-1] < 0) \n",
    "        #        * positive_reinforcement_binary \n",
    "        #        + logprob.view(batchsize, nsamples)[:, :-1] \n",
    "        #        #* (logprob.view(batchsize, nsamples)[:, :-1] > greedy_prob.unsqueeze(1) - 13) \n",
    "        #        * negative_reinforcement_binary \n",
    "        #).mean()\n",
    "        \"\"\"\n",
    "        clipped binary postive, clipped weighted negative\n",
    "        \"\"\"\n",
    "        #loss = ( - logprob.view(batchsize, nsamples)[:, :-1] * (logprob.view(batchsize, nsamples)[:, :-1] < -0.2) * positive_reinforcement_binary + logprob.view(batchsize, nsamples)[:, :-1] * (logprob.view(batchsize, nsamples)[:, :-1] > -2) * negative_reinforcement ).mean()\n",
    "        \"\"\"\n",
    "        clipped reinforcement without rescaling\n",
    "        \"\"\"\n",
    "        #loss = ((logprob.view(batchsize, nsamples)[:, :-1] < -0.7) * logprob.view(batchsize, nsamples)[:, :-1] * positive_reinforcement + (logprob.view(batchsize, nsamples)[:, :-1] > -5) * logprob.view(batchsize, nsamples)[:, :-1] * negative_reinforcement).mean()\n",
    "        \"\"\"\n",
    "        clipped reinforcement\n",
    "        \"\"\"\n",
    "        #loss = (logprob.view(batchsize, nsamples)[:, :-1] * positive_reinforcement / (positive_reinforcement.var() + 0.001).sqrt() + (logprob.view(batchsize, nsamples)[:, :-1] > -3) * logprob.view(batchsize, nsamples)[:, :-1] * negative_reinforcement / (negative_reinforcement.var() + 0.001).sqrt()).mean()\n",
    "        \"\"\"\n",
    "        balanced reinforcement\n",
    "        \"\"\"\n",
    "        #loss = (logprob.view(batchsize, nsamples)[:, :-1] * (positive_reinforcement / (positive_reinforcement.var() + 0.001).sqrt() + negative_reinforcement / (negative_reinforcement.var() + 0.001).sqrt())).mean()\n",
    "        \"\"\"\n",
    "        regular loss\n",
    "        \"\"\"\n",
    "        loss = (logprob.view(batchsize, nsamples)[:, :-1] * (positive_reinforcement + negative_reinforcement)).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #print(NN.encoder.weight.grad)\n",
    "        optimizer.step()\n",
    "        #print(greedy_baseline.mean().item())\n",
    "        print(greedy_baseline.mean().item(), logprob.view(batchsize, nsamples)[:, -1].mean().item(), logprob.view(batchsize, nsamples)[:, :-1].mean().item(), logprob[batchsize - 1].item(), logprob[0].item(), env.logprob[0].item())\n",
    "        \n",
    "def evaluate(epochs = 30000, npoints = 14, batchsize = 100, nsamples = 8):\n",
    "    NN.eval()\n",
    "    for i in range(epochs):\n",
    "        env.reset(npoints, batchsize, nsamples)\n",
    "        \"\"\"include the boundary points, kinda makes sense that they should contribute (atm only in the encoder, difficult to see how in the decoder)\"\"\"\n",
    "        memory = NN.encode(env.points) #[npoints, batchsize * nsamples, emsize]\n",
    "        #### #### #### remember to include tgt.detach() when reinstate with torch.no_grad()\n",
    "        tgt = NN.start_token.unsqueeze(0).unsqueeze(1).expand(1, batchsize * nsamples, -1).detach() #[1, batchsize * nsamples, emsize]\n",
    "        #with torch.no_grad(): #to speed up computation, selecting routes is done without gradient\n",
    "        with torch.no_grad():\n",
    "            for j in range(4, npoints):\n",
    "                #### #### #### remember to include memory.detach() when reinstate with torch.no_grad()\n",
    "                _, logits = NN.decode_next(memory.detach(), tgt, env.points_mask)\n",
    "                next_point = env.sampleandgreedy_point(logits)\n",
    "                \"\"\"\n",
    "                for inputing the previous embedding into decoder\n",
    "                \"\"\"\n",
    "                tgt = torch.cat([tgt, memory.gather(0, next_point.unsqueeze(0).unsqueeze(2).expand(1, -1, memory.size(2)))]) #[nsofar, batchsize * nsamples, emsize]\n",
    "                \"\"\"\n",
    "                for inputing the previous decoder output into the decoder (allows for an evolving strategy, but doesn't allow for fast training\n",
    "                \"\"\"\n",
    "                ############\n",
    "        \n",
    "        \n",
    "        \n",
    "        _, logprob = NN.calculate_logprob(memory, env.points_sequence) #[batchsize * nsamples]\n",
    "        NN.train()\n",
    "        \"\"\"\n",
    "        clip logprob so doesn't reinforce things it already knows\n",
    "        TBH WANT SOMETHING DIFFERENT ... want to massively increase training if find something unexpected and otherwise not\n",
    "        \"\"\"\n",
    "        \n",
    "        greedy_baseline = env.cost.view(batchsize, nsamples)[:, -1] #[batchsize], greedy sample\n",
    "        \n",
    "        print(greedy_baseline.mean().item(), logprob.view(batchsize, nsamples)[:, -1].mean().item(), logprob.view(batchsize, nsamples)[:, :-1].mean().item(), logprob[batchsize - 1].item(), logprob[0].item(), env.logprob[0].item())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610.5599975585938 -141.84889221191406 -144.45701599121094 -144.08462524414062 -144.19662475585938 -144.19662475585938\n"
     ]
    }
   ],
   "source": [
    "evaluate(1, 53, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.load_state_dict(torch.load('3D_100points_small_1230'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(epochs = 30000, npoints = 14, batchsize = 100, nsamples = 8):\n",
    "    NN.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(epochs):\n",
    "            env.reset(npoints, batchsize, nsamples)\n",
    "            \"\"\"include the boundary points, kinda makes sense that they should contribute (atm only in the encoder, difficult to see how in the decoder)\"\"\"\n",
    "            memory = NN.encode(env.points) #[npoints, batchsize * nsamples, emsize]\n",
    "            #### #### #### remember to include tgt.detach() when reinstate with torch.no_grad()\n",
    "            tgt = NN.start_token.unsqueeze(0).unsqueeze(1).expand(1, batchsize * nsamples, -1).detach() #[1, batchsize * nsamples, emsize]\n",
    "            #with torch.no_grad(): #to speed up computation, selecting routes is done without gradient\n",
    "            for j in range(4, npoints):\n",
    "                #### #### #### remember to include memory.detach() when reinstate with torch.no_grad()\n",
    "                _, logits = NN.decode_next(memory.detach(), tgt, env.points_mask)\n",
    "                next_point = env.sampleandgreedy_point(logits)\n",
    "                \"\"\"\n",
    "                for inputing the previous embedding into decoder\n",
    "                \"\"\"\n",
    "                tgt = torch.cat([tgt, memory.gather(0, next_point.unsqueeze(0).unsqueeze(2).expand(1, -1, memory.size(2)))]) #[nsofar, batchsize * nsamples, emsize]\n",
    "                \"\"\"\n",
    "                for inputing the previous decoder output into the decoder (allows for an evolving strategy, but doesn't allow for fast training\n",
    "                \"\"\"\n",
    "                ############\n",
    "\n",
    "\n",
    "            NN.eval()\n",
    "            _, logprob = NN.calculate_logprob(memory, env.points_sequence) #[batchsize * nsamples]\n",
    "            NN.train()\n",
    "            \"\"\"\n",
    "            clip logprob so doesn't reinforce things it already knows\n",
    "            TBH WANT SOMETHING DIFFERENT ... want to massively increase training if find something unexpected and otherwise not\n",
    "            \"\"\"\n",
    "            \n",
    "            greedy_baseline = env.cost.view(batchsize, nsamples)[:, -1] #[batchsize], greedy sample\n",
    "            \n",
    "            print(greedy_baseline.mean().item(), logprob.view(batchsize, nsamples)[:, -1].mean().item(), logprob.view(batchsize, nsamples)[:, :-1].mean().item(), logprob[batchsize - 1].item(), logprob[0].item(), env.logprob[0].item())\n",
    "        \n",
    "#evaluate(epochs = 10, npoints = 24, batchsize = 100, nsamples = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26674.0 -5835.2109375 nan -5835.2109375 -5835.2109375 0.0\n",
      "26598.0 -5832.29833984375 nan -5832.29833984375 -5832.29833984375 0.0\n",
      "26653.0 -5835.2451171875 nan -5835.2451171875 -5835.2451171875 0.0\n",
      "26557.0 -5838.02685546875 nan -5838.02685546875 -5838.02685546875 0.0\n",
      "26723.0 -5836.95703125 nan -5836.95703125 -5836.95703125 0.0\n",
      "26678.0 -5835.369140625 nan -5835.369140625 -5835.369140625 0.0\n",
      "26694.0 -5833.19921875 nan -5833.19921875 -5833.19921875 0.0\n",
      "26587.0 -5833.3115234375 nan -5833.3115234375 -5833.3115234375 0.0\n",
      "26701.0 -5832.5869140625 nan -5832.5869140625 -5832.5869140625 0.0\n",
      "26702.0 -5837.625 nan -5837.625 -5837.625 0.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(10, 1004, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "REDO THIS WITHOUT KEEPING TRACK OF EDGES\n",
    "\n",
    "Idea: among removed triangles, pair up faces that both apear, left with faces that don't - the boundary, from which we construct new triangles\n",
    "\n",
    "have two lists, faces left to check, and faces to check against (these will be all 3 anticlockwise versions of each face)\n",
    "keep track of the batch you came from, and the index against which you are currently checking\n",
    "increase index by one each time until either: find a match, or: no longer checking against same batch\n",
    "at which point we remove FROM THE FIRST LIST\n",
    "repeat until all removed\n",
    "when find a match, mark it in second list\n",
    "removed all marked faces\n",
    "somehow find number remaining in each batch, and make sure to copy that many 'new points' into a long list\n",
    "construct new triangles from the above information\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "device = \"cuda:0\"\n",
    "floattype = torch.float\n",
    "\n",
    "\n",
    "class environment:    \n",
    "    def reset(self, npoints, batchsize, nsamples=1, corner_points = None, initial_triangulation = None):\n",
    "        \"\"\"\n",
    "        corner_points, etc., shoudn't include a batch dimension\n",
    "        \"\"\"\n",
    "        if corner_points == None:\n",
    "            ncornerpoints = 4\n",
    "        else:\n",
    "            ncornerpoints = corner_points.size(0)\n",
    "        if npoints <= ncornerpoints:\n",
    "            print(\"Error: not enough points for valid problem instance\")\n",
    "            return\n",
    "        self.batchsize = (\n",
    "            batchsize * nsamples\n",
    "        )  # so that I don't have to rewrite all this code, we store these two dimensions together\n",
    "        self.nsamples = nsamples\n",
    "        self.npoints = npoints\n",
    "        self.points = (\n",
    "            torch.rand([batchsize, npoints - ncornerpoints, 3], dtype = floattype, device=device)\n",
    "            .unsqueeze(1)\n",
    "            .expand(-1, nsamples, -1, -1)\n",
    "            .reshape(self.batchsize, npoints - ncornerpoints, 3)\n",
    "        )\n",
    "        if corner_points == None:\n",
    "            self.corner_points = torch.tensor(\n",
    "                [[0, 0, 0], [3, 0, 0], [0, 3, 0], [0, 0, 3]], dtype = floattype, device=device\n",
    "            )\n",
    "        else:\n",
    "            self.corner_points = corner_points\n",
    "        self.points = torch.cat(\n",
    "            [\n",
    "                self.corner_points.unsqueeze(0).expand(self.batchsize, -1, -1),\n",
    "                self.points,\n",
    "            ],\n",
    "            dim=-2,\n",
    "        )  # [batchsize * nsamples, npoints, 3]\n",
    "        self.points_mask = torch.cat(\n",
    "            [\n",
    "                torch.ones([self.batchsize, ncornerpoints], dtype=torch.bool, device=device),\n",
    "                torch.zeros(\n",
    "                    [self.batchsize, npoints - ncornerpoints], dtype=torch.bool, device=device\n",
    "                ),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        self.points_sequence = torch.empty(\n",
    "            [self.batchsize, 0], dtype=torch.long, device=device\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        points are now triples\n",
    "        triangles are now quadruples\n",
    "        edges are now still just indices, but there are four of them per 'triangle', and they correspond to triples of points, not pairs\n",
    "        we use  0,2,1  0,3,2  0,1,3  1,2,3  as the order of the four 'edges'/faces\n",
    "        opposite face is always ordered such that the last two indices are swapped\n",
    "        faces are always read ANTICLOCKWISE\n",
    "        \n",
    "        first three points of tetrahedron MUST be read clockwise (from the outside) to get correct sign on incircle test\n",
    "        \n",
    "        new point will be inserted in zeroth position, so if corresponding face of REMOVED tetrahedron is [x,y,z] (being read anticlockwise from outside in) new tetrahedron is [p, x, y, z]\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        number of tetrahedra is not the same for each batch (in 3D), so store as a big list, and remember batch index that it comes from\n",
    "        \"\"\"\n",
    "        if corner_points == None:\n",
    "            initial_triangulation = torch.tensor([[0, 1, 2, 3]], dtype=torch.long, device=device)\n",
    "        \n",
    "        self.partial_delaunay_triangles = initial_triangulation.unsqueeze(0).expand(self.batchsize, -1, -1).reshape(-1, 4)\n",
    "        self.batch_index = torch.arange(self.batchsize, dtype = torch.long, device = device).unsqueeze(1).expand(-1, initial_triangulation.size(0)).reshape(-1)\n",
    "        \n",
    "        self.batch_triangles = self.partial_delaunay_triangles.size(0) #[0]\n",
    "        self.ntriangles = torch.full([self.batchsize], initial_triangulation.size(0), dtype = torch.long, device = device) #[self.batchsize]\n",
    "        \n",
    "        self.cost = torch.zeros([self.batchsize], dtype = floattype, device=device)\n",
    "\n",
    "        self.logprob = torch.zeros([self.batchsize], dtype = floattype, device=device, requires_grad=True)\n",
    "\n",
    "    def update(self, point_index):  # point_index is [batchsize]\n",
    "        \n",
    "        assert point_index.size(0) == self.batchsize\n",
    "        assert str(point_index.device) == device\n",
    "        assert self.points_mask.gather(1, point_index.unsqueeze(1)).sum() == 0\n",
    "        \n",
    "        triangles_coordinates = self.points[self.batch_index.unsqueeze(1), self.partial_delaunay_triangles] # [batch_triangles, 4, 3]\n",
    "        \n",
    "        newpoint = self.points[self.batch_index, point_index[self.batch_index]] # [batch_triangles, 3]\n",
    "\n",
    "        incircle_matrix = torch.cat(\n",
    "            [\n",
    "                newpoint.unsqueeze(1),\n",
    "                triangles_coordinates,\n",
    "            ],\n",
    "            dim=-2,\n",
    "        )  # [batch_triangles, 5, 3]\n",
    "        incircle_matrix = torch.cat(\n",
    "            [\n",
    "                (incircle_matrix * incircle_matrix).sum(-1, keepdim=True),\n",
    "                incircle_matrix,\n",
    "                torch.ones([self.batch_triangles, 5, 1], dtype = floattype, device=device),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )  # [batch_triangles, 5, 5]\n",
    "        assert incircle_matrix.dtype == floattype\n",
    "        assert str(incircle_matrix.device) == device\n",
    "        \n",
    "        incircle_test = (\n",
    "            incircle_matrix.det() > 0\n",
    "        )  # [batch_triangles], is True if inside incircle\n",
    "        \n",
    "        conflicts = incircle_test.sum()\n",
    "        \n",
    "        conflicting_triangles = self.partial_delaunay_triangles[incircle_test] # [conflicts, 4]\n",
    "        \n",
    "        conflicting_edges_index0 = torch.empty_like(conflicting_triangles)\n",
    "        indices = torch.LongTensor([0, 0, 0, 1])\n",
    "        conflicting_edges_index0 = conflicting_triangles[:, indices] # [conflicts, 4]\n",
    "        \n",
    "        conflicting_edges_index1 = torch.empty_like(conflicting_triangles)\n",
    "        indices = torch.LongTensor([2, 3, 1, 2])\n",
    "        conflicting_edges_index1 = conflicting_triangles[:, indices] # [conflicts, 4]\n",
    "        \n",
    "        conflicting_edges_index2 = torch.empty_like(conflicting_triangles)\n",
    "        indices = torch.LongTensor([1, 2, 3, 3])\n",
    "        conflicting_edges_index2 = conflicting_triangles[:, indices] # [conflicts, 4]\n",
    "        \n",
    "        conflicting_edges = torch.cat([conflicting_edges_index0.view(-1).unsqueeze(-1), conflicting_edges_index1.view(-1).unsqueeze(-1), conflicting_edges_index2.view(-1).unsqueeze(-1)], dim = -1).reshape(-1, 3) # [conflicts * 4, 3]\n",
    "        \n",
    "        edge_batch_index = self.batch_index[incircle_test].unsqueeze(1).expand(-1, 4).reshape(-1) # [conflicts * 4]\n",
    "        \n",
    "        indices = torch.LongTensor([0, 2, 1])\n",
    "        comparison_edges = conflicting_edges[:, indices] # [conflicts * 4, 3]        \n",
    "        \n",
    "        unravel_nomatch_mask = torch.ones([conflicts * 4], dtype = torch.bool, device = device) # [conflicts * 4]\n",
    "        i = 1\n",
    "        while True:\n",
    "            \n",
    "            todo_mask = unravel_nomatch_mask[:-i].logical_and(edge_batch_index[:-i] == edge_batch_index[i:])\n",
    "            if i % 4 == 0:\n",
    "                if todo_mask.sum() == 0:\n",
    "                    break\n",
    "            \n",
    "            match_mask = todo_mask.clone()\n",
    "            match_mask[todo_mask] = (conflicting_edges[:-i][todo_mask] != comparison_edges[i:][todo_mask]).sum(-1).logical_not()\n",
    "            \n",
    "            unravel_nomatch_mask[:-i][match_mask] = False\n",
    "            unravel_nomatch_mask[i:][match_mask] = False\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        batch_newtriangles = unravel_nomatch_mask.sum()\n",
    "        \n",
    "        nomatch_edges = conflicting_edges[unravel_nomatch_mask] # [batch_newtriangles, 3], already in correct order to insert into 1,2,3 (since already anticlockwise from outside in)\n",
    "        assert list(nomatch_edges.size()) == [batch_newtriangles, 3]\n",
    "        nomatch_batch_index = edge_batch_index[unravel_nomatch_mask] # [batch_newtriangles]\n",
    "        \n",
    "        nomatch_newpoint = point_index[nomatch_batch_index] # [batch_newtriangles]\n",
    "        \n",
    "        newtriangles = torch.cat([nomatch_newpoint.unsqueeze(1), nomatch_edges], dim = -1) # [batch_newtriangles, 4]\n",
    "        \n",
    "        \n",
    "        nremoved_triangles = torch.zeros([self.batchsize], dtype = torch.long, device = device)\n",
    "        nnew_triangles = torch.zeros([self.batchsize], dtype = torch.long, device = device)\n",
    "        \n",
    "        indices = self.batch_index[incircle_test]\n",
    "        nremoved_triangles.put_(indices, torch.ones_like(indices, dtype = torch.long), accumulate = True) # [batchsize]\n",
    "        \n",
    "        indices = edge_batch_index[unravel_nomatch_mask]\n",
    "        nnew_triangles.put_(indices, torch.ones_like(indices, dtype = torch.long), accumulate = True) # [batchsize]\n",
    "        \n",
    "        assert (nnew_triangles <= 2 * nremoved_triangles + 2).logical_not().sum().logical_not()\n",
    "        \n",
    "        \"\"\"\n",
    "        NOTE:\n",
    "        I THINK it's possible for nnew_triangles to be less than nremoved_triangles (or my code is just buggy...)\n",
    "        \"\"\"\n",
    "        \n",
    "        assert nnew_triangles.sum() == batch_newtriangles\n",
    "        assert nremoved_triangles.sum() == incircle_test.sum()\n",
    "        \n",
    "        nadditional_triangles = nnew_triangles - nremoved_triangles # [batchsize]\n",
    "        ntriangles = self.ntriangles + nadditional_triangles # [batchsize]\n",
    "        \n",
    "        partial_delaunay_triangles = torch.empty([ntriangles.sum(), 4], dtype = torch.long, device = device)\n",
    "        batch_index = torch.empty([ntriangles.sum()], dtype = torch.long, device = device)\n",
    "        \n",
    "        cumulative_triangles = torch.cat([torch.zeros([1], dtype = torch.long, device = device), nnew_triangles.cumsum(0)[:-1]]) # [batchsize], cumulative sum starts at zero\n",
    "        \n",
    "        \"\"\"\n",
    "        since may actually have LESS triangles than previous round, we insert all that survive into the first slots (in that batch)\n",
    "        \"\"\"\n",
    "        good_triangle_indices = torch.arange(incircle_test.logical_not().sum(), dtype = torch.long, device = device)\n",
    "        good_triangle_indices += cumulative_triangles[self.batch_index[incircle_test.logical_not()]]\n",
    "        bad_triangle_indices_mask = torch.ones([ntriangles.sum(0)], dtype = torch.bool, device = device)\n",
    "        bad_triangle_indices_mask.scatter_(0, good_triangle_indices, False)\n",
    "        \n",
    "        assert good_triangle_indices.size(0) == incircle_test.logical_not().sum()\n",
    "        assert bad_triangle_indices_mask.sum() == batch_newtriangles\n",
    "        \n",
    "        partial_delaunay_triangles[good_triangle_indices] = self.partial_delaunay_triangles[~incircle_test]\n",
    "        batch_index[good_triangle_indices] = self.batch_index[~incircle_test]\n",
    "        \n",
    "        partial_delaunay_triangles[bad_triangle_indices_mask] = newtriangles\n",
    "        batch_index[bad_triangle_indices_mask] = nomatch_batch_index\n",
    "        \n",
    "        self.partial_delaunay_triangles = partial_delaunay_triangles\n",
    "        self.batch_index = batch_index\n",
    "        \n",
    "        self.ntriangles = ntriangles\n",
    "        self.batch_triangles = self.partial_delaunay_triangles.size(0)\n",
    "        \n",
    "        self.points_mask.scatter_(\n",
    "            1, point_index.unsqueeze(1).expand(-1, self.npoints), True\n",
    "        )\n",
    "        self.points_sequence = torch.cat(\n",
    "            [self.points_sequence, point_index.unsqueeze(1)], dim=1\n",
    "        )\n",
    "        \n",
    "        self.cost += nremoved_triangles\n",
    "        return\n",
    "    \n",
    "    def allindices(self): #generate all orders of point insertion\n",
    "        npoints = self.npoints - 4\n",
    "        allroutes = torch.empty([1, 0], dtype = torch.long, device = device)\n",
    "        for i in range(npoints):\n",
    "            nroutes = allroutes.size(0)\n",
    "            remaining_mask = torch.ones([nroutes], dtype = torch.bool, device = device).unsqueeze(1).expand(-1, npoints).clone().scatter_(-1, allroutes, False)\n",
    "            remaining_indices = remaining_mask.nonzero(as_tuple = True)[1]\n",
    "            allroutes = allroutes.unsqueeze(1).expand(-1, remaining_mask[0, :].sum(), -1)\n",
    "            allroutes = torch.cat([allroutes, remaining_indices.view(nroutes, -1).unsqueeze(2)], dim = -1).view(-1, allroutes.size(-1) + 1)\n",
    "        return allroutes #[npoints!, npoints]\n",
    "\n",
    "\n",
    "# turn integer x,y coords (in nxn grid) into position d (0 to n^2-1) along the Hilbert curve.\n",
    "def xy2d(n, x, y):\n",
    "    [x, y] = [math.floor(x), math.floor(y)]\n",
    "    [rx, ry, s, d] = [0, 0, 0, 0]\n",
    "    s = n / 2\n",
    "    s = math.floor(s)\n",
    "    while s > 0:\n",
    "        rx = (x & s) > 0  # bitwise and, and then boolean is it greater than 0?\n",
    "        ry = (y & s) > 0\n",
    "        d += s * s * ((3 * rx) ^ ry)\n",
    "        [x, y] = rot(n, x, y, rx, ry)\n",
    "        s = s / 2\n",
    "        s = math.floor(s)\n",
    "    return d\n",
    "\n",
    "\n",
    "def rot(n, x, y, rx, ry):\n",
    "    if ry == 0:\n",
    "        if rx == 1:\n",
    "            x = n - 1 - x\n",
    "            y = n - 1 - y\n",
    "        # Swap x and y\n",
    "        t = x\n",
    "        x = y\n",
    "        y = t\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def order(\n",
    "    n, points\n",
    "):  # turns tensor of points into integer distances along hilbert curve of itteration n\n",
    "    grid = n * points.to(\"cpu\")\n",
    "    x = torch.empty([grid.size(0)])\n",
    "    for i in range(points.size(0)):\n",
    "        x[i] = xy2d(n, grid[i, 0], grid[i, 1])\n",
    "    return x\n",
    "\n",
    "\"\"\"\n",
    "CURRENTLY ONLY 2D VERSION\n",
    "\"\"\"\n",
    "def hilbert_insertion(npoints=103, batchsize=200):\n",
    "    env.reset(npoints, batchsize)\n",
    "    points = env.points[:, 3:]  # [batchsize, npoints - 3]\n",
    "    insertion_order = torch.full([batchsize, npoints], float(\"inf\"), device=device)\n",
    "    for i in range(batchsize):\n",
    "        insertion_order[i, 3:] = order(\n",
    "            2 ** 6, points[i]\n",
    "        )  # number of possible positions is n ** 2\n",
    "    for i in range(npoints - 3):\n",
    "        next_index = insertion_order.min(-1)[1]\n",
    "        env.update(next_index)\n",
    "        insertion_order.scatter_(1, next_index.unsqueeze(1), float(\"inf\"))\n",
    "    print(env.cost.mean().item(), env.cost.var().sqrt().item())\n",
    "    return\n",
    "\n",
    "\n",
    "def random_insertion(npoints=104, batchsize=200):\n",
    "    env.reset(npoints, batchsize)\n",
    "    for i in range(npoints - 4):\n",
    "        env.update(torch.full([batchsize], i + 4, dtype=torch.long, device=device))\n",
    "    print(env.cost.mean().item(), env.cost.var().sqrt().item())\n",
    "    return\n",
    "\n",
    "    \"\"\"\n",
    "    UNDER CONSTRUCTION\n",
    "    \"\"\"\n",
    "def kdtree_insertion(npoints=103, batchsize=200):\n",
    "    env.reset(npoints, batchsize)\n",
    "    points = env.points[:, 3:]  # [batchsize, npoints - 3]\n",
    "    \n",
    "env = environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0 43.19682312011719\n",
      "27.0 42.42301559448242\n",
      "26.0 43.520633697509766\n",
      "26.0 41.43492126464844\n",
      "30.0 40.770633697509766\n",
      "32.0 42.38730239868164\n",
      "26.0 39.82460403442383\n",
      "31.0 41.69682312011719\n",
      "28.0 40.746826171875\n",
      "28.0 42.14603042602539\n"
     ]
    }
   ],
   "source": [
    "env = environment()\n",
    "npoints = 9\n",
    "batchsize = 1\n",
    "        \n",
    "env.reset(npoints + 4, batchsize, math.factorial(npoints))\n",
    "allroutes = env.allindices() + 4\n",
    "allroutes = allroutes.unsqueeze(0).expand(batchsize, -1, -1).reshape(-1, npoints)\n",
    "for j in range(10):\n",
    "    for i in range(npoints):\n",
    "        env.update(allroutes[:, i])\n",
    "    print(env.cost.view(batchsize, -1).min(-1)[0].mean().item(), env.cost.mean().item())\n",
    "    env.reset(npoints + 4, batchsize, math.factorial(npoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.5 34.148094177246094\n",
      "23.200000762939453 34.55595016479492\n",
      "23.0 34.28666687011719\n",
      "22.700000762939453 33.59000015258789\n",
      "21.80000114440918 34.04595184326172\n",
      "23.0 33.55976104736328\n",
      "23.600000381469727 34.18499755859375\n",
      "22.899999618530273 33.813331604003906\n",
      "22.200000762939453 34.28190231323242\n",
      "22.100000381469727 34.10976028442383\n"
     ]
    }
   ],
   "source": [
    "env = environment()\n",
    "npoints = 8\n",
    "batchsize = 10\n",
    "        \n",
    "env.reset(npoints + 4, batchsize, math.factorial(npoints))\n",
    "allroutes = env.allindices() + 4\n",
    "allroutes = allroutes.unsqueeze(0).expand(batchsize, -1, -1).reshape(-1, npoints)\n",
    "for j in range(10):\n",
    "    for i in range(npoints):\n",
    "        env.update(allroutes[:, i])\n",
    "    print(env.cost.view(batchsize, -1).min(-1)[0].mean().item(), env.cost.mean().item())\n",
    "    env.reset(npoints + 4, batchsize, math.factorial(npoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100000, 10])\n",
      "29.0 49.65717697143555\n",
      "36.0 48.87055969238281\n",
      "38.0 51.05009841918945\n",
      "36.0 51.02914047241211\n",
      "31.0 45.161277770996094\n",
      "41.0 52.800697326660156\n",
      "38.0 49.204898834228516\n",
      "39.0 50.78003692626953\n",
      "38.0 49.051597595214844\n",
      "33.0 47.717559814453125\n",
      "32.0 46.3649787902832\n",
      "35.0 47.345619201660156\n",
      "35.0 45.504600524902344\n",
      "35.0 49.83915710449219\n",
      "33.0 44.56079864501953\n",
      "37.0 51.07265853881836\n",
      "37.0 52.16703796386719\n",
      "38.0 49.48624038696289\n",
      "36.0 51.30870056152344\n",
      "35.0 46.69947814941406\n",
      "39.0 53.22419738769531\n",
      "33.0 48.71516036987305\n",
      "36.0 52.49327850341797\n",
      "38.0 51.98114013671875\n",
      "44.0 52.908878326416016\n",
      "34.0 49.60095977783203\n",
      "35.0 50.536598205566406\n",
      "35.0 51.26199722290039\n",
      "38.0 49.712860107421875\n",
      "36.0 53.11921691894531\n",
      "36.0 49.411720275878906\n",
      "33.0 49.54199981689453\n",
      "35.0 47.198917388916016\n",
      "39.0 49.790977478027344\n",
      "36.0 54.170318603515625\n",
      "37.0 49.390159606933594\n",
      "38.0 49.5429573059082\n",
      "37.0 48.32611846923828\n",
      "39.0 51.07919692993164\n",
      "35.0 49.789737701416016\n",
      "35.0 51.28583908081055\n",
      "35.0 47.72520065307617\n",
      "38.0 50.656158447265625\n",
      "34.0 51.30072021484375\n",
      "41.0 53.660980224609375\n",
      "35.0 53.712120056152344\n",
      "32.0 47.239356994628906\n",
      "36.0 50.370277404785156\n",
      "37.0 47.92791748046875\n",
      "39.0 51.462398529052734\n",
      "38.0 50.3607177734375\n",
      "37.0 49.50925827026367\n",
      "35.0 52.82551956176758\n",
      "30.0 49.39141845703125\n",
      "36.0 51.3376579284668\n",
      "40.0 55.07181930541992\n",
      "36.0 51.96335983276367\n",
      "37.0 51.517539978027344\n",
      "43.0 54.643978118896484\n",
      "26.0 46.2115592956543\n",
      "34.0 51.82817840576172\n",
      "39.0 49.960899353027344\n",
      "32.0 48.46746063232422\n",
      "33.0 41.949440002441406\n",
      "31.0 46.541358947753906\n",
      "39.0 49.138877868652344\n",
      "40.0 49.00469970703125\n",
      "35.0 51.744197845458984\n",
      "33.0 47.886199951171875\n",
      "36.0 50.72591781616211\n",
      "36.0 50.100337982177734\n",
      "36.0 46.658897399902344\n",
      "37.0 47.9061393737793\n",
      "36.0 51.31406021118164\n",
      "37.0 48.802337646484375\n",
      "40.0 51.77513885498047\n",
      "41.0 50.56869888305664\n",
      "37.0 47.37221908569336\n",
      "35.0 47.90971755981445\n",
      "35.0 46.825138092041016\n",
      "38.0 47.11967849731445\n",
      "36.0 47.221736907958984\n",
      "37.0 53.3896369934082\n",
      "35.0 47.72135925292969\n",
      "40.0 51.06879806518555\n",
      "37.0 52.39297866821289\n",
      "35.0 52.64259719848633\n",
      "39.0 51.39939880371094\n",
      "38.0 49.057220458984375\n",
      "34.0 47.85561752319336\n",
      "36.0 50.26471710205078\n",
      "37.0 51.41129684448242\n",
      "32.0 48.31901931762695\n",
      "35.0 51.13361740112305\n",
      "34.0 50.00729751586914\n",
      "41.0 55.756797790527344\n",
      "35.0 50.21487808227539\n",
      "32.0 45.2541389465332\n",
      "38.0 53.72211837768555\n",
      "36.0 47.18179702758789\n",
      "34.0 49.89809799194336\n",
      "36.0 47.23854064941406\n",
      "36.0 49.52433776855469\n",
      "38.0 52.13353729248047\n",
      "38.0 50.36909866333008\n",
      "37.0 50.51961898803711\n",
      "40.0 49.9570198059082\n",
      "31.0 42.73725891113281\n",
      "32.0 46.29657745361328\n",
      "34.0 50.613399505615234\n",
      "36.0 50.17918014526367\n",
      "38.0 50.183677673339844\n",
      "35.0 47.71017837524414\n",
      "38.0 50.91893768310547\n",
      "38.0 50.51457977294922\n",
      "31.0 46.555259704589844\n",
      "33.0 52.14127731323242\n",
      "36.0 50.320640563964844\n",
      "36.0 49.821800231933594\n",
      "36.0 51.831298828125\n",
      "36.0 48.59379959106445\n",
      "41.0 49.7045783996582\n",
      "42.0 50.815399169921875\n",
      "41.0 52.47834014892578\n",
      "37.0 48.8701171875\n",
      "35.0 49.64262008666992\n",
      "34.0 48.97373962402344\n",
      "34.0 54.15999984741211\n",
      "41.0 52.76564025878906\n",
      "33.0 46.31126022338867\n",
      "37.0 49.82815933227539\n",
      "38.0 52.48243713378906\n",
      "33.0 49.751399993896484\n",
      "33.0 44.8978385925293\n",
      "33.0 48.12828063964844\n",
      "34.0 52.17424011230469\n",
      "34.0 50.147640228271484\n",
      "36.0 52.4056396484375\n",
      "36.0 52.63583755493164\n",
      "39.0 51.87303924560547\n",
      "36.0 49.55253982543945\n",
      "35.0 48.0678596496582\n",
      "38.0 49.35715866088867\n",
      "38.0 48.47924041748047\n",
      "43.0 53.68143844604492\n",
      "36.0 48.01028060913086\n",
      "36.0 48.6767578125\n",
      "41.0 50.34977722167969\n",
      "37.0 50.66103744506836\n",
      "37.0 48.237037658691406\n",
      "30.0 49.63084030151367\n",
      "40.0 52.016357421875\n",
      "38.0 53.03379821777344\n",
      "37.0 49.03125762939453\n",
      "36.0 46.735939025878906\n",
      "32.0 45.820640563964844\n",
      "34.0 49.837459564208984\n",
      "34.0 46.71175765991211\n",
      "35.0 44.51667785644531\n",
      "34.0 48.78019714355469\n",
      "34.0 48.352718353271484\n",
      "37.0 50.11553955078125\n",
      "33.0 48.09611892700195\n"
     ]
    }
   ],
   "source": [
    "env = environment()\n",
    "npoints = 10\n",
    "batchsize = 1\n",
    "nsamples = 100000\n",
    "        \n",
    "env.reset(npoints + 4, nsamples)\n",
    "allroutes = env.allindices() + 4\n",
    "allroutes = allroutes.unsqueeze(0).expand(batchsize, -1, -1).reshape(-1, npoints)[:nsamples, :]\n",
    "print(allroutes.size())\n",
    "for j in range(1000):\n",
    "    for i in range(npoints):\n",
    "        env.update(allroutes[:, i])\n",
    "    print(env.cost.view(batchsize, -1).min(-1)[0].mean().item(), env.cost.mean().item())\n",
    "    env.reset(npoints + 4, batchsize, nsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17675.400390625 274.7860412597656\n",
      "17795.80078125 259.2129821777344\n",
      "17695.30078125 179.09890747070312\n",
      "17705.099609375 249.88575744628906\n",
      "17682.30078125 188.93975830078125\n",
      "17710.599609375 286.4358215332031\n",
      "17809.400390625 172.30747985839844\n",
      "17747.30078125 254.40084838867188\n",
      "17731.80078125 197.7528076171875\n",
      "17709.400390625 200.54603576660156\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    random_insertion(1004, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Install pytorch first, see https://pytorch.org/get-started/locally/ for instructions\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "If a GPU is available set device to \"cuda\" below, and this will speed up the code significantly (be careful not to overflow the GPU memory)\n",
    "To find out if a GPU is available you can use the command\n",
    "    torch.cuda.is_available()\n",
    "\"\"\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "To just compute the cost of the Hilbert Curve alogrithm use the function\n",
    "    hilbert_insertion(npoints, batchsize)\n",
    "npoints is the total number of points (including the 3 original corner points!!!!)\n",
    "batchsize is the number of problem instances we triangulate (say 200, then we would have 200 sets of npoints)\n",
    "\n",
    "This function will just print the average number of 'additional' triangles deleted, and the standard deviation of the number of 'additional' triangles deleted\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "To just compute the cost of the random insertion alogrithm use the function\n",
    "    random_insertion(npoints, batchsize)\n",
    "npoints is the total number of points (including the 3 original corner points!!!!)\n",
    "batchsize is the number of problem instances we triangulate (say 200, then we would have 200 sets of npoints)\n",
    "\n",
    "this function will just print the average number of 'additional' triangles deleted, and the standard deviation of the number of 'additional' triangles deleted\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This class (below) is the actual delaunay algorithm\n",
    "\n",
    "Initiate by\n",
    "    env = environment()\n",
    "To generate new random points use\n",
    "    env.reset(npoints, batchsize)\n",
    "(don't worry about what nsamples does in the actual code below, it's used for training NNs using reinforcement learning)\n",
    "\n",
    "npoints is the total number of points to be generated, the first 3 are the 3 corners of the big triangle, all the rest are randomly chosen in the unit square\n",
    "batchsize is how many problems you want to simultaneously create (lets say 200), then you would have 200 sets of npoints, currently all 200 are untriangulated\n",
    "\n",
    "The current point locations can be found by\n",
    "    env.points\n",
    "The current (partial) delaunay triangulations can be found by\n",
    "    env.triangles\n",
    "These are stored as triples of indices, so the triangle [0, 2426, 564] refers to the 0th point, 2426th point, and 564th point stored in env.points\n",
    "Note: the big outside triangle has two copies, a clockwise ([2, 1, 0]) and anticlockwise ([0, 1, 2]) one, the clockwise one will always stay and never get removed - it's used for some trickery for when the delaunay cavity includes one of the external edges, don't worry about it\n",
    "\n",
    "To add a point to the current (partial) delaunay triangulation use\n",
    "    env.update(points)\n",
    "where points is a torch.tensor (see the pytorch website for how to generate and use tensors) of size [batchsize], containing the index of the points you want to add FOR EACH PROBLEM INSTANCE (remember there might be batchsize = 200 problem instances, so you would have to specify 200 points to add, one for each problem instance)\n",
    "This will update the current (partial) delaunay triangulation, env.triangles\n",
    "\n",
    "To see the current cost (the 'additional' triangles that had to be removed) use\n",
    "    env.cost\n",
    "This will output a torch.tensor of size [batchsize], i.e. a 'list' of costs, corresponding to each of the (say 200) problem instances\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class environment:\n",
    "    def reset(self, npoints, batchsize, nsamples=1):\n",
    "        if npoints <= 3:\n",
    "            print(\"Error: not enough points for valid problem instance\")\n",
    "            return\n",
    "        self.batchsize = (\n",
    "            batchsize * nsamples\n",
    "        )  # so that I don't have to rewrite all this code, we store these two dimensions together\n",
    "        self.nsamples = nsamples\n",
    "        self.npoints = npoints\n",
    "        self.points = (\n",
    "            torch.rand([batchsize, npoints - 3, 2], device=device)\n",
    "            .unsqueeze(1)\n",
    "            .expand(-1, nsamples, -1, -1)\n",
    "            .reshape(self.batchsize, npoints - 3, 2)\n",
    "        )\n",
    "        self.corner_points = torch.tensor(\n",
    "            [[0, 0], [2, 0], [0, 2]], dtype=torch.float, device=device\n",
    "        )\n",
    "        self.points = torch.cat(\n",
    "            [\n",
    "                self.corner_points.unsqueeze(0).expand(self.batchsize, -1, -1),\n",
    "                self.points,\n",
    "            ],\n",
    "            dim=-2,\n",
    "        )  # [batchsize * nsamples, npoints, 2]\n",
    "        self.points_mask = torch.cat(\n",
    "            [\n",
    "                torch.ones([self.batchsize, 3], dtype=torch.bool, device=device),\n",
    "                torch.zeros(\n",
    "                    [self.batchsize, npoints - 3], dtype=torch.bool, device=device\n",
    "                ),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        self.points_sequence = torch.empty(\n",
    "            [self.batchsize, 0], dtype=torch.long, device=device\n",
    "        )\n",
    "\n",
    "        \"\"\"use a trick, for the purpose of an 'external' triangle that is always left untouched, which means we don't have to deal with boundary edges as being different. external triangle is [0, 1, 2] traversed clockwise...\"\"\"\n",
    "        self.partial_delaunay_triangles = (\n",
    "            torch.tensor([[0, 2, 1], [0, 1, 2]], dtype=torch.int64, device=device)\n",
    "            .unsqueeze(0)\n",
    "            .expand(self.batchsize, -1, -1)\n",
    "            .contiguous()\n",
    "        )  # [batchsize, ntriangles, 3] contains index of points, always anticlockwise\n",
    "        self.partial_delaunay_edges = (\n",
    "            torch.tensor([5, 4, 3, 2, 1, 0], dtype=torch.int64, device=device)\n",
    "            .unsqueeze(0)\n",
    "            .expand(self.batchsize, -1)\n",
    "            .contiguous()\n",
    "        )  # [batchsize, ntriangles * 3] contains location of corresponding edge (edges go in order 01, 12, 20). Edges will always flip since triangles are stored anticlockwise.\n",
    "\n",
    "        self.ntriangles = 2  # can store as scalar, since will always be the same\n",
    "        self.cost = torch.zeros([self.batchsize], device=device)\n",
    "\n",
    "        self.logprob = torch.zeros([self.batchsize], device=device, requires_grad=True)\n",
    "\n",
    "    def update(self, point_index):  # point_index is [batchsize]\n",
    "        if point_index.size(0) != self.batchsize:\n",
    "            print(\n",
    "                \"Error: point_index.size() doesn't match expected size, should be [batchsize]\"\n",
    "            )\n",
    "            return\n",
    "        if self.points_mask.gather(1, point_index.unsqueeze(1)).sum():\n",
    "            print(\"Error: some points already added\")\n",
    "            return\n",
    "        triangles_coordinates = self.points.gather(\n",
    "            1,\n",
    "            self.partial_delaunay_triangles.view(self.batchsize, self.ntriangles * 3)\n",
    "            .unsqueeze(2)\n",
    "            .expand(-1, -1, 2),\n",
    "        ).view(\n",
    "            self.batchsize, self.ntriangles, 3, 2\n",
    "        )  # [batchsize, ntriangles, 3, 2]\n",
    "        newpoint = self.points.gather(\n",
    "            1, point_index.unsqueeze(1).unsqueeze(2).expand(self.batchsize, 1, 2)\n",
    "        ).squeeze(\n",
    "            1\n",
    "        )  # [batchsize, 2]\n",
    "\n",
    "        incircle_matrix = torch.cat(\n",
    "            [\n",
    "                triangles_coordinates,\n",
    "                newpoint.unsqueeze(1).unsqueeze(2).expand(-1, self.ntriangles, 1, -1),\n",
    "            ],\n",
    "            dim=-2,\n",
    "        )  # [batchsize, ntriangles, 4, 2]\n",
    "        incircle_matrix = torch.cat(\n",
    "            [\n",
    "                incircle_matrix,\n",
    "                (incircle_matrix * incircle_matrix).sum(-1, keepdim=True),\n",
    "                torch.ones([self.batchsize, self.ntriangles, 4, 1], device=device),\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )  # [batchsize, ntriangles, 4, 4]\n",
    "        incircle_test = (\n",
    "            incircle_matrix.det() > 0\n",
    "        )  # [batchsize, ntriangles], is True if inside incircle\n",
    "        removed_edge_mask = (\n",
    "            incircle_test.unsqueeze(2).expand(-1, -1, 3).reshape(-1)\n",
    "        )  # [batchsize * ntriangles * 3]\n",
    "\n",
    "        edges = (\n",
    "            self.partial_delaunay_edges\n",
    "            + self.ntriangles\n",
    "            * 3\n",
    "            * torch.arange(self.batchsize, device=device).unsqueeze(1)\n",
    "        ).view(\n",
    "            -1\n",
    "        )  # [batchsize * ntriangles * 3]\n",
    "        neighbouring_edge = edges.masked_select(removed_edge_mask)\n",
    "        neighbouring_edge_mask = torch.zeros(\n",
    "            [self.batchsize * self.ntriangles * 3], device=device, dtype=torch.bool\n",
    "        )\n",
    "        neighbouring_edge_mask[neighbouring_edge] = True\n",
    "        neighbouring_edge_mask = (\n",
    "            neighbouring_edge_mask * removed_edge_mask.logical_not()\n",
    "        )  # [batchsize * ntriangles * 3]\n",
    "\n",
    "        n_new_triangles = neighbouring_edge_mask.view(self.batchsize, -1).sum(\n",
    "            -1\n",
    "        )  # [batchsize]\n",
    "\n",
    "        new_point = (\n",
    "            point_index.unsqueeze(1)\n",
    "            .expand(-1, self.ntriangles * 3)\n",
    "            .masked_select(neighbouring_edge_mask.view(self.batchsize, -1))\n",
    "        )\n",
    "\n",
    "        second_point_mask = neighbouring_edge_mask.view(\n",
    "            self.batchsize, -1, 3\n",
    "        )  # [batchsize, ntriangles 3]\n",
    "        (\n",
    "            first_point_indices0,\n",
    "            first_point_indices1,\n",
    "            first_point_indices2,\n",
    "        ) = second_point_mask.nonzero(as_tuple=True)\n",
    "        first_point_indices2 = (first_point_indices2 != 2) * (first_point_indices2 + 1)\n",
    "\n",
    "        first_point = self.partial_delaunay_triangles[\n",
    "            first_point_indices0, first_point_indices1, first_point_indices2\n",
    "        ]  # [?]\n",
    "        second_point = self.partial_delaunay_triangles.masked_select(\n",
    "            second_point_mask\n",
    "        )  # [?]\n",
    "\n",
    "        new_triangles_mask = torch.cat(\n",
    "            [\n",
    "                incircle_test,\n",
    "                torch.ones([self.batchsize, 2], dtype=torch.bool, device=device),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )  # [batchsize, ntriangles + 2]\n",
    "\n",
    "        new_neighbouring_edges = (\n",
    "            3 * new_triangles_mask.nonzero(as_tuple=True)[1]\n",
    "        )  # [?], 3* since is the 01 edge of new triangles (see later)\n",
    "        self.partial_delaunay_edges.masked_scatter_(\n",
    "            neighbouring_edge_mask.view(self.batchsize, -1), new_neighbouring_edges\n",
    "        )  # still [batchsize, ntriangles * 3] for now\n",
    "\n",
    "        self.partial_delaunay_triangles = torch.cat(\n",
    "            [\n",
    "                self.partial_delaunay_triangles,\n",
    "                torch.empty([self.batchsize, 2, 3], dtype=torch.long, device=device),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        self.partial_delaunay_edges = torch.cat(\n",
    "            [\n",
    "                self.partial_delaunay_edges,\n",
    "                torch.empty([self.batchsize, 6], dtype=torch.long, device=device),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        new_triangles = torch.stack(\n",
    "            [first_point, second_point, new_point], dim=1\n",
    "        )  # [?, 3], edge here is flipped compared to edge in neighbouring triangle (so first_point is the second point in neighbouring edge)\n",
    "        self.partial_delaunay_triangles.masked_scatter_(\n",
    "            new_triangles_mask.unsqueeze(2).expand(-1, -1, 3), new_triangles\n",
    "        )  # [batchsize, ntriangles + 2, 3]\n",
    "\n",
    "        new_edge01 = neighbouring_edge_mask.view(self.batchsize, -1).nonzero(\n",
    "            as_tuple=True\n",
    "        )[\n",
    "            1\n",
    "        ]  # [?]\n",
    "\n",
    "        \"\"\"we are currently storing which triangles have to be inserted, via the edges along the perimeter of the delaunay cavity, we need to compute which edge is to the 'left'/'right' of each edge\"\"\"\n",
    "        \"\"\"don't have the memory to do a batchsize * n * n boolean search, don't have the speed to do a batchsize^2 search (as would be the case for sparse matrix or similar)\"\"\"\n",
    "        \"\"\"best alternative: rotate the edge around right point, repeat until hit edge in mask (will never go to an edge of a removed triangle before we hit edge in mask) should basically be order 1!\"\"\"\n",
    "\n",
    "        neighbouring_edge_index = neighbouring_edge_mask.nonzero(as_tuple=True)[\n",
    "            0\n",
    "        ]  # [?]\n",
    "        next_neighbouring_edge_index = torch.empty_like(neighbouring_edge_index)  # [?]\n",
    "\n",
    "        rotating_flipped_neighbouring_edge_index = neighbouring_edge_mask.nonzero(\n",
    "            as_tuple=True\n",
    "        )[\n",
    "            0\n",
    "        ]  # [?], initialise\n",
    "        todo_mask = torch.ones_like(\n",
    "            next_neighbouring_edge_index, dtype=torch.bool\n",
    "        )  # [?]\n",
    "        while todo_mask.sum():\n",
    "            rotating_neighbouring_edge_index = (\n",
    "                rotating_flipped_neighbouring_edge_index\n",
    "                + 1\n",
    "                - 3 * (rotating_flipped_neighbouring_edge_index % 3 == 2)\n",
    "            )  # [todo_mask.sum()], gets smaller until nothing left EFFICIENCY (this may be seriously stupid, as it requires making a bunch of copies when I could be doing stuff inplace)\n",
    "\n",
    "            update_mask = neighbouring_edge_mask[\n",
    "                rotating_neighbouring_edge_index\n",
    "            ]  # [todo_mask.sum()]\n",
    "            update_mask_unravel = torch.zeros_like(todo_mask).masked_scatter(\n",
    "                todo_mask, update_mask\n",
    "            )  # [?]\n",
    "\n",
    "            next_neighbouring_edge_index.masked_scatter_(\n",
    "                update_mask_unravel,\n",
    "                rotating_neighbouring_edge_index.masked_select(update_mask),\n",
    "            )  # [?]\n",
    "\n",
    "            todo_mask.masked_fill_(update_mask_unravel, False)  # [?]\n",
    "            rotating_flipped_neighbouring_edge_index = edges[\n",
    "                rotating_neighbouring_edge_index.masked_select(\n",
    "                    update_mask.logical_not()\n",
    "                )\n",
    "            ]  # [todo_mask.sum()]\n",
    "        triangle_index = new_triangles_mask.view(-1).nonzero(as_tuple=True)[\n",
    "            0\n",
    "        ]  # [?], index goes up to batchsize * (ntriangles + 2), this is needed for when we invert the permutation by scattering (won't scatter same number of triangles per batch)\n",
    "\n",
    "        next_triangle_index = torch.empty_like(edges).masked_scatter_(\n",
    "            neighbouring_edge_mask, triangle_index\n",
    "        )[\n",
    "            next_neighbouring_edge_index\n",
    "        ]  # [?], index goes up to batchsize * (ntriangles + 2)\n",
    "        next_edge = 3 * next_triangle_index + 1  # [?]\n",
    "\n",
    "        invert_permutation = torch.empty_like(\n",
    "            new_triangles_mask.view(-1), dtype=torch.long\n",
    "        )  # [batchsize * (ntriangles + 2)]\n",
    "        invert_permutation[\n",
    "            next_triangle_index\n",
    "        ] = triangle_index  # [batchsize * (ntriangles + 2)]\n",
    "        previous_triangle_index = invert_permutation.masked_select(\n",
    "            new_triangles_mask.view(-1)\n",
    "        )  # [?]\n",
    "        previous_edge = 3 * previous_triangle_index + 2  # [?]\n",
    "\n",
    "        \"\"\"in the above we rotated around 'first_point' in our new triangles\"\"\"\n",
    "        new_edge20 = next_edge % ((self.ntriangles + 2) * 3)  # [?]\n",
    "        new_edge12 = previous_edge % ((self.ntriangles + 2) * 3)  # [?]\n",
    "\n",
    "        new_edges = torch.stack([new_edge01, new_edge12, new_edge20], dim=1)  # [?, 3]\n",
    "        self.partial_delaunay_edges.masked_scatter_(\n",
    "            new_triangles_mask.unsqueeze(2)\n",
    "            .expand(-1, -1, 3)\n",
    "            .reshape(self.batchsize, -1),\n",
    "            new_edges,\n",
    "        )  # [batchsize, (ntriangles + 2) * 3]\n",
    "\n",
    "        self.ntriangles += 2\n",
    "        \"\"\"currently only count the extra triangles you replace (not the one you have to remove because you're located there, and not the ones you make because you have to create two more\"\"\"\n",
    "        self.cost += n_new_triangles - 3\n",
    "        self.points_mask.scatter_(\n",
    "            1, point_index.unsqueeze(1).expand(-1, self.npoints), True\n",
    "        )\n",
    "        self.points_sequence = torch.cat(\n",
    "            [self.points_sequence, point_index.unsqueeze(1)], dim=1\n",
    "        )\n",
    "\n",
    "\n",
    "# turn integer x,y coords (in nxn grid) into position d (0 to n^2-1) along the Hilbert curve.\n",
    "def xy2d(n, x, y):\n",
    "    [x, y] = [math.floor(x), math.floor(y)]\n",
    "    [rx, ry, s, d] = [0, 0, 0, 0]\n",
    "    s = n / 2\n",
    "    s = math.floor(s)\n",
    "    while s > 0:\n",
    "        rx = (x & s) > 0  # bitwise and, and then boolean is it greater than 0?\n",
    "        ry = (y & s) > 0\n",
    "        d += s * s * ((3 * rx) ^ ry)\n",
    "        [x, y] = rot(n, x, y, rx, ry)\n",
    "        s = s / 2\n",
    "        s = math.floor(s)\n",
    "    return d\n",
    "\n",
    "\n",
    "def rot(n, x, y, rx, ry):\n",
    "    if ry == 0:\n",
    "        if rx == 1:\n",
    "            x = n - 1 - x\n",
    "            y = n - 1 - y\n",
    "        # Swap x and y\n",
    "        t = x\n",
    "        x = y\n",
    "        y = t\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def order(\n",
    "    n, points\n",
    "):  # turns tensor of points into integer distances along hilbert curve of itteration n\n",
    "    grid = n * points.to(\"cpu\")\n",
    "    x = torch.empty([grid.size(0)])\n",
    "    for i in range(points.size(0)):\n",
    "        x[i] = xy2d(n, grid[i, 0], grid[i, 1])\n",
    "    return x\n",
    "\n",
    "\n",
    "def hilbert_insertion(npoints=103, batchsize=200):\n",
    "    env.reset(npoints, batchsize, nsamples=1)\n",
    "    points = env.points[:, 3:]  # [batchsize, npoints - 3]\n",
    "    insertion_order = torch.full([batchsize, npoints], float(\"inf\"), device=device)\n",
    "    for i in range(batchsize):\n",
    "        insertion_order[i, 3:] = order(\n",
    "            2 ** 6, points[i]\n",
    "        )  # number of possible positions is n ** 2\n",
    "    for i in range(npoints - 3):\n",
    "        next_index = insertion_order.min(-1)[1]\n",
    "        env.update(next_index)\n",
    "        insertion_order.scatter_(1, next_index.unsqueeze(1), float(\"inf\"))\n",
    "    print(env.cost.mean().item(), env.cost.var().sqrt().item())\n",
    "\n",
    "\n",
    "def random_insertion(npoints=103, batchsize=200):\n",
    "    env.reset(npoints, batchsize, nsamples=1)\n",
    "    for i in range(npoints - 3):\n",
    "        env.update(torch.full([batchsize], i + 3, dtype=torch.long, device=device))\n",
    "    print(env.cost.mean().item(), env.cost.var().sqrt().item())\n",
    "\n",
    "env = environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2890.909912109375 42.50613021850586\n",
      "2891.059814453125 42.404930114746094\n",
      "2890.579833984375 42.23149490356445\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    random_insertion(1003, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
