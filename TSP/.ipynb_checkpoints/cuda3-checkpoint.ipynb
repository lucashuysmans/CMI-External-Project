{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rational-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "device = \"cuda:1\"\n",
    "floattype = torch.float\n",
    "\n",
    "batchsize = 512\n",
    "nsamples = 8\n",
    "npoints = 5\n",
    "emsize = 512\n",
    "\n",
    "\n",
    "class Graph_Transformer(nn.Module):\n",
    "    def __init__(self, emsize = 64, nhead = 8, nhid = 1024, nlayers = 4, ndecoderlayers = 2, dropout = 0):\n",
    "        super().__init__()\n",
    "        self.emsize = emsize\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "        encoder_layers = TransformerEncoderLayer(emsize, nhead, nhid, dropout = dropout)\n",
    "        decoder_layers = TransformerDecoderLayer(emsize, nhead, nhid, dropout = dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layers, ndecoderlayers)\n",
    "        self.encoder = nn.Linear(2, emsize)\n",
    "        self.outputattention_query = nn.Linear(emsize, emsize, bias = False)\n",
    "        self.outputattention_key = nn.Linear(emsize, emsize, bias = False)\n",
    "        self.start_token = nn.Parameter(torch.randn([emsize], device = device))\n",
    "    \n",
    "    def generate_subsequent_mask(self, sz): #last dimension will be softmaxed over when adding to attention logits, if boolean the ones turn into -inf\n",
    "        #mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        #mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        \n",
    "        mask = torch.triu(torch.ones([sz, sz], dtype = torch.bool, device = device), diagonal = 1)\n",
    "        return mask\n",
    "    \n",
    "    def encode(self, src): #src must be [batchsize * nsamples, npoints, 3]\n",
    "        src = self.encoder(src).transpose(0, 1)\n",
    "        output = self.transformer_encoder(src)\n",
    "        return output #[npoints, batchsize * nsamples, emsize]\n",
    "    \n",
    "    def decode_next(self, memory, tgt, route_mask): #route mask is [batchsize * nsamples, npoints], both memory and tgt must have batchsize and nsamples in same dimension (the 1th one)\n",
    "        npoints = memory.size(0)\n",
    "        batchsize = tgt.size(1)\n",
    "        \"\"\"if I really wanted this to be efficient I'd only recompute the decoder for the last tgt, and just remebering what the others looked like from before (won't change due to mask)\"\"\"\n",
    "        \"\"\"have the option to freeze the autograd on all but the last part of tgt, although at the moment this is a very natural way to say: initial choices matter more\"\"\"\n",
    "        tgt_mask = self.generate_subsequent_mask(tgt.size(0))\n",
    "        output = self.transformer_decoder(tgt, memory, tgt_mask) #[tgt, batchsize * nsamples, emsize]\n",
    "        output_query = self.outputattention_query(memory).transpose(0, 1) #[batchsize * nsamples, npoints, emsize]\n",
    "        output_key = self.outputattention_key(output[-1]) #[batchsize * nsamples, emsize]\n",
    "        output_attention = torch.matmul(output_query * self.emsize ** -0.5, output_key.unsqueeze(-1)).squeeze(-1) #[batchsize * nsamples, npoints], technically don't need to scale attention as we divide by variance next anyway\n",
    "        output_attention_tanh = output_attention.tanh() #[batchsize * nsamples, npoints]\n",
    "        \n",
    "        #we clone the route_mask incase we want to backprop using it (else it was modified by inplace opporations)\n",
    "        output_attention = output_attention.masked_fill(route_mask.clone(), float('-inf')) #[batchsize * nsamples, npoints]\n",
    "        output_attention_tanh = output_attention_tanh.masked_fill(route_mask.clone(), float('-inf')) #[batchsize * nsamples, npoints]\n",
    "        \n",
    "        return output_attention_tanh, output_attention #[batchsize * nsamples, npoints]\n",
    "    \n",
    "    def calculate_logprob(self, memory, routes): #memory is [npoints, batchsize * nsamples, emsize], routes is [batchsize * nsamples, npoints - 4], rather than backproping the entire loop, this saves vram (and computation)\n",
    "        npoints = memory.size(0)\n",
    "        ninternalpoints = routes.size(1)\n",
    "        bigbatchsize = memory.size(1)\n",
    "        memory_ = memory.gather(0, routes.transpose(0, 1).unsqueeze(2).expand(-1, -1, self.emsize)) #[npoints - 4, batchsize * nsamples, emsize] reorder memory into order of routes\n",
    "        tgt = torch.cat([self.start_token.unsqueeze(0).unsqueeze(1).expand(1, bigbatchsize, -1), memory_[:-1]]) #[npoints - 4, batchsize * nroutes, emsize], want to go from memory to tgt\n",
    "        tgt_mask = self.generate_subsequent_mask(ninternalpoints)\n",
    "        output = self.transformer_decoder(tgt, memory, tgt_mask) #[npoints - 4, batchsize * nsamples, emsize]\n",
    "        \"\"\"want probability of going from key to query, but first need to normalise (softmax with mask)\"\"\"\n",
    "        output_query = self.outputattention_query(memory_).transpose(0, 1) #[batchsize * nsamples, npoints - 4, emsize]\n",
    "        output_key = self.outputattention_key(output).transpose(0, 1) #[batchsize * nsamples, npoints - 4, emsize]\n",
    "        attention_mask = torch.full([ninternalpoints, ninternalpoints], True, device = device).triu(1) #[npoints - 4, npoints - 4], True for i < j\n",
    "        output_attention = torch.matmul(output_query * self.emsize ** -0.5, output_key.transpose(-1, -2))\n",
    "        \"\"\"quick fix to stop divergence\"\"\"\n",
    "        output_attention_tanh = output_attention.tanh()\n",
    "        \n",
    "        output_attention_tanh = output_attention_tanh.masked_fill(attention_mask, float('-inf'))\n",
    "        output_attention_tanh = output_attention_tanh - output_attention_tanh.logsumexp(-2, keepdim = True) #[batchsize * nsamples, npoints - 4, npoints - 4]\n",
    "        \n",
    "        output_attention = output_attention.masked_fill(attention_mask, float('-inf'))\n",
    "        output_attention = output_attention - output_attention.logsumexp(-2, keepdim = True) #[batchsize * nsamples, npoints - 4, npoints - 4]\n",
    "        \n",
    "        \"\"\"infact I'm almost tempted to not mask choosing a previous point, so it's forced to learn it and somehow incorporate it into its computation, but without much impact on reinforcing good examples\"\"\"\n",
    "        logprob_tanh = output_attention_tanh.diagonal(dim1 = -1, dim2 = -2).sum(-1) #[batchsize * nsamples]\n",
    "        logprob = output_attention.diagonal(dim1 = -1, dim2 = -2).sum(-1) #[batchsize * nsamples]\n",
    "        return logprob_tanh, logprob #[batchsize * nsamples]\n",
    "\n",
    "NN = Graph_Transformer().to(device)\n",
    "optimizer = optim.Adam(NN.parameters())\n",
    "\n",
    "\n",
    "class environment:    \n",
    "    def reset(self, npoints, batchsize, nsamples=1):\n",
    "        self.batchsize = (\n",
    "            batchsize * nsamples\n",
    "        )  # so that I don't have to rewrite all this code, we store these two dimensions together\n",
    "        self.nsamples = nsamples\n",
    "        self.npoints = npoints\n",
    "        self.points = (\n",
    "            torch.rand([batchsize, npoints, 2], dtype = floattype, device=device)\n",
    "            .unsqueeze(1)\n",
    "            .expand(-1, nsamples, -1, -1)\n",
    "            .reshape(self.batchsize, npoints, 2)\n",
    "        )\n",
    "        \n",
    "        self.distance_matrix = (self.points.unsqueeze(1) - self.points.unsqueeze(2)).square().sum(-1).sqrt() # [batchsize * nsamples, npoints, npoints]\n",
    "        \n",
    "        self.previous_point = None\n",
    "        \n",
    "        self.points_mask = torch.zeros(\n",
    "                    [self.batchsize, npoints], dtype=torch.bool, device=device\n",
    "                )\n",
    "        self.points_sequence = torch.empty(\n",
    "            [self.batchsize, 0], dtype=torch.long, device=device\n",
    "        )\n",
    "        \n",
    "        self.cost = torch.zeros([self.batchsize], dtype = floattype, device=device)\n",
    "\n",
    "        self.logprob = torch.zeros([self.batchsize], dtype = floattype, device=device, requires_grad=True)\n",
    "\n",
    "    def update(self, point_index):  # point_index is [batchsize]\n",
    "        \n",
    "        assert list(point_index.size()) == [self.batchsize]\n",
    "        assert str(point_index.device) == device\n",
    "        assert self.points_mask.gather(1, point_index.unsqueeze(1)).sum() == 0\n",
    "        \n",
    "        if self.previous_point != None:\n",
    "            self.cost += self.distance_matrix.gather(2, self.previous_point.unsqueeze(1).unsqueeze(2).expand(-1, self.npoints, 1)).squeeze(2).gather(1, point_index.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        self.previous_point = point_index\n",
    "        self.points_mask.scatter_(1, point_index.unsqueeze(1), True)\n",
    "        self.points_sequence = torch.cat([self.points_sequence, point_index.unsqueeze(1)], dim = 1)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def sample_point(self, logits): #logits must be [batchsize * nsamples, npoints]\n",
    "        probs = torch.distributions.categorical.Categorical(logits = logits)\n",
    "        next_point = probs.sample() #size is [batchsize * nsamples]\n",
    "        self.update(next_point)\n",
    "        self.logprob = self.logprob + probs.log_prob(next_point)\n",
    "        return next_point #[batchsize * nsamples]\n",
    "    \n",
    "    def sampleandgreedy_point(self, logits): #logits must be [batchsize * nsamples, npoints], last sample will be the greedy choice (but we still need to keep track of its logits)\n",
    "        logits_sample = logits.view(-1, self.nsamples, self.npoints)[:, :-1, :]\n",
    "        probs = torch.distributions.categorical.Categorical(logits = logits_sample)\n",
    "        \n",
    "        sample_point = probs.sample() #[batchsize, (nsamples - 1)]\n",
    "        greedy_point = logits.view(-1, self.nsamples, self.npoints)[:, -1, :].max(-1, keepdim = True)[1] #[batchsize, 1]\n",
    "        next_point = torch.cat([sample_point, greedy_point], dim = 1).view(-1)\n",
    "        self.update(next_point)\n",
    "        self.logprob = self.logprob + torch.cat([probs.log_prob(sample_point), torch.zeros([sample_point.size(0), 1], device = device)], dim = 1).view(-1)\n",
    "        return next_point\n",
    "    \n",
    "    def laststep(self):\n",
    "        \n",
    "        assert self.points_sequence.size(1) == self.npoints\n",
    "        \n",
    "        self.cost += self.distance_matrix.gather(2, self.points_sequence[:, 0].unsqueeze(1).unsqueeze(2).expand(-1, self.npoints, 1)).squeeze(2).gather(1, self.points_sequence[:, -1].unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "\n",
    "env = environment()\n",
    "\n",
    "\n",
    "def evaluate(epochs = 10, npoints = 10, batchsize = 100, nsamples = 2, negative_cutoff = 1):\n",
    "    NN.eval()\n",
    "    for i in range(epochs):\n",
    "        env.reset(npoints, batchsize, nsamples)\n",
    "        \"\"\"include the boundary points, kinda makes sense that they should contribute (atm only in the encoder, difficult to see how in the decoder)\"\"\"\n",
    "        memory = NN.encode(env.points) #[npoints, batchsize * nsamples, emsize]\n",
    "        #### #### #### remember to include tgt.detach() when reinstate with torch.no_grad()\n",
    "        tgt = NN.start_token.unsqueeze(0).unsqueeze(1).expand(1, batchsize * nsamples, -1).detach() #[1, batchsize * nsamples, emsize]\n",
    "        #with torch.no_grad(): #to speed up computation, selecting routes is done without gradient\n",
    "        with torch.no_grad():\n",
    "            for j in range(0, npoints):\n",
    "                #### #### #### remember to include memory.detach() when reinstate with torch.no_grad()\n",
    "                _, logits = NN.decode_next(memory.detach(), tgt, env.points_mask)\n",
    "                next_point = env.sampleandgreedy_point(logits)\n",
    "                \"\"\"\n",
    "                for inputing the previous embedding into decoder\n",
    "                \"\"\"\n",
    "                tgt = torch.cat([tgt, memory.gather(0, next_point.unsqueeze(0).unsqueeze(2).expand(1, -1, memory.size(2)))]) #[nsofar, batchsize * nsamples, emsize]\n",
    "                \"\"\"\n",
    "                for inputing the previous decoder output into the decoder (allows for an evolving strategy, but doesn't allow for fast training\n",
    "                \"\"\"\n",
    "                ############\n",
    "\n",
    "        env.laststep()\n",
    "        \n",
    "        _, logprob = NN.calculate_logprob(memory, env.points_sequence) #[batchsize * nsamples]\n",
    "        NN.train()\n",
    "        \"\"\"\n",
    "        clip logprob so doesn't reinforce things it already knows\n",
    "        TBH WANT SOMETHING DIFFERENT ... want to massively increase training if find something unexpected and otherwise not\n",
    "        \"\"\"\n",
    "        greedy_baseline = env.cost.view(batchsize, nsamples)[:, -1] #[batchsize], greedy sample\n",
    "        \n",
    "        print(greedy_baseline.mean().item(), logprob.view(batchsize, nsamples)[:, -1].mean().item())\n",
    "\n",
    "\n",
    "def train(epochs = 30000, npoints = 10, batchsize = 100, nsamples = 8, negative_cutoff = 1):\n",
    "    NN.train()\n",
    "    for i in range(epochs):\n",
    "        env.reset(npoints, batchsize, nsamples)\n",
    "        \"\"\"include the boundary points, kinda makes sense that they should contribute (atm only in the encoder, difficult to see how in the decoder)\"\"\"\n",
    "        memory = NN.encode(env.points) #[npoints, batchsize * nsamples, emsize]\n",
    "        #### #### #### remember to include tgt.detach() when reinstate with torch.no_grad()\n",
    "        tgt = NN.start_token.unsqueeze(0).unsqueeze(1).expand(1, batchsize * nsamples, -1).detach() #[1, batchsize * nsamples, emsize]\n",
    "        \n",
    "        NN.eval()\n",
    "        #with torch.no_grad(): #to speed up computation, selecting routes is done without gradient\n",
    "        with torch.no_grad():\n",
    "            for j in range(0, npoints):\n",
    "                #### #### #### remember to include memory.detach() when reinstate with torch.no_grad()\n",
    "                _, logits = NN.decode_next(memory.detach(), tgt, env.points_mask)\n",
    "                next_point = env.sampleandgreedy_point(logits)\n",
    "                \"\"\"\n",
    "                for inputing the previous embedding into decoder\n",
    "                \"\"\"\n",
    "                tgt = torch.cat([tgt, memory.gather(0, next_point.unsqueeze(0).unsqueeze(2).expand(1, -1, memory.size(2)))]) #[nsofar, batchsize * nsamples, emsize]\n",
    "                \"\"\"\n",
    "                for inputing the previous decoder output into the decoder (allows for an evolving strategy, but doesn't allow for fast training\n",
    "                \"\"\"\n",
    "                ############\n",
    "\n",
    "        env.laststep()\n",
    "        \n",
    "        _, logprob = NN.calculate_logprob(memory, env.points_sequence) #[batchsize * nsamples]\n",
    "        NN.train()\n",
    "        \"\"\"\n",
    "        clip logprob so doesn't reinforce things it already knows\n",
    "        TBH WANT SOMETHING DIFFERENT ... want to massively increase training if find something unexpected and otherwise not\n",
    "        \"\"\"\n",
    "        greedy_prob = logprob.view(batchsize, nsamples)[:, -1].detach() #[batchsize]\n",
    "        greedy_baseline = env.cost.view(batchsize, nsamples)[:, -1] #[batchsize], greedy sample\n",
    "        fixed_baseline = 0.5 * torch.ones([1], device = device)\n",
    "        min_baseline = env.cost.view(batchsize, nsamples)[:, :-1].min(-1)[0] #[batchsize], minimum cost\n",
    "        baseline = greedy_baseline\n",
    "        positive_reinforcement = - F.relu( - (env.cost.view(batchsize, nsamples)[:, :-1] - baseline.unsqueeze(1))) #don't scale positive reinforcement\n",
    "        negative_reinforcement = F.relu(env.cost.view(batchsize, nsamples)[:, :-1] - baseline.unsqueeze(1))\n",
    "        positive_reinforcement_binary = env.cost.view(batchsize, nsamples)[:, :-1] - baseline.unsqueeze(1) <= -0.05\n",
    "        negative_reinforcement_binary = env.cost.view(batchsize, nsamples)[:, :-1] - baseline.unsqueeze(1) > 0.5\n",
    "        \"\"\"\n",
    "        binary positive reinforcement\n",
    "        \"\"\"\n",
    "        #loss = - ((logprob.view(batchsize, nsamples)[:, :-1] < -0.2) * logprob.view(batchsize, nsamples)[:, :-1] * positive_reinforcement_binary).mean() #+ (logprob.view(batchsize, nsamples)[:, :-1] > -1) * logprob.view(batchsize, nsamples)[:, :-1] * negative_reinforcement_binary\n",
    "        \"\"\"\n",
    "        clipped binary reinforcement\n",
    "        \"\"\"\n",
    "        #loss = ( \n",
    "        #        - logprob.view(batchsize, nsamples)[:, :-1] \n",
    "        #        #* (logprob.view(batchsize, nsamples)[:, :-1] < 0) \n",
    "        #        * positive_reinforcement_binary \n",
    "        #        + logprob.view(batchsize, nsamples)[:, :-1] \n",
    "        #        #* (logprob.view(batchsize, nsamples)[:, :-1] > greedy_prob.unsqueeze(1) - 40) \n",
    "        #        * negative_reinforcement_binary \n",
    "        #).mean()\n",
    "        \"\"\"\n",
    "        scaled binary reinforcement\n",
    "        \"\"\"\n",
    "        #loss = ( \n",
    "        #        (- logprob.view(batchsize, nsamples)[:, :-1] \n",
    "        #        * positive_reinforcement_binary).sum() / (positive_reinforcement_binary.sum() + 1)\n",
    "        #        + (logprob.view(batchsize, nsamples)[:, :-1] \n",
    "        #        * negative_reinforcement_binary).sum() / (negative_reinforcement_binary.sum() + 1)\n",
    "        #)\n",
    "        \"\"\"\n",
    "        clipped binary postive, clipped weighted negative\n",
    "        \"\"\"\n",
    "        #loss = ( - logprob.view(batchsize, nsamples)[:, :-1] * (logprob.view(batchsize, nsamples)[:, :-1] < -0.2) * positive_reinforcement_binary + logprob.view(batchsize, nsamples)[:, :-1] * (logprob.view(batchsize, nsamples)[:, :-1] > -2) * negative_reinforcement ).mean()\n",
    "        \"\"\"\n",
    "        clipped reinforcement without rescaling\n",
    "        \"\"\"\n",
    "        #loss = ((logprob.view(batchsize, nsamples)[:, :-1] < -0.7) * logprob.view(batchsize, nsamples)[:, :-1] * positive_reinforcement + (logprob.view(batchsize, nsamples)[:, :-1] > -5) * logprob.view(batchsize, nsamples)[:, :-1] * negative_reinforcement).mean()\n",
    "        \"\"\"\n",
    "        clipped reinforcement\n",
    "        \"\"\"\n",
    "        #loss = (logprob.view(batchsize, nsamples)[:, :-1] * positive_reinforcement / (positive_reinforcement.var() + 0.001).sqrt() + (logprob.view(batchsize, nsamples)[:, :-1] > -3) * logprob.view(batchsize, nsamples)[:, :-1] * negative_reinforcement / (negative_reinforcement.var() + 0.001).sqrt()).mean()\n",
    "        \"\"\"\n",
    "        balanced reinforcement\n",
    "        \"\"\"\n",
    "        #loss = (logprob.view(batchsize, nsamples)[:, :-1] * (positive_reinforcement / (positive_reinforcement.var() + 0.001).sqrt() + negative_reinforcement / (negative_reinforcement.var() + 0.001).sqrt())).mean()\n",
    "        \"\"\"\n",
    "        regular loss\n",
    "        \"\"\"\n",
    "        loss = (logprob.view(batchsize, nsamples)[:, :-1] * (positive_reinforcement + negative_reinforcement)).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #print(NN.encoder.weight.grad)\n",
    "        optimizer.step()\n",
    "        #print(greedy_baseline.mean().item())\n",
    "        print(greedy_baseline.mean().item(), logprob.view(batchsize, nsamples)[:, -1].mean().item(), logprob.view(batchsize, nsamples)[:, :-1].mean().item(), logprob[batchsize - 1].item(), logprob[0].item(), env.logprob[0].item())\n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(300000, 20, 1000, 8, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "concerned-prayer",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-046942d9ec8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TSP_50points_big_6.00'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/nfs/software/Conda/conda_environments/pytorch-1.8.0-cuda10.2/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    590\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/software/Conda/conda_environments/pytorch-1.8.0-cuda10.2/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/software/Conda/conda_environments/pytorch-1.8.0-cuda10.2/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/software/Conda/conda_environments/pytorch-1.8.0-cuda10.2/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/software/Conda/conda_environments/pytorch-1.8.0-cuda10.2/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/software/Conda/conda_environments/pytorch-1.8.0-cuda10.2/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mstorage_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/software/Conda/conda_environments/pytorch-1.8.0-cuda10.2/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nfs/software/Conda/conda_environments/pytorch-1.8.0-cuda10.2/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;31m# We may need to call lazy init again if we are a forked child\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;31m# del _CudaBase.__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "NN.load_state_dict(torch.load('TSP_50points_big_6.00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(epochs = 30000, npoints = 10, batchsize = 100, nsamples = 8, negative_cutoff = 1):\n",
    "    NN.eval()\n",
    "    for i in range(epochs):\n",
    "        env.reset(npoints, batchsize, nsamples)\n",
    "        \"\"\"include the boundary points, kinda makes sense that they should contribute (atm only in the encoder, difficult to see how in the decoder)\"\"\"\n",
    "        memory = NN.encode(env.points) #[npoints, batchsize * nsamples, emsize]\n",
    "        #### #### #### remember to include tgt.detach() when reinstate with torch.no_grad()\n",
    "        tgt = NN.start_token.unsqueeze(0).unsqueeze(1).expand(1, batchsize * nsamples, -1).detach() #[1, batchsize * nsamples, emsize]\n",
    "        \n",
    "        NN.eval()\n",
    "        #with torch.no_grad(): #to speed up computation, selecting routes is done without gradient\n",
    "        with torch.no_grad():\n",
    "            for j in range(0, npoints):\n",
    "                #### #### #### remember to include memory.detach() when reinstate with torch.no_grad()\n",
    "                _, logits = NN.decode_next(memory.detach(), tgt, env.points_mask)\n",
    "                next_point = env.sampleandgreedy_point(logits)\n",
    "                \"\"\"\n",
    "                for inputing the previous embedding into decoder\n",
    "                \"\"\"\n",
    "                tgt = torch.cat([tgt, memory.gather(0, next_point.unsqueeze(0).unsqueeze(2).expand(1, -1, memory.size(2)))]) #[nsofar, batchsize * nsamples, emsize]\n",
    "                \"\"\"\n",
    "                for inputing the previous decoder output into the decoder (allows for an evolving strategy, but doesn't allow for fast training\n",
    "                \"\"\"\n",
    "                ############\n",
    "\n",
    "        env.laststep()\n",
    "        \n",
    "        _, logprob = NN.calculate_logprob(memory, env.points_sequence) #[batchsize * nsamples]\n",
    "        \n",
    "        \"\"\"\n",
    "        clip logprob so doesn't reinforce things it already knows\n",
    "        TBH WANT SOMETHING DIFFERENT ... want to massively increase training if find something unexpected and otherwise not\n",
    "        \"\"\"\n",
    "        greedy_prob = logprob.view(batchsize, nsamples)[:, -1].detach() #[batchsize]\n",
    "        greedy_baseline = env.cost.view(batchsize, nsamples)[:, -1] #[batchsize], greedy sample\n",
    "        \n",
    "        print(greedy_baseline.mean().item(), logprob.view(batchsize, nsamples)[:, -1].mean().item(), logprob.view(batchsize, nsamples)[:, :-1].mean().item(), logprob[batchsize - 1].item(), logprob[0].item(), env.logprob[0].item())\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(10, 50, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "european-supplier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 17 23:58:15 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.118.02   Driver Version: 440.118.02   CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   57C    P0   157W / 250W |  16229MiB / 16280MiB |     88%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  Off  | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    32W / 250W |   4747MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     26844      C   ...ments/pytorch-1.8.0-cuda10.2/bin/python 16219MiB |\n",
      "|    1     28130      C   ...ments/pytorch-1.8.0-cuda10.2/bin/python  4737MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "             total       used       free     shared    buffers     cached\n",
      "Mem:        385627       7384     378242         46          2       2351\n",
      "-/+ buffers/cache:       5030     380596\n",
      "Swap:       196614        137     196477\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvidia-smi\n",
    "free -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-outreach",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
