{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "specified-mobile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 17 00:19:05 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.118.02   Driver Version: 440.118.02   CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    33W / 250W |   9979MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  Off  | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   55C    P0   153W / 250W |  15501MiB / 16280MiB |    100%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     29636      C   ...ments/pytorch-1.8.0-cuda10.2/bin/python  9969MiB |\n",
      "|    1      1065      C   ...ments/pytorch-1.8.0-cuda10.2/bin/python 15491MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "device = \"cpu\"\n",
    "floattype = torch.float\n",
    "\n",
    "\n",
    "class environment:    \n",
    "    def reset(self, npoints, batchsize, nsamples=1):\n",
    "        self.batchsize = (\n",
    "            batchsize * nsamples\n",
    "        )  # so that I don't have to rewrite all this code, we store these two dimensions together\n",
    "        self.nsamples = nsamples\n",
    "        self.npoints = npoints\n",
    "        self.points = (\n",
    "            torch.rand([batchsize, npoints, 2], dtype = floattype, device=device)\n",
    "            .unsqueeze(1)\n",
    "            .expand(-1, nsamples, -1, -1)\n",
    "            .reshape(self.batchsize, npoints, 2)\n",
    "        )\n",
    "        \n",
    "        self.distance_matrix = (self.points.unsqueeze(1) - self.points.unsqueeze(2)).square().sum(-1).sqrt() # [batchsize * nsamples, npoints, npoints]\n",
    "        \n",
    "        self.previous_point = None\n",
    "        \n",
    "        self.points_mask = torch.zeros(\n",
    "                    [self.batchsize, npoints], dtype=torch.bool, device=device\n",
    "                )\n",
    "        self.points_sequence = torch.empty(\n",
    "            [self.batchsize, 0], dtype=torch.long, device=device\n",
    "        )\n",
    "        \n",
    "        self.cost = torch.zeros([self.batchsize], dtype = floattype, device=device)\n",
    "\n",
    "        self.logprob = torch.zeros([self.batchsize], dtype = floattype, device=device, requires_grad=True)\n",
    "\n",
    "    def update(self, point_index):  # point_index is [batchsize]\n",
    "        \n",
    "        assert list(point_index.size()) == [self.batchsize]\n",
    "        assert str(point_index.device) == device\n",
    "        assert self.points_mask.gather(1, point_index.unsqueeze(1)).sum() == 0\n",
    "        \n",
    "        if self.previous_point != None:\n",
    "            self.cost += self.distance_matrix.gather(2, self.previous_point.unsqueeze(1).unsqueeze(2).expand(-1, self.npoints, 1)).squeeze(2).gather(1, point_index.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        self.previous_point = point_index\n",
    "        self.points_mask.scatter_(1, point_index.unsqueeze(1), True)\n",
    "        self.points_sequence = torch.cat([self.points_sequence, point_index.unsqueeze(1)], dim = 1)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def laststep(self):\n",
    "        \n",
    "        assert self.points_sequence.size(1) == self.npoints\n",
    "        \n",
    "        self.cost += self.distance_matrix.gather(2, self.points_sequence[:, 0].unsqueeze(1).unsqueeze(2).expand(-1, self.npoints, 1)).squeeze(2).gather(1, self.points_sequence[:, -1].unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "\n",
    "\n",
    "def farthest_insertion(npoints, batchsize):\n",
    "    points = torch.rand([batchsize, npoints, 2], device = device)\n",
    "    points_mask = torch.zeros([self.batchsize, npoints], dtype=torch.bool, device=device)\n",
    "    distance_matrix = (points.unsqueeze(1) - points.unsqueeze(2)).square().sum(-1).sqrt() # [batchsize * nsamples, npoints, npoints]\n",
    "    points_sequence = torch.empty([batchsize, 0], dtype = torch.long, device = device_)\n",
    "    for i in range(npoints - 1):\n",
    "        distance_from_tour = distance_matrix.gather(2, self.points_mask.unsqueeze(1).expand(-1, npoints, -1)).min(2)[0] #[batchsize, npoints]\n",
    "        next_point = distance_from_tour.max(1)[1] #[batchsize], is index of point\n",
    "        points_mask.scatter_(1, next_point.unsqueeze(1), True)\n",
    "        insertion_segment = torch.cat([points_sequence, next_point, points_sequence.roll(1, 1)], dim = 2) #[batchsize, sequence, 3]\n",
    "        insertion_cost = distance_matrix.gather(2, ) + distance_matrix.gather(2, ) - distance_matrix.gather(2, ) #[batchsize, sequence]\n",
    "        insertion_location = insertion_cost.min(insertion_cost)[1] #[batchsize], insertion location\n",
    "        #turn location into indices i, i+1, modulo back to zero, so can insert next point into last position or something"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
